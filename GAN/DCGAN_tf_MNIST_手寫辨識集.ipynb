{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189602f9",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ac42c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.12 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (25.2.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (1.71.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (3.13.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (0.4.30)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (18.1.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (4.25.7)\n",
      "Requirement already satisfied: setuptools in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (1.17.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (4.13.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorflow==2.12) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12) (1.15.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.39.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
      "Requirement already satisfied: matplotlib in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: numpy in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# GPU 加速版 TensorFlow 從 2.12 開始，不需要自己另外安裝 CUDA/cuDNN，pip版會自帶，需要驅動夠新即可！（NVIDIA Driver >= 450.xx）\n",
    "!pip install tensorflow==2.12\n",
    "!pip install matplotlib numpy pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0c3bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580873af",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the dataset\n",
    "- 從 TensorFlow 內建的資料集中，載入 MNIST 數字手寫資料集的「訓練資料」部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3255cea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2bb3f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# 查看每張圖片維度\n",
    "print(X_train.shape) # 60000 張圖片，每張圖28*28\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a10bb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 取出單張圖片\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(2,2))  # 設定畫布大小，(寬, 高)，單位是英吋\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccc688",
   "metadata": {},
   "source": [
    "- 深度學習的 CNN 模型（比如 DCGAN）要吃4D資料（圖片高、寬、通道數） => 轉reshape\n",
    "- 把資料型態轉成 float32，因為之後進神經網路計算，會希望用浮點數（float）而不是原本的整數（uint8）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72562ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042a4b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 255.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看原本像素值範圍\n",
    "X_train[0].min(), X_train[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aeb18b",
   "metadata": {},
   "source": [
    "## data Standardized()\n",
    "有兩種方式：\n",
    "- 方式01：tf.keras.layers.Rescaling\n",
    "  - 從 TensorFlow 2.6 以後官方推薦的方法！\n",
    "  - 直接加一層在模型最前面，會自動做標準化，超級乾淨！\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "rescale_layer = Rescaling(scale=1./127.5, offset=-1) # 加一層 Rescaling 把數值從 [0,255] 正規化到 [-1,1]\n",
    "```\n",
    "\n",
    "- 方式02：\n",
    "  - 這個不是轉成 -1 ~ 1，而是讓每張圖片變成：平均值是 0、標準差是 1\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = tf.image.per_image_standardization(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58040a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:32:03.300568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-28 15:32:03.300888: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "rescale_layer = Rescaling(1./127.5, offset=-1)\n",
    "\n",
    "X_train = rescale_layer(X_train) # 用 rescale_layer 處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763c1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 Buffer 和 Batch Size\n",
    "buffer_size = 60000 # 是全部資料的數量（60000筆）\n",
    "batch_size = 256 # 每次拿幾筆資料進去訓練，通常設定 batch_size = 256、128、64 等常見數字\n",
    "\n",
    "# 轉換成 TensorFlow Dataset：把 numpy array 轉成 TensorFlow 的 Dataset 格式，方便後面拿資料訓練\n",
    "X_train = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "651d39b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df2a4b",
   "metadata": {},
   "source": [
    "##  DCGAN的 Generator（生成器）模型\n",
    "\n",
    "- Batch normalization: https://keras.io/api/layers/normalization_layers/batch_normalization/\n",
    "- Leaky Relu: https://keras.io/api/layers/activation_layers/leaky_relu/\n",
    "- Padding: https://www.pico.net/kb/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3576cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12544)             1254400   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 12544)            50176     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 7, 7, 128)        819200    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 14, 14, 64)       204800    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        1601      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,330,945\n",
      "Trainable params: 2,305,473\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_generator():\n",
    "    network = tf.keras.Sequential() # 建立一個順序型的神經網路\n",
    "\n",
    "    network.add(layers.Dense(units=7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    # 期望輸出 7×7×256 = 12544 個神經元 ➔ 為了之後可以 reshape 成一個 7x7 的小小圖片，並且有 256個通道\n",
    "    # 不加偏置項（bias），因為後面會有 BatchNormalization（BatchNorm會自己處理偏移問題）。\n",
    "    # 輸入是一個 100維的向量\n",
    "\n",
    "    network.add(layers.BatchNormalization())\n",
    "    # Batch Normalization 層：讓輸出的資料標準化成 平均值0、標準差1，加速模型訓練、穩定收斂\n",
    "\n",
    "    network.add(layers.LeakyReLU())\n",
    "    # LeakyReLU 激活函數：跟 ReLU 很像，但對於負數部分不是直接設成0，而是給一個小小斜率（例如0.2）。\n",
    "    # 避免「死神經」（神經元不再更新）\n",
    "\n",
    "    network.add(layers.Reshape((7,7,256))) \n",
    "    # 把剛剛 12544 維的向量，重塑成一個形狀是 7×7×256 的小立方體（小圖片）\n",
    "    # 希望後面能用卷積操作 (Conv2DTranspose) 來放大這個小圖片。\n",
    "\n",
    "    ##### 第一次轉置卷積（調特徵數，不放大）######\n",
    "    network.add(layers.Conv2DTranspose(filters=128, kernel_size=(5,5), padding='same', use_bias=False))\n",
    "    # 轉置卷積層（有時又叫「反卷積層」\n",
    "    # 這一層要學習 128 個特徵圖（feature maps）\n",
    "    # padding='same'：代表卷積時輸出大小不變（保持7x7）。\n",
    "\n",
    "    network.add(layers.BatchNormalization())\n",
    "    network.add(layers.LeakyReLU())\n",
    "\n",
    "    ##### 第二次轉置卷積（開始放大尺寸）######\n",
    "    network.add(layers.Conv2DTranspose(filters=64, kernel_size=(5,5), padding='same', strides=(2,2), use_bias=False))\n",
    "    # 設成 (2,2) ➔ 讓圖從 7x7 放大到 14x14！\n",
    "    network.add(layers.BatchNormalization())\n",
    "    network.add(layers.LeakyReLU())\n",
    "\n",
    "    ##### 第三次轉置卷積（放大到28x28，生成最終圖片）######\n",
    "    network.add(layers.Conv2DTranspose(filters=1, kernel_size=(5,5), padding='same', strides=(2,2), use_bias=True, activation='tanh'))\n",
    "    # filters=1：這層只有一個通道，因為我們要生成灰階圖片。\n",
    "    # 繼續使用5x5濾波器\n",
    "    # strides=(2,2)：再放大一次，讓 14x14 ➔ 28x28。\n",
    "    # activation='tanh'：使用 tanh，把輸出值限制在 [-1, 1]，符合資料標準化後的範圍\n",
    "\n",
    "    network.summary() # 印出網路結構\n",
    "    return network\n",
    "\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e8f4dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='dense_input'), name='dense_input', description=\"created by layer 'dense_input'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), name='conv2d_transpose_2/Tanh:0', description=\"created by layer 'conv2d_transpose_2'\")\n"
     ]
    }
   ],
   "source": [
    "print(generator.input)\n",
    "print(generator.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f59973",
   "metadata": {},
   "source": [
    "### 製造隨機噪聲來餵給 Generator\n",
    "- Generator 要從一個隨機噪聲（Noise Vector）去「想像」出一張假的圖片\n",
    "- GAN不是直接給圖，它是從無到有生成的。\n",
    "- 這個噪聲就是「創作靈感」啦！（不然 Generator 都沒靈感怎麼畫圖XD）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c59a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
       "array([[-0.7451495 , -0.20478016, -0.3776187 ,  0.35397094, -0.81424314,\n",
       "         1.4398834 , -1.3104447 , -0.27728567,  0.05576651,  0.7985706 ,\n",
       "         1.1394254 , -0.5686352 , -0.67105806, -1.2619956 ,  1.664235  ,\n",
       "        -2.2196343 ,  0.83680505,  1.4650173 , -0.5107319 , -1.9835262 ,\n",
       "         0.372117  ,  0.36646312, -1.724092  , -0.74328583, -0.47174698,\n",
       "        -0.8187597 ,  0.11189617,  0.20853078, -0.5016957 , -0.280536  ,\n",
       "         1.2839919 ,  0.8322274 ,  0.91118157, -1.3032984 ,  0.19316788,\n",
       "        -0.0748798 , -1.22422   ,  1.3572233 ,  1.1264218 , -0.57732904,\n",
       "        -0.35953268,  0.21649563,  0.14786848,  0.55405045, -1.7453936 ,\n",
       "        -0.13212025,  0.0931502 ,  1.3893565 , -0.31791005, -0.9252585 ,\n",
       "        -2.088558  , -0.15548965,  1.2585212 ,  1.6764551 , -1.6103531 ,\n",
       "        -0.27169856, -1.7406347 , -1.4495041 ,  0.03219678,  0.69310796,\n",
       "        -1.1074347 , -0.10590471,  0.9194603 ,  0.4810676 ,  0.30101874,\n",
       "        -0.26924348,  1.138853  , -0.10369212, -1.6543838 , -0.730714  ,\n",
       "        -0.29072487, -0.927184  ,  1.6138453 ,  2.009454  , -0.14794825,\n",
       "        -0.5838763 ,  0.3732367 ,  0.3159087 ,  0.58494675, -1.2384557 ,\n",
       "         0.2834082 ,  0.4662771 ,  0.5281305 ,  0.9320023 , -0.9298176 ,\n",
       "        -1.2601448 , -1.3083268 ,  0.21097623,  0.47912005,  1.2917771 ,\n",
       "         0.3311791 ,  1.7935374 ,  2.561124  , -0.3801755 , -1.2134656 ,\n",
       "         2.096615  ,  1.2069025 , -0.77311426, -0.38988477, -0.05589499]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 創造一個長度是100的隨機亂數向量，就是 DCGAN 的 Generator 要吃的東西\n",
    "noise = tf.random.normal([1, 100])\n",
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7072f",
   "metadata": {},
   "source": [
    "### 有 noise 了，可以馬上拿去丟給你的 Generator 啦！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bebee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image = generator(noise, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7964a303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(generated_image.shape)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(2,2))  # 設定畫布大小，(寬, 高)，單位是英吋\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0d6b5",
   "metadata": {},
   "source": [
    "##  DCGAN的 discriminator（判斷器）模型\n",
    "\n",
    "- Dropout: https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "- Conv2d x Conv2dTranspose: https://stackoverflow.com/questions/68976745/in-keras-what-is-the-difference-between-conv2dtranspose-and-conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc94b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 64)        1664      \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 6273      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 212,865\n",
      "Trainable params: 212,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator():\n",
    "    network = tf.keras.Sequential()\n",
    "    network = tf.keras.Sequential() # 建立一個順序型的神經網路\n",
    "\n",
    "    ##### 第一層卷積層（Conv2D）##### \n",
    "    network.add(layers.Conv2D(filters = 64, strides = (2,2), kernel_size = (5,5), padding = 'same', input_shape = [28,28,1]))\n",
    "    # filters=64：要學 64 張特徵圖（Feature Maps），像是學邊緣、角落這些特徵。\n",
    "    # strides=(2,2)：每次移動 2 格 → 讓圖變小一半（從28x28 ➔ 14x14）。\n",
    "    # kernel_size=(5,5)：用 5x5 大小的小視窗掃過圖片。\n",
    "    # padding='same'：掃完後保持圖片大小一致。\n",
    "    # input_shape=[28,28,1]：輸入的是 28×28 的灰階圖（1個通道）。\n",
    "\n",
    "    network.add(layers.LeakyReLU())\n",
    "    # 讓神經網路學到非線性特徵。\n",
    "    # 比普通的 ReLU 更好，因為負數不會直接死掉，而是有小小斜率。\n",
    "\n",
    "    network.add(layers.Dropout(0.3)) # 隨機丟掉 30% 的神經元，防止過擬合（讓模型更穩定）。\n",
    "\n",
    "\n",
    "    ##### 第二層卷積層（再壓縮一次）##### \n",
    "    network.add(layers.Conv2D(filters = 128, strides = (2,2), kernel_size = (5,5), padding = 'same'))\n",
    "    # 再加一個卷積層，這次學 128 張特徵圖\n",
    "    # 同樣使用 5x5 小視窗，每次移動2格。\n",
    "    # 把 14x14 的特徵圖繼續壓縮成 7x7。    \n",
    "    network.add(layers.LeakyReLU())\n",
    "    network.add(layers.Dropout(0.3)) # 隨機丟掉 30% 的神經元，防止過擬合（讓模型更穩定）。\n",
    "\n",
    "    ##### Flatten（平鋪）##### \n",
    "    network.add(layers.Flatten()) # 把 7x7x128 的資料展平成一條長長的向量，因為下面要接 Dense（全連接層），需要一維資料。\n",
    "\n",
    "\t##### Dense（輸出1個數）##### \n",
    "    network.add(layers.Dense(1))\n",
    "\n",
    "    network.summary()\n",
    "    return network\n",
    "\n",
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ccebf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='conv2d_input'), name='conv2d_input', description=\"created by layer 'conv2d_input'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_1/BiasAdd:0', description=\"created by layer 'dense_1'\")\n"
     ]
    }
   ],
   "source": [
    "print(discriminator.input)\n",
    "print(discriminator.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d1b63ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.00266892]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把生成器做出來的假圖片，拿去丟到判別器\n",
    "# training=False：代表現在是測試/推論，不是訓練\n",
    "discriminator(generated_image, training = False) # 輸出一個數字 => 原始logit，它可以是正的、負的、小的、大的都可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15947b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5001976>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid 函數可以把一個「任意數字」轉成介於【0到1】之間的機率值。\n",
    "# 代表「這張圖是真的的機率」是多少。\n",
    "tf.sigmoid(0.00079025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fed4d8",
   "metadata": {},
   "source": [
    "## Error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91388db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立二元交叉熵損失函數(這個函數用來量化分類結果到底有多錯。)\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# from_logits=True，代表：丟進來的分數（discriminator的輸出）是logit（生分數），不是0~1之間的機率！\n",
    "# 這樣 TensorFlow 會自動先幫你做 sigmoid 再算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6962b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones_like(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135ccf0",
   "metadata": {},
   "source": [
    "### 定義 Discriminator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeb9f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(expected_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(expected_output), expected_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bfab0",
   "metadata": {},
   "source": [
    "### 定義 Generator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e9f5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86038b",
   "metadata": {},
   "source": [
    "### 最後定義 Optimizer\n",
    "- 給 生成器 和 判別器各自一個 Adam Optimizer。\n",
    "- 這樣他們在訓練時，可以自己更新自己的參數。\n",
    "- 設定學習率（learning rate）為 0.00001 ➔ 很小，因為GAN很容易不穩，學太快會爆炸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "397a11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a513c5",
   "metadata": {},
   "source": [
    "# Training the GAN and visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6572272d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ce532b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "noise_dimension = 100\n",
    "number_of_images = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4bd488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # 把下面這個 train 函數「加速」成 TensorFlow 的 計算圖（Graph）。計算圖可以讓 TensorFlow 自動優化，跑得比純 Python 快很多。\n",
    "def train(images):\n",
    "\tnoise = tf.random.normal([batch_size, noise_dimension]) # 產生隨機噪聲\n",
    "\t# 隨機生成一堆亂數，當作「生成器」的輸入。\n",
    "\t# 每個批次有 batch_size 筆、每筆 noise_dimension=100 維的向量。\n",
    "\twith tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: # 開兩個 GradientTape(兩卷錄影帶)\n",
    "\t\t# gen_tape 負責記錄「生成器」的運算過程\n",
    "\t\t# disc_tape 負責記錄「判別器」的運算過程\n",
    "\t\tgenerated_images = generator(noise, training=True) # 生成假圖片\n",
    "\n",
    "\t\t# 判別真假圖片\n",
    "\t\texpected_output = discriminator(images, training=True)\n",
    "\t\tfake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "\t\t# 算 Loss\n",
    "\t\tgen_loss = generator_loss(fake_output) # 生成器的目標是「騙倒判別器」。\n",
    "\t\tdisc_loss = discriminator_loss(expected_output, fake_output) # 判別器的目標是「正確辨識真假」。\n",
    "\n",
    "\t\t# 算梯度\n",
    "\t\t# 分別針對生成器、判別器的 Loss：算出每個參數需要怎麼微調（梯度）。\n",
    "\t\tgenerator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\t\tdiscriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "\n",
    "\t\t# 更新參數\n",
    "\t\t# 把剛剛的梯度套用回網路裡 ➔ 更新網路權重\n",
    "\t\tgenerator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "\t\tdiscriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1742f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:32:04.043789: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,28,28,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2025-04-28 15:32:04.044078: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,28,28,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "# 打開 eager mode\n",
    "# 預設 @tf.function 會把函數編譯成計算圖 (graph)，這樣快但難除錯\n",
    "# 開啟 run_functions_eagerly(True) ➔ 讓 TensorFlow 每一行都即時執行，比較慢，但方便看錯誤訊息（適合開發&debug用）。\n",
    "\n",
    "X_train_batch = X_train.as_numpy_iterator().next()\n",
    "# X_train 本來是一個 tf.data.Dataset（可以一批一批拿資料）。\n",
    "# .as_numpy_iterator()：把它變成一個可以像 Python 迴圈那樣拿資料的物件。\n",
    "# .next()：拿出第一個 batch的資料。\n",
    "\n",
    "train(X_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf9b40ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100)\n"
     ]
    }
   ],
   "source": [
    "# 測試生成器的噪聲：產生 number_of_images 個噪聲向量（每個長度是 noise_dimension，也就是 100 維）。\n",
    "test_images = tf.random.normal([number_of_images, noise_dimension])\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1e017",
   "metadata": {},
   "source": [
    "### DCGAN 的完整訓練總流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08bdf059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(dataset, epochs, test_images):\n",
    "    '''\n",
    "    dataset：訓練資料（手寫數字圖）\n",
    "    epochs：訓練多少回合\n",
    "    test_images：固定的一組噪聲，用來觀察每個 epoch 生成出來的圖進步到哪\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        #print(epoch)\n",
    "        for image_batch in dataset:\n",
    "            #print(image_batch.shape)\n",
    "            train(image_batch)\n",
    "\n",
    "        print('Epoch: ', epoch + 1)\n",
    "        generated_images = generator(test_images, training = False)\n",
    "        fig = plt.figure(figsize = (10,10))\n",
    "        for i in range(generated_images.shape[0]):\n",
    "            plt.subplot(4,4,i + 1)\n",
    "            plt.imshow(generated_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fcd9676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fbb50eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.12.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: /home/kcwc1029/miniconda3/envs/gan/lib/python3.11/site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, jax, keras, libclang, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bbc8dfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_1929/100295713.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_gan(X_train, epochs, test_images)\n",
      "\u001b[32m/tmp/ipykernel_1929/3028605792.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(dataset, epochs, test_images)\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;28;01min\u001b[39;00m range(epochs):\n\u001b[32m      8\u001b[39m         \u001b[38;5;66;03m#print(epoch)\u001b[39;00m\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m image_batch \u001b[38;5;28;01min\u001b[39;00m dataset:\n\u001b[32m     10\u001b[39m             \u001b[38;5;66;03m#print(image_batch.shape)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m             train(image_batch)\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m         print(\u001b[33m'Epoch: '\u001b[39m, epoch + \u001b[32m1\u001b[39m)\n\u001b[32m     14\u001b[39m         generated_images = generator(test_images, training = \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[32m~/miniconda3/envs/gan/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/miniconda3/envs/gan/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    867\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m __call__(self, *args, **kwds):\n\u001b[32m    868\u001b[39m     \u001b[38;5;66;03m# Implements GenericFunction.__call__.\u001b[39;00m\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m self._run_functions_eagerly:\n\u001b[32m    870\u001b[39m       \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(self._name, tf_function_call=\u001b[33m\"eager\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._python_function(*args, **kwds)\n\u001b[32m    872\u001b[39m \n\u001b[32m    873\u001b[39m     \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    874\u001b[39m     \u001b[38;5;66;03m# place.\u001b[39;00m\n",
      "\u001b[32m/tmp/ipykernel_1929/1531717118.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(images)\u001b[39m\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m                 \u001b[38;5;66;03m# 算梯度\u001b[39;00m\n\u001b[32m     20\u001b[39m                 \u001b[38;5;66;03m# 分別針對生成器、判別器的 Loss：算出每個參數需要怎麼微調（梯度）。\u001b[39;00m\n\u001b[32m     21\u001b[39m                 generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m                 discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m                 \u001b[38;5;66;03m# 更新參數\u001b[39;00m\n",
      "\u001b[32m~/miniconda3/envs/gan/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[39m\n\u001b[32m   1059\u001b[39m               output_gradients))\n\u001b[32m   1060\u001b[39m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[32m   1061\u001b[39m                           for x in output_gradients]\n\u001b[32m   1062\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     flat_grad = imperative_grad.imperative_grad(\n\u001b[32m   1064\u001b[39m         self._tape,\n\u001b[32m   1065\u001b[39m         flat_targets,\n\u001b[32m   1066\u001b[39m         flat_sources,\n",
      "\u001b[32m~/miniconda3/envs/gan/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[39m\n\u001b[32m     63\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ValueError:\n\u001b[32m     64\u001b[39m     raise ValueError(\n\u001b[32m     65\u001b[39m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[32m     66\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[32m     68\u001b[39m       tape._tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m     69\u001b[39m       target,\n\u001b[32m     70\u001b[39m       sources,\n",
      "\u001b[32m~/miniconda3/envs/gan/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[39m\n\u001b[32m    142\u001b[39m     gradient_name_scope = \u001b[33m\"gradient_tape/\"\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m forward_pass_name_scope:\n\u001b[32m    144\u001b[39m       gradient_name_scope += forward_pass_name_scope + \u001b[33m\"/\"\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(gradient_name_scope):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n\u001b[32m    147\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n",
      "\u001b[32m~/miniconda3/envs/gan/lib/python3.11/site-packages/tensorflow/python/ops/nn_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(op, grad)\u001b[39m\n\u001b[32m    578\u001b[39m   \u001b[38;5;66;03m# to use the nn_ops functions, we would have to convert `padding` and\u001b[39;00m\n\u001b[32m    579\u001b[39m   \u001b[38;5;66;03m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[39;00m\n\u001b[32m    580\u001b[39m   \u001b[38;5;66;03m# in Eager mode.\u001b[39;00m\n\u001b[32m    581\u001b[39m   return [\n\u001b[32m--> \u001b[39m\u001b[32m582\u001b[39m       gen_nn_ops.conv2d_backprop_input(\n\u001b[32m    583\u001b[39m           shape_0,\n\u001b[32m    584\u001b[39m           op.inputs[\u001b[32m1\u001b[39m],\n\u001b[32m    585\u001b[39m           grad,\n",
      "\u001b[32m~/miniconda3/envs/gan/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[39m\n\u001b[32m   1623\u001b[39m         data_format, \"dilations\", dilations)\n\u001b[32m   1624\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   1625\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1626\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   1628\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1629\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1630\u001b[39m       return conv2d_backprop_input_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_gan(X_train, epochs, test_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
