{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# DeepSeek API KEY\n",
    "import os\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")  # 你的 .env 要存這個 API KEY\n",
    "\n",
    "# 初始化 DeepSeek\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    base_url=\"https://api.deepseek.com\",  \n",
    "    model=\"deepseek-chat\",  \n",
    ")\n",
    "\n",
    "# 建立訊息\n",
    "messages = [\n",
    "    SystemMessage(content=\"將下列的英文翻譯成中文\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "# 呼叫\n",
    "response = model.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c1ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='將下列英文翻譯成 中文', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]\n",
      "嗨 / 你好\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"將下列英文翻譯成 {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"language\": \"中文\", \"text\": \"hi\"})\n",
    "print(prompt)\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f88a43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp310-cp310-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.10.0-cp310-cp310-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 5.2/13.7 MB 39.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 61.3 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef593db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這篇演講的核心內容可精要概述如下：\n",
      "\n",
      "1. **夢想的主軸**  \n",
      "   反覆強調「我有一個夢想」（I have a dream），表達對平等與自由的深切渴望，並以宗教意象（如「神的榮耀顯現」）和自然隱喻（山谷升高、山嶺降低）象徵社會不平等的消弭。\n",
      "\n",
      "2. **美國夢的實踐**  \n",
      "   呼籲國家實現立國精神，引用《獨立宣言》中「人皆生而平等」的信念，期待不同種族（如昔日奴隸與奴隸主的後代）能和解共融，體現「兄弟情誼」。\n",
      "\n",
      "3. **當下與未來的挑戰**  \n",
      "   承認當前困境，但仍以「希望」和「信念」為動力，強調夢想根植於美國夢，需持續奮鬥以達成社會轉型。\n",
      "\n",
      "4. **重複修辭的感染力**  \n",
      "   透過重複句式強化訴求，如三次「I have a dream today」及對「one day」的描繪，凸顯理想的迫切性與普世性。\n",
      "\n",
      "全文結合宗教願景、國家理想與現實批判，傳遞對種族平等與社會正義的堅定呼籲。\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")  # 你的 .env 要存這個 API KEY\n",
    "\n",
    "# 清理文字資料（Data Cleaning）\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # 只留英文字母和空白\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # 把多餘的空白合成一個空白\n",
    "    text = text.lower()                      # 全部轉小寫\n",
    "    return text\n",
    "\n",
    "# 載入文字檔案\n",
    "documents = TextLoader(\"./dream.txt\").load()\n",
    "cleaned_documents = [clean_text(doc.page_content) for doc in documents] # 讀取 dream.txt 這個檔案內容。\n",
    "\n",
    "# 切割文字成小段（分段）\n",
    "# chunk_size=500：每段最多500個字     \n",
    "# chunk_overlap=100：前後段落重疊100個字。\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100) \n",
    "texts = text_splitter.split_documents(documents)\n",
    "texts = [clean_text(text.page_content) for text in texts]\n",
    "\n",
    "# 把小段轉成向量 Embedding\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 建立檢索器 Retriever\n",
    "retriever = Chroma.from_texts(texts, embeddings).as_retriever()\n",
    "\n",
    "# 查詢 retriever\n",
    "query = \"請以精要點概述演講內容\"\n",
    "docs = retriever.invoke(query) # 最有可能回答你問題的文件段落集合\n",
    "# print(docs)\n",
    "\n",
    "# Chat with the model and our docs\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Please use the following docs {docs},and answer the following question {query}\",\n",
    ")\n",
    "\n",
    "\n",
    "# 初始化 DeepSeek\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    base_url=\"https://api.deepseek.com\",  # DeepSeek官方API URL\n",
    "    model=\"deepseek-chat\",  # 也可以是 deepseek-coder\n",
    ")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"docs\": docs, \"query\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35cc2e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1:\n",
      "page_content='And so even though we face the difficulties of today and tomorrow, I still have a dream. It is a' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 2:\n",
      "page_content='a dream. It is a dream deeply rooted in the American dream.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 3:\n",
      "page_content='I have a dream that one day this nation will rise up and live out the true meaning of its creed:' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 4:\n",
      "page_content='We hold these truths to be self-evident, that all men are created equal.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 5:\n",
      "page_content='I have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 6:\n",
      "page_content='and the sons of former slave owners will be able to sit down together at the table of brotherhood.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 7:\n",
      "page_content='I have a dream that one day even the state of Mississippi, a state sweltering with the heat of' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 8:\n",
      "page_content='with the heat of injustice, sweltering with the heat of oppression, will be transformed into an' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 9:\n",
      "page_content='transformed into an oasis of freedom and justice.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 10:\n",
      "page_content='I have a dream that my four little children will one day live in a nation where they will not be' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 11:\n",
      "page_content='they will not be judged by the color of their skin but by the content of their character.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 12:\n",
      "page_content='I have a dream today!' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 13:\n",
      "page_content='I have a dream that one day, down in Alabama, with its vicious racists, with its governor having' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 14:\n",
      "page_content='its governor having his lips dripping with the words of interposition and nullification, one day' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 15:\n",
      "page_content='one day right there in Alabama little black boys and black girls will be able to join hands with' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 16:\n",
      "page_content='to join hands with little white boys and white girls as sisters and brothers.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 17:\n",
      "page_content='I have a dream today!' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 18:\n",
      "page_content='I have a dream that one day every valley shall be exalted, and every hill and mountain shall be' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 19:\n",
      "page_content='mountain shall be made low, the rough places will be made plain, and the crooked places will be' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 20:\n",
      "page_content='places will be made straight; and the glory of the Lord shall be revealed and all flesh shall see' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 21:\n",
      "page_content='all flesh shall see it together.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 22:\n",
      "page_content='This is our hope, and this is the faith that I go back to the South with.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 23:\n",
      "page_content='With this faith, we will be able to hew out of the mountain of despair a stone of hope. With this' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 24:\n",
      "page_content='of hope. With this faith, we will be able to transform the jangling discords of our nation into a' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 25:\n",
      "page_content='our nation into a beautiful symphony of brotherhood. With this faith, we will be able to work' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 26:\n",
      "page_content='be able to work together, to pray together, to struggle together, to go to jail together, to stand' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 27:\n",
      "page_content='together, to stand up for freedom together, knowing that we will be free one day.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 28:\n",
      "page_content='And this will be the day, this will be the day when all of God s children will be able to sing with' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 29:\n",
      "page_content='able to sing with new meaning:' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 30:\n",
      "page_content='My country  tis of thee, sweet land of liberty, of thee I sing.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 31:\n",
      "page_content='Land where my fathers died, land of the Pilgrim s pride,\n",
      "From every mountainside, let freedom ring!' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 32:\n",
      "page_content='And if America is to be a great nation, this must become true.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 33:\n",
      "page_content='And so let freedom ring from the prodigious hilltops of New Hampshire.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 34:\n",
      "page_content='Let freedom ring from the mighty mountains of New York.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 35:\n",
      "page_content='Let freedom ring from the heightening Alleghenies of Pennsylvania.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 36:\n",
      "page_content='Let freedom ring from the snow-capped Rockies of Colorado.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 37:\n",
      "page_content='Let freedom ring from the curvaceous slopes of California.\n",
      " \n",
      "But not only that:' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 38:\n",
      "page_content='But not only that:\n",
      "Let freedom ring from Stone Mountain of Georgia.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 39:\n",
      "page_content='Let freedom ring from Lookout Mountain of Tennessee.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 40:\n",
      "page_content='Let freedom ring from every hill and molehill of Mississippi.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 41:\n",
      "page_content='From every mountainside, let freedom ring.' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 42:\n",
      "page_content='And when this happens, when we allow freedom ring, when we let it ring from every village and every' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 43:\n",
      "page_content='village and every hamlet, from every state and every city, we will be able to speed up that day' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 44:\n",
      "page_content='speed up that day when all of God s children, black men and white men, Jews and Gentiles,' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 45:\n",
      "page_content='Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 46:\n",
      "page_content='in the words of the old Negro spiritual:' metadata={'source': './dream.txt'}\n",
      "\n",
      "Split 47:\n",
      "page_content='Free at last! Free at last!\n",
      " \n",
      "Thank God Almighty, we are free at last!' metadata={'source': './dream.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load your document \n",
    "text_loader = TextLoader(\"./dream.txt\")  \n",
    "documents = text_loader.load()  \n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(documents)\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"Split {i+1}:\\n{split}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799070b7",
   "metadata": {},
   "source": [
    "LangChain DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdfaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Text Documents: [Document(metadata={'source': 'data\\\\dream.txt'}, page_content='And so even though we face the difficulties of today and tomorrow, I still have a dream. It is a dream deeply rooted in the American dream.\\n \\nI have a dream that one day this nation will rise up and live out the true meaning of its creed:\\n \\nWe hold these truths to be self-evident, that all men are created equal.\\n \\nI have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood.\\n \\nI have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice.\\n \\nI have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.\\n \\nI have a dream today!\\n \\nI have a dream that one day, down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of interposition and nullification, one day right there in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers.\\n \\nI have a dream today!\\n \\nI have a dream that one day every valley shall be exalted, and every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight; and the glory of the Lord shall be revealed and all flesh shall see it together.\\n \\nThis is our hope, and this is the faith that I go back to the South with.\\n \\nWith this faith, we will be able to hew out of the mountain of despair a stone of hope. With this faith, we will be able to transform the jangling discords of our nation into a beautiful symphony of brotherhood. With this faith, we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.\\n \\nAnd this will be the day, this will be the day when all of God s children will be able to sing with new meaning:\\n \\nMy country  tis of thee, sweet land of liberty, of thee I sing.\\nLand where my fathers died, land of the Pilgrim s pride,\\nFrom every mountainside, let freedom ring!\\nAnd if America is to be a great nation, this must become true.\\nAnd so let freedom ring from the prodigious hilltops of New Hampshire.\\nLet freedom ring from the mighty mountains of New York.\\nLet freedom ring from the heightening Alleghenies of Pennsylvania.\\nLet freedom ring from the snow-capped Rockies of Colorado.\\nLet freedom ring from the curvaceous slopes of California.\\n \\nBut not only that:\\nLet freedom ring from Stone Mountain of Georgia.\\nLet freedom ring from Lookout Mountain of Tennessee.\\nLet freedom ring from every hill and molehill of Mississippi.\\nFrom every mountainside, let freedom ring.\\nAnd when this happens, when we allow freedom ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God s children, black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual:\\nFree at last! Free at last!\\n \\nThank God Almighty, we are free at last!'), Document(metadata={'source': 'data\\\\gettysburg.txt'}, page_content='Four score and seven years ago our fathers brought forth, on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\\n\\nNow we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.\\n\\nBut, in a larger sense, we can not dedicate—we can not consecrate—we can not hallow—this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us—that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in vain—that this nation, under God, shall have a new birth of freedom—and that government of the people, by the people, for the people, shall not perish from the earth.\\n\\nAbraham Lincoln\\n\\nNovember 19, 1863.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader, DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\"./data/\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "dir_documents = dir_loader.load()\n",
    "\n",
    "print(\"Directory Text Documents:\", dir_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89464f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Downloading lxml-5.3.2-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (4.12.3)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 981.5/981.5 kB 11.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.13.0-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: backoff in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.33.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (1.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (4.67.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured) (6.1.0)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured)\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from dataclasses-json->unstructured) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from html5lib->unstructured) (1.16.0)\n",
      "Collecting webencodings (from html5lib->unstructured)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: click in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from requests->unstructured) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from requests->unstructured) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured-client->unstructured) (43.0.3)\n",
      "Collecting eval-type-backport>=0.2.0 (from unstructured-client->unstructured)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured-client->unstructured) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Collecting pydantic>=2.11.2 (from unstructured-client->unstructured)\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Collecting typing-inspection>=0.4.0 (from unstructured-client->unstructured)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.6.0)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic>=2.11.2->unstructured-client->unstructured)\n",
      "  Downloading pydantic_core-2.33.1-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.21)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\33313\\.conda\\envs\\openai\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.2)\n",
      "Downloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 48.7 MB/s eta 0:00:00\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 590.6/590.6 kB ? eta 0:00:00\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading lxml-5.3.2-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 75.4 MB/s eta 0:00:00\n",
      "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 85.1 MB/s eta 0:00:00\n",
      "Downloading unstructured_client-0.33.0-py3-none-any.whl (181 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 112.7 MB/s eta 0:00:00\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=85c414442e922792156429c283735f3880770fabd78c78c476412aba4fdcc8f8\n",
      "  Stored in directory: c:\\users\\33313\\appdata\\local\\pip\\cache\\wheels\\95\\03\\7d\\59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "Successfully built langdetect\n",
      "Installing collected packages: webencodings, filetype, typing-inspection, rapidfuzz, python-magic, python-iso639, pypdf, pydantic-core, olefile, lxml, langdetect, html5lib, eval-type-backport, emoji, chardet, aiofiles, python-oxmsg, pydantic, unstructured-client, unstructured\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.27.0\n",
      "    Uninstalling pydantic_core-2.27.0:\n",
      "      Successfully uninstalled pydantic_core-2.27.0\n",
      "  Attempting uninstall: aiofiles\n",
      "    Found existing installation: aiofiles 22.1.0\n",
      "    Uninstalling aiofiles-22.1.0:\n",
      "      Successfully uninstalled aiofiles-22.1.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.0\n",
      "    Uninstalling pydantic-2.10.0:\n",
      "      Successfully uninstalled pydantic-2.10.0\n",
      "Successfully installed aiofiles-23.2.1 chardet-5.2.0 emoji-2.14.1 eval-type-backport-0.2.2 filetype-1.2.0 html5lib-1.1 langdetect-1.0.9 lxml-5.3.2 olefile-0.47 pydantic-2.10.3 pydantic-core-2.27.1 pypdf-5.4.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 typing-inspection-0.4.0 unstructured-0.17.2 unstructured-client-0.33.0 webencodings-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 5.15.0 requires aiofiles<24.0,>=22.0, but you have aiofiles 24.1.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78978b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Documents: [Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 0}, page_content='DeepSeek-Coder: When the Large Language Model Meets\\nProgramming - The Rise of Code Intelligence\\nDaya Guo*1, Qihao Zhu∗1,2, Dejian Yang1, Zhenda Xie1, Kai Dong1, Wentao Zhang1\\nGuanting Chen1, Xiao Bi 1, Y. Wu1, Y.K. Li1, Fuli Luo1, Yingfei Xiong2, Wenfeng Liang1\\n1DeepSeek-AI\\n2Key Lab of HCST (PKU), MOE; SCS, Peking University\\n{zhuqh, guodaya}@deepseek.com\\nhttps://github.com/deepseek-ai/DeepSeek-Coder\\nAbstract\\nThe rapid development of large language models has revolutionized code intelligence in\\nsoftware development. However, the predominance of closed-source models has restricted\\nextensive research and development. To address this, we introduce the DeepSeek-Coder series,\\na range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2\\ntrillion tokens. These models are pre-trained on a high-quality project-level code corpus and\\nemploy a fill-in-the-blank task with a 16K window to enhance code generation and infilling.\\nOur extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art\\nperformance among open-source code models across multiple benchmarks but also surpasses\\nexisting closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\\nare under a permissive license that allows for both research and unrestricted commercial use.\\nFigure 1 |The Performance of DeepSeek-Coder\\n*Core contributors, ordered alphabetically by the name.\\narXiv:2401.14196v2  [cs.SE]  26 Jan 2024'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 1}, page_content='1. Introduction\\nThe field of software development has been significantly transformed by the swift advancement\\nof large language models (OpenAI, 2023; Touvron et al., 2023), which have brought about\\na new era of code intelligence. These models have the potential to automate and streamline\\nmany aspects of coding, from bug detection to code generation, thereby enhancing productivity\\nand reducing the likelihood of human error. However, a major challenge in this field is the\\nperformance gap between open-source models (Li et al., 2023; Nijkamp et al., 2022; Roziere\\net al., 2023; Wang et al., 2021) and closed-source models (Gemini Team, 2023; OpenAI, 2023).\\nThe giant closed-source models, while powerful, are often inaccessible to many researchers and\\ndevelopers due to their proprietary nature.\\nIn response to this challenge, we present the DeepSeek-Coder series. This series comprises\\na range of open-source code models, varying in size from 1.3B to 33B, including the base\\nversion and instructed version for each size. Each model in the series has been trained from\\nscratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive\\nunderstanding of coding languages and syntax. Besides, we attempt to organize the pre-\\ntraining data at the repository level to enhance the pre-trained model’s understanding capability\\nwithin the context of cross-files within a repository. In addition to employing the next token\\nprediction loss during pre-training, we have also incorporated the Fill-In-Middle (FIM) approach\\n(Bavarian et al., 2022; Li et al., 2023). This approach is designed to further bolster the model’s\\ncode completion capabilities. To meet the requirements of handling longer code inputs, we\\nhave extended the context length to 16K. This adjustment allows our models to handle more\\ncomplex and extensive coding tasks, thereby increasing their versatility and applicability in\\nvarious coding scenarios.\\nWe have carried out comprehensive experiments using a variety of public code-related\\nbenchmarks. The findings reveal that among open-source models, DeepSeek-Coder-Base 33B\\nconsistently delivers superior performance across all benchmarks. Furthermore, DeepSeek-\\nCoder-Instruct 33B surpassesOpenAI GPT-3.5 Turboin the majority of the evaluation benchmarks,\\nsignificantly narrowing the performance gap between OpenAI GPT-4 and open-source models.\\nRemarkably, despite having fewer parameters, DeepSeek-Coder-Base 7B demonstrates competi-\\ntive performance when compared to models that are five times larger, such as CodeLlama-33B\\n(Roziere et al., 2023). To summarize, our main contributions are:\\n• We introduce DeepSeek-Coder-Base and DeepSeek-Coder-Instruct, our advanced code-\\nfocused large language models (LLMs). Developed through extensive training on an\\nexpansive code corpus, these models exhibit proficiency in understanding 87 programming\\nlanguages. Additionally, they are available in various model scales to cater to a wide range\\nof computational and application needs.\\n• We make the first attempt to incorporate repository-level data construction during the\\npre-training phase of our models. We find that it can significantly boost the capability of\\ncross-file code generation.\\n• Our analysis rigorously examines the impact of FIM training strategies on the pretraining\\nphase of code models. The outcomes of these comprehensive studies shed light on intriguing\\naspects of FIM configurations, offering valuable insights that significantly contribute to the\\nenhancement and development of code pretrained models.\\n• We conduct extensive evaluations of our code LLMs against a wide array of benchmarks en-\\ncompassing numerous code-related tasks. The findings demonstrate that DeepSeek-Coder-\\nBase surpasses all existing open-source code LLMs across these benchmarks. Furthermore,\\n2'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 2}, page_content='with meticulous fine-tuning using instructional data, DeepSeek-Coder-Instruct achieves\\nbetter performance compared to the OpenAI GPT-3.5 Turbomodel in code-related tasks.\\n2. Data Collection\\nThe training dataset of DeepSeek-Coder is composed of 87% source code, 10% English code-\\nrelated natural language corpus, and 3% code-unrelated Chinese natural language corpus. The\\nEnglish corpus consists of materials from GitHub’s Markdown and StackExchange1, which are\\nused to enhance the model’s understanding of code-related concepts and improve its ability\\nto handle tasks like library usage and bug fixing. Meanwhile, the Chinese corpus consists of\\nhigh-quality articles aimed at improving the model’s proficiency in understanding the Chinese\\nlanguage. In this section, we will provide an overview of how we construct the code training\\ndata. This process involves data crawling, rule-based filtering, dependency parsing, repository-\\nlevel deduplication, and quality screening, as illustrated in Figure 2. In the following, we will\\ndescribe the data creation procedure step by step.\\nRule\\t\\nFiltering\\nDependency\\t\\nParsing\\nRepo-level\\t\\nDeduplication\\nQuality\\t\\nScreening\\nData\\t\\nCrawling\\nFigure 2 |The Procedure of Dataset Creation\\n2.1. GitHub Data Crawling and Filtering\\nWe collect public repositories created before February 2023 on GitHub and retain only 87\\nprogramming languages, as listed in Table 1. To reduce the amount of data to be processed, we\\napply filtering rules similar to those used in the StarCoder project (Li et al., 2023) to preliminarily\\nfilter out lower-quality code. By applying these filtering rules, we reduce the total amount of\\ndata to only 32.8% of its original size. To make the paper self-contained, we briefly describe the\\nfilter rules used in the StarCoder Data project:\\nFirstly, we filter out files with an average line length exceeding 100 characters or a maximum\\nline length surpassing 1000 characters. Additionally, we remove files with fewer than 25%\\nalphabetic characters. Except for the XSLT programming language, we further filter out files\\nwhere the string \"<?xml version=\" appeared in the first 100 characters. For HTML files, we\\nconsider the ratio of visible text to HTML code. We retain files where the visible text constitutes\\nat least 20% of the code and is no less than 100 characters. For JSON and YAML files, which\\ntypically contain more data, we only keep files that have a character count ranging from 50 to\\n5000 characters. This effectively removes most data-heavy files.\\n2.2. Dependency Parsing\\nIn previous works (Chen et al., 2021; Li et al., 2023; Nijkamp et al., 2022; Roziere et al., 2023),\\nlarge language models for code are mainly pre-trained on file-level source code, which ignores\\nthe dependencies between different files in a project. However, in practical applications, such\\nmodels struggle to effectively scale to handle entire project-level code scenarios. Therefore, we\\n1https://stackexchange.com\\n3'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 3}, page_content='Algorithm 1Topological Sort for Dependency Analysis\\n1: procedure T O P O L O G I C A LS O R T( 𝑓𝑖𝑙𝑒𝑠)\\n2: 𝑔𝑟𝑎𝑝ℎ𝑠 ←{} ⊲Initialize an empty adjacency list\\n3: 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒 ←{} ⊲Initialize an empty dictionary for in-degrees\\n4: for each𝑓𝑖𝑙𝑒 in 𝑓𝑖𝑙𝑒𝑠 do\\n5: 𝑔𝑟𝑎𝑝ℎ𝑠[𝑓𝑖𝑙𝑒]←[]\\n6: 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒[𝑓𝑖𝑙𝑒]← 0\\n7: end for\\n8:\\n9: for each𝑓𝑖𝑙𝑒𝐴 in 𝑓𝑖𝑙𝑒𝑠 do\\n10: for each𝑓𝑖𝑙𝑒𝐵 in 𝑓𝑖𝑙𝑒𝑠 do\\n11: if H A SD E P E N D E N C Y( 𝑓𝑖𝑙𝑒𝐴 , 𝑓𝑖𝑙𝑒𝐵) then ⊲If fileA depends on fileB\\n12: 𝑔𝑟𝑎𝑝ℎ𝑠[𝑓𝑖𝑙𝑒𝐵].append(𝑓𝑖𝑙𝑒𝐴 ) ⊲Add edge from B to A\\n13: 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒[𝑓𝑖𝑙𝑒𝐴 ]← 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒[𝑓𝑖𝑙𝑒𝐴 ]+1 ⊲Increment in-degree of A\\n14: end if\\n15: end for\\n16: end for\\n17:\\n18: 𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ𝑠 ←getDisconnectedSubgraphs(𝑔𝑟𝑎𝑝ℎ𝑠) ⊲Identify disconnected subgraphs\\n19: 𝑎𝑙𝑙𝑅𝑒𝑠𝑢𝑙𝑡𝑠 ←[]\\n20: for each𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ in 𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ𝑠 do\\n21: 𝑟𝑒𝑠𝑢𝑙𝑡𝑠 ←[]\\n22: while length(𝑟𝑒𝑠𝑢𝑙𝑡𝑠)≠ NumberOfNodes(𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ)do\\n23: 𝑓𝑖𝑙𝑒 ←argmin({𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒[𝑓𝑖𝑙𝑒]| 𝑓𝑖𝑙𝑒 ∈𝑠𝑢𝑏𝑔𝑟𝑎𝑝ℎ and 𝑓𝑖𝑙𝑒 ∉ 𝑟𝑒𝑠𝑢𝑙𝑡𝑠})\\n24: for each𝑛𝑜𝑑𝑒 in 𝑔𝑟𝑎𝑝ℎ𝑠[𝑓𝑖𝑙𝑒]do\\n25: 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒[𝑛𝑜𝑑𝑒]← 𝑖𝑛𝐷𝑒𝑔𝑟𝑒𝑒[𝑛𝑜𝑑𝑒]−1\\n26: end for\\n27: 𝑟𝑒𝑠𝑢𝑙𝑡𝑠.append(𝑓𝑖𝑙𝑒)\\n28: end while\\n29: 𝑎𝑙𝑙𝑅𝑒𝑠𝑢𝑙𝑡𝑠 .append(𝑟𝑒𝑠𝑢𝑙𝑡𝑠)\\n30: end for\\n31:\\n32: return 𝑎𝑙𝑙𝑅𝑒𝑠𝑢𝑙𝑡𝑠\\n33: end procedure\\nwill consider how to leverage the dependencies between files within the same repository in\\nthis step. Specifically, we first parse the dependencies between files and then arrange these\\nfiles in an order that ensures the context each file relies on is placed before that file in the\\ninput sequence. By aligning the files in accordance with their dependencies, our dataset more\\naccurately represents real coding practices and structures. This enhanced alignment not only\\nmakes our dataset more relevant but also potentially increases the practicality and applicability\\nof the model in handling project-level code scenarios. It’s worth noting that we only consider\\nthe invocation relationships between files and use regular expressions to extract them, such as\\n\"import\" in Python, \"using\" in C#, and \"include\" in C.\\nThe algorithm 1 describes a topological sort for dependency analysis on a list of files within\\nthe same project. Initially, it sets up two data structures: an empty adjacency list named\"graphs\"\\nto represent dependencies between files and an empty dictionary called \"inDegree\" for storing\\nthe in-degrees of each file. The algorithm then iterates over each file pair to identify depen-\\n4'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 4}, page_content='dencies, updating \"graphs\" and \"inDegree\" accordingly. Next, it identifies any disconnected\\nsubgraphs within the overall dependency graph. For each subgraph, the algorithm employs a\\nmodified topological sort. Unlike the standard approach that selects nodes with zero in-degrees,\\nthis algorithm selects nodes with minimal in-degrees, which allows it to handle cycles within\\nthe graph. Selected nodes are added to a \"results\" list, and the in-degrees of their connected\\nnodes are decreased. This process continues until a topologically sorted sequence is generated\\nfor each subgraph. The algorithm concludes by returning a list of these sorted sequences, and\\neach sequence’s files are concatenated to form a single training sample. To incorporate file path\\ninformation, a comment indicating the file’s path is added at the beginning of each file. This\\nmethod ensures that the path information is preserved in the training data.\\n2.3. Repo-Level Deduplication\\nRecent studies have demonstrated the significant performance improvements that can be\\nachieved by deduplicating training datasets for Large Language Models (LLMs). Lee et al. (2022)\\nhave shown that language model training corpora often contain numerous near-duplicates, and\\nthe performance of LLMs can be enhanced by removing long repetitive substrings. Kocetkov\\net al. (2022) have applied a near-deduplication method to training data, resulting in dramatic\\nimprovements, and they emphasize that near-deduplication is a crucial preprocessing step for\\nachieving competitive performance on code benchmark tasks. In our dataset, we have also\\nemployed near-deduplication. However, there is a distinction in our approach compared to\\nprevious works. We perform deduplication at the repository level of code, rather than at the file\\nlevel, as the latter approach may filter out certain files within a repository, potentially disrupting\\nthe structure of the repository. Specifically, we treat the concatenated code from the repository\\nlevel as a single sample and apply the same near-deduplication algorithm to ensure the integrity\\nof the repository structure.\\n2.4. Quality Screening and Decontamination\\nIn addition to applying the filtering rules mentioned in Section 2.1, we also employ a compiler\\nand a quality model, combined with heuristic rules, to further filter out low-quality data. This\\nincludes code with syntax errors, poor readability, and low modularity. We provide the statistical\\nsummary of source code in Table 1, which includes a total of 87 languages, detailing the disk\\nsize, number of files, and percentage for each language. The total data volume is 798 GB with\\n603 million files. To ensure that our code training data is not contaminated by information\\nfrom the test set, which may be present on GitHub, we’ve implemented an n-gram filtering\\nprocess. This process involves the removal of any code segments that match specific criteria.\\nSpecifically, we filter out files containing docstrings, questions, and solutions from sources such\\nas HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), GSM8K (Cobbe et al., 2021) and\\nMATH (Hendrycks et al., 2021). For the filtering criteria, we apply the following rules: if a\\npiece of code includes a 10-gram string identical to any in the test data, it is excluded from our\\ntraining data. In cases where the test data comprises strings that are shorter than 10-grams but\\nno less than 3-grams, we use an exact match approach for filtering.\\n5'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 5}, page_content='Language Size (GB) Files (k) Prop. (%) Language Size (GB) Files (k) Prop. (%)\\nAda 0.91 126 0.11 Literate Haskell 0.16 20 0.02\\nAgda 0.26 59 0.03 Lua 0.82 138 0.10\\nAlloy 0.07 24 0.01 Makefile 0.92 460 0.12\\nANTLR 0.19 38 0.02 Maple 0.03 6 0.00\\nAppleScript 0.03 17 0.00 Mathematica 0.82 10 0.10\\nAssembly 0.91 794 0.11 MATLAB 0.01 1 0.00\\nAugeas 0.00 1 0.00 OCaml 0.91 139 0.11\\nAWK 0.09 53 0.01 Pascal 0.79 470 0.10\\nBatchfile 0.92 859 0.12 Perl 0.81 148 0.10\\nBluespec 0.10 15 0.01 PHP 58.92 40,627 7.38\\nC 28.64 27,111 3.59 PowerShell 0.91 236 0.11\\nC# 58.56 53,739 7.34 Prolog 0.03 5 0.00\\nClojure 0.90 295 0.11 Protocol Buffer 0.92 391 0.12\\nCMake 0.90 359 0.11 Python 120.68 75,188 15.12\\nCoffeeScript 0.92 361 0.12 R 0.92 158 0.11\\nCommon Lisp 0.92 105 0.11 Racket 0.09 13 0.01\\nC++ 90.87 36,006 11.39 RMarkdown 6.83 1,606 0.86\\nCSS 5.63 11,638 0.71 Ruby 15.01 18,526 1.88\\nCUDA 0.91 115 0.11 Rust 0.61 692 0.08\\nDart 0.89 264 0.11 SAS 0.92 70 0.11\\nDockerfile 0.04 48 0.00 Scala 0.81 971 0.10\\nElixir 0.91 549 0.11 Scheme 0.92 216 0.12\\nElm 0.92 232 0.12 Shell 13.92 10,890 1.74\\nEmacs Lisp 0.91 148 0.11 Smalltalk 0.92 880 0.12\\nErlang 0.92 145 0.12 Solidity 0.85 83 0.11\\nF# 0.91 340 0.11 Sparql 0.10 88 0.01\\nFortran 1.67 654 0.21 SQL 15.14 7,009 1.90\\nGLSL 0.92 296 0.11 Stan 0.20 41 0.03\\nGo 2.58 1,365 0.32 Standard ML 0.74 117 0.09\\nGroovy 0.89 340 0.11 Stata 0.91 122 0.11\\nHaskell 0.87 213 0.11 SystemVerilog 0.91 165 0.11\\nHTML 30.05 14,998 3.77 TCL 0.90 110 0.11\\nIdris 0.11 32 0.01 Tcsh 0.17 53 0.02\\nIsabelle 0.74 39 0.09 Tex 20.46 2,867 2.56\\nJava 148.66 134,367 18.63 Thrift 0.05 21 0.01\\nJava Server Pages 0.86 1072 0.11 TypeScript 60.62 62,432 7.60\\nJavaScript 53.84 71,895 6.75 Verilog 0.01 1 0.00\\nJSON 4.61 11956 0.58 VHDL 0.85 392 0.11\\nJulia 0.92 202 0.12 Visual Basic 0.75 73 0.09\\nJupyter Notebook 14.38 2,555 1.80 XSLT 0.36 48 0.04\\nKotlin 6.00 3,121 0.75 Yacc 0.72 67 0.09\\nLean 0.52 68 0.07 YAML 0.74 890 0.09\\nLiterate Agda 0.05 4 0.01 Zig 0.81 70 0.10\\nLiterate CoffeeScript 0.01 3 0.00 Total 797.92 603,173 100.00\\nTable 1 |A summary of the cleaned training data for the selected programming languages.\\n3. Training Policy\\n3.1. Training Strategy\\n3.1.1. Next Token Prediction\\nThe first training objective for our model is known as next token prediction . In this process,\\nvarious files are concatenated to form a fixed-length entry. Then, these entries are used to train\\nthe model, enabling it to predict the subsequent token based on the provided context.\\n6'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 6}, page_content='3.1.2. Fill-in-the-Middle\\nThe second training objective for our model is known asfill-in-the-middle. In the code pre-training\\nscenario, it is often necessary to generate corresponding inserted content based on the given\\ncontext and subsequent text. Due to specific dependencies in a programming language, relying\\nsolely on next token prediction is insufficient to learn this fill-in-the-middle capability. Therefore,\\nseveral approaches (Bavarian et al., 2022; Li et al., 2023) propose the pretraining method of\\nFill-in-the-Midlle (FIM). This approach involves randomly dividing the text into three parts,\\nthen shuffling the order of these parts and connecting them with special characters. This method\\naims to incorporate a fill-in-the-blank pretraining task during the training process. Within the\\nFIM methodology, two distinct modes are employed: PSM (Prefix-Suffix-Middle) and SPM\\n(Suffix-Prefix-Middle). In the PSM mode, the training corpus is organized in the sequence\\nof 𝑃𝑟𝑒𝑓𝑖𝑥 , 𝑆𝑢𝑓𝑓𝑖𝑥 , 𝑀𝑖𝑑𝑑𝑙𝑒, aligning the text in a way that the middle segment is flanked by the\\nprefix and suffix. Conversely, the SPM mode arranges the segments as 𝑆𝑢𝑓𝑓𝑖𝑥 , 𝑃𝑟𝑒𝑓𝑖𝑥 , 𝑀𝑖𝑑𝑑𝑙𝑒,\\npresenting a different structural challenge. These modes are instrumental in enhancing the\\nmodel’s capability to handle various structural arrangements in code, providing a robust training\\nframework for advanced code prediction tasks.\\n0 2000 4000 6000 8000\\nSteps\\n0.000\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\n0.150\\n0.175\\n0.200Value\\nBenchmark/HumanEval-Pass@1\\n0 2000 4000 6000 8000\\nSteps\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7Value\\nBenchmark/HumanFIM-Pass@1\\n0 2000 4000 6000 8000\\nSteps\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25Value\\nBenchmark/MBPP-Pass@1\\nfim_0\\nfim_0.5\\nfim_1.0\\nmsp_0.5\\nFigure 3 |The effectiveness of using FIM objective.\\nTo determine the effectiveness of various hyperparameters within the FIM approach, we\\nconducted a series of ablation experiments.\\nExperiment Settings:In this experiment, we employ DeepSeek-Coder-Base 1.3B as our model\\narchitecture. We focused on a Python subset from our training dataset to streamline the ex-\\nperimental process. Our primary objective was to assess the efficacy of the Fill-in-the-Middle\\n(FIM) technique, utilizing the HumanEval-FIM benchmark (Fried et al., 2022). This benchmark\\nspecializes in a single-line FIM task for Python, in which one line of code from a HumanEval\\nsolution is randomly obscured, testing the model’s proficiency in predicting the missing line.\\nWe hypothesize that the PSM mode may exhibit subtle differences compared to the traditional\\nnext-token prediction objective. This is primarily because PSM involves rearranging the order\\nof the original text, potentially impacting the learning dynamics of the model. Therefore, we\\nimplement the PSM mode for FIM across four distinct configurations: 0% FIM rate, 50% FIM\\nrate, 100% FIM rate, and 50% MSP rate. The Masked Span Prediction (MSP) strategy, initially\\nintroduced in T5 (Raffel et al., 2023), conceals multiple text spans and trains the model to recon-\\nstruct these segments. According to CodeGen2.5 (Nijkamp et al., 2023), MSP may enhance FIM\\nperformance compared to PSM. Thus, we include this method in our comparative analysis.\\nResults: The outcomes of our experiment are illustrated in Figure 3. While the model demon-\\nstrates peak performance on the HumanEval-FIM with a 100% FIM rate, this configuration also\\nresults in the weakest code completion capability. This indicates a trade-off between FIM and\\n7'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 7}, page_content='code completion abilities. Moreover, we observe that with a 50% PSM rate, the model outper-\\nforms the MSP strategy. To achieve a balance between FIM efficiency and code completion\\nproficiency, we ultimately choose the 50% PSM rate as our preferred training policy.\\nIn our implementation, we have introduced three sentinel tokens specifically for this task.\\nFor each code file, we initially divide its content into three segments, denoted as 𝑓𝑝𝑟𝑒, 𝑓𝑚𝑖𝑑𝑑𝑙𝑒, and\\n𝑓𝑠𝑢𝑓 . Using the PSM mode, we construct the training example as follows:\\n<｜fim_start｜> 𝑓𝑝𝑟𝑒<｜fim_hole｜> 𝑓𝑠𝑢𝑓 <｜fim_end｜> 𝑓𝑚𝑖𝑑𝑑𝑙𝑒<|eos_token|>\\nWe implement the Fill-in-the-Middle (FIM) method at the document level before the packing\\nprocess, as proposed in the original work by Bavarian et al. (2022). This is done with an FIM\\nrate of 0.5, following the PSM mode.\\n3.2. Tokenizer\\nFor the tokenization process, we employ the HuggingFace Tokenizer library2 to train Byte Pair\\nEncoding (BPE) tokenizers, as outlined in Sennrich et al. (2015) (Sennrich et al., 2015), on a\\nsubset of our training corpus. Ultimately, we utilize a tokenizer configured with a vocabulary\\nsize of 32,000.\\n3.3. Model Architecture\\nWe develop a range of models with varying parameters to cater to diverse applications, including\\nmodels with 1.3B, 6.7B, and 33B parameters. These models are built upon the same framework\\nas the DeepSeek Large Language Model (LLM) outlined by DeepSeek-AI (2024). Each model is a\\ndecoder-only Transformer, incorporating Rotary Position Embedding (RoPE) as described by Su\\net al. (2023). Notably, the DeepSeek 33B model integrates Grouped-Query-Attention (GQA) with\\na group size of 8, enhancing both training and inference efficiency. Additionally, we employ\\nFlashAttention v2 (Dao, 2023) to expedite the computation involved in the attention mechanism.\\nThe architectural details of our models are summarized in Table 2.\\n3.4. Optimization\\nFollowing DeepSeek LLM (DeepSeek-AI, 2024), we use AdamW (Loshchilov and Hutter, 2019)\\nas the optimizer with 𝛽1 and 𝛽2 values of 0.9 and 0.95. We adapt batch sizes and learning rates by\\nthe scaling laws suggested in DeepSeek LLM. For the learning rate scheduling, we implement a\\nthree-stage policy, which includes 2000 warm-up steps, and set the final learning rate to 10% of\\nthe initial rate. Notably, the learning rate at each stage is scaled down to\\n√︃\\n1\\n10 of the preceding\\nstage’s rate, following the guidelines established in DeepSeek LLM (DeepSeek-AI, 2024).\\n3.5. Environments\\nOur experiments are conducted using the HAI-LLM (High-Flyer, 2023) framework, known for\\nits efficiency and lightweight approach in training large language models. This framework\\nincorporates a variety of parallelism strategies to optimize computational efficiency. These in-\\nclude tensor parallelism (Korthikanti et al., 2023), alongside ZeRO data parallelism (Rajbhandari\\net al., 2020) and PipeDream pipeline parallelism (Narayanan et al., 2019). Our experiments\\n2https://github.com/huggingface/tokenizers\\n8'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 8}, page_content='Hyperparameter DeepSeek-Coder 1.3B DeepSeek-Coder 6.7B DeepSeek-Coder 33B\\nHidden Activation SwiGLU SwiGLU SwiGLU\\nHidden size 2048 4096 7168\\nIntermediate size 5504 11008 19200\\nHidden layers number 24 32 62\\nAttention heads number 16 32 56\\nAttention Multi-head Multi-head Grouped-query (8)\\nBatch Size 1024 2304 3840\\nMax Learning Rate 5.3e-4 4.2e-4 3.5e-4\\nTable 2 |Hyperparameters of DeepSeek-Coder.\\nutilize clusters outfitted with NVIDIA A100 and H800 GPUs. In the A100 cluster, each node\\nis configured with 8 GPUs, interconnected in pairs using NVLink bridges. The H800 cluster is\\nsimilarly arranged, with each node containing 8 GPUs. These GPUs are interconnected using\\na combination of NVLink and NVSwitch technologies, ensuring efficient data transfer within\\nnodes. To facilitate seamless communication between nodes in both A100 and H800 clusters, we\\nemploy InfiniBand interconnects, known for their high throughput and low latency. This setup\\nprovides a robust and efficient infrastructure for our computational experiments.\\n3.6. Long Context\\nTo enhance the capabilities of DeepSeek-Coder in handling extended contexts, particularly for\\nscenarios like repository-level code processing, we have reconfigured the RoPE (Su et al., 2023)\\nparameters to extend the default context window. Following previous practices (Chen et al.,\\n2023; kaiokendev, 2023), we employed a linear scaling strategy, increasing the scaling factor from\\n1 to 4 and altering the base frequency from 10000 to 100000. The model underwent an additional\\n1000 steps of training, using a batch size of 512 and a sequence length of 16K. The learning rate\\nwas maintained as in the final pre-training phase. Theoretically, these modifications enable our\\nmodel to process up to 64K tokens in context. However, empirical observations suggest that\\nthe model delivers its most reliable outputs within a 16K token range. Future research will\\ncontinue to refine and evaluate the long-context adaptation methodology, aiming to further\\nenhance DeepSeek-Coder’s efficiency and user-friendliness in processing extended contexts.\\n3.7. Instruction Tuning\\nWe develop DeepSeek-Coder-Instruct by enhancing the DeepSeek-Coder-Base through instruction-\\nbased fine-tuning using high-quality data. This data comprises helpful and impartial human\\ninstructions, structured by the Alpaca Instruction format (Taori et al., 2023). To demarcate each\\ndialogue turn, we employed a unique delimiter token <|EOT|> to signify the conclusion of each\\nsegment. For training, we use a cosine schedule with 100 warm-up steps and an initial learning\\nrate 1e-5. We also use a batch size of 4M tokens and 2B tokens in total.\\nAn example of using DeepSeek-Coder-Instruct 34B is depicted in Figure 4. This example is\\na multi-turn dialogue scenario for building a snake game. Initially, we ask the model to write\\na game snake using pygame. The model successfully creates a basic snake game that can run\\nwithout bugs. To improve the game, we further request adding a scoring system in the top left\\ncorner. The model then introduces a \"score\" variable and a \"display_score\" function, along with\\nan explanation of how to integrate these features. This example illustrates DeepSeek-Coder-\\nInstruct’s ability to provide complete solutions in multi-turn dialogue settings. More cases can\\nbe found in the Appendix A.\\n9'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 9}, page_content=\"Q1: Write a game snake using pygame\\nSure, here is a simple implementation of the classic Snake \\ngame using Pygame. This code will create a window with a \\nsnake that can be controlled using the arrow keys. The snake \\nwill grow in length each time it eats the food. The game will \\nend if the snake hits the edge of the window or its own tail.\\n```python\\nimport pygame \\nimport random\\n…\\nrunning = True\\nWhile running:\\n…\\n# Draw everything \\nscreen.fill(BLACK) \\nsnake.draw() \\nfood.draw() \\npygame.display.flip() \\n# Cap the frame rate \\nclock.tick(SPEED) \\n# Quit Pygame \\npygame.quit()\\n```\\nThis code sets up a basic game loop where the snake moves in \\nresponse to key presses, and the game ends if the snake hits the \\nedge of the window or its own tail. The snake grows in length \\neach time it eats the food. The game continues until the player \\ncloses the window.\\nRun Code\\nQ2: Add a scoring system in the top left corner\\nTo add a scoring system to the top left corner of the game \\nwindow, you can create a function to display the score and \\nincrement it each time the snake eats the food. Here's how you \\ncan modify the code to include a scoring system:\\n```python\\n…\\nrunning = True\\nWhile running:\\n…\\n# Draw everything \\nscreen.fill(BLACK) \\nsnake.draw() \\nfood.draw()\\ndisplay_score(score) # Display the score\\npygame.display.flip() \\n# Cap the frame rate \\nclock.tick(SPEED) \\n# Quit Pygame \\npygame.quit()\\n```\\nIn this updated code, we've added a `score` variable to keep \\ntrack of the player's score, which is incremented each time the \\nsnake eats the food. The `display_score` function is used to \\nrender the score text and display it in the top left corner of the \\nwindow. The `display_score` function is called after the snake \\nhas moved and the screen has been filled with the black color, \\nbut before the `pygame.display.flip()` call, so that the score is \\ndrawn on top of everything else.\\nRun Code\\nFigure 4 |An example of responses from DeepSeek-Coder-Instruct 33B in a multi-turn setting.\\n4. Experimental Results\\nIn this section, we evaluate DeepSeek-Coder on four tasks, including code generation (§4.1), FIM\\ncode completion (§4.2), cross-file code completion (§4.3) and program-based math reasoning\\n(§4.4). We compare DeepSeek-Coder with the previous state-of-the-art large language models:\\n• CodeGeeX2 (Zheng et al., 2023) represents the second generation of the multilingual\\ncode generation model CodeGeeX. It is developed using the ChatGLM2 (Du et al., 2022)\\narchitecture and is enhanced with an extensive dataset of coding examples.\\n• StarCoder (Li et al., 2023) is a publicly accessible model with a substantial parameter count\\nof 15 billion. It is specifically trained on a meticulously curated subset of the Stack dataset\\n(Kocetkov et al., 2022), covering 86 programming languages, ensuring its proficiency\\nacross a wide range of coding tasks.\\n• CodeLlama (Roziere et al., 2023) encompasses a series of code-centric Large Language\\nModels (LLMs) that are derivatives of LLaMA2 (Touvron et al., 2023). Available in three\\nsizes — 7B, 13B, and 34B — these models undergo continued training on a vast 500 billion\\ntoken code corpus, building upon the foundational LLaMA2 architecture.\\n• code-cushman-001 Chen et al. (2021) is a 12 billion parameter model developed by OpenAI\\nand served as the initial model for Github Copilot.\\n• GPT-3.5 and GPT-4(OpenAI, 2023) are advanced generative AI models developed by\\nOpenAI. While they are not explicitly trained for code generation, they also demonstrate\\n10\"), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 10}, page_content='notable performance in this domain. Their effectiveness in handling code generation tasks\\nis largely attributed to their massive scale in terms of parameter count.\\n4.1. Code Generation\\nHumanEval and MBPP BenchmarksThe HumanEval (Chen et al., 2021) and MBPP (Austin\\net al., 2021) benchmarks are widely used for evaluating code LLMs. HumanEval consists of 164\\nhand-written Python problems that are validated using test cases to assess the code generated\\nby a Code LLM in a zero-shot setting, while the MBPP benchmark includes 500 problems\\nin a few-shot setting. To evaluate the model’s multilingual capabilities, we expanded the\\nPython problems of Humaneval Benchmark to seven additional commonly used programming\\nlanguages, namely C++, Java, PHP , TypeScript (TS), C#, Bash, and JavaScript (JS) (Cassano et al.,\\n2023). For both benchmarks, We adopted a greedy search approach and re-implemented the\\nbaseline results using the same script and environment for fair comparison.\\nModel Size Python C++ Java PHP TS C# Bash JS Avg MBPP\\nMultilingual Base Models\\ncode-cushman-001 12B 33.5% 31.9% 30.6% 28.9% 31.3% 22.1% 11.7% - - -\\nCodeGeeX2 6B 36.0% 29.2% 25.9% 23.6% 20.8% 29.7% 6.3% 24.8% 24.5% 36.2%\\nStarCoderBase 16B 31.7% 31.1% 28.5% 25.4% 34.0% 34.8% 8.9% 29.8% 28.0% 42.8%\\nCodeLlama 7B 31.7% 29.8% 34.2% 23.6% 36.5% 36.7% 12.0% 29.2% 29.2% 38.6%\\nCodeLlama 13B 36.0% 37.9% 38.0% 34.2% 45.2% 43.0% 16.5% 32.3% 35.4% 48.4%\\nCodeLlama 34B 48.2% 44.7% 44.9% 41.0% 42.1% 48.7% 15.8% 42.2% 41.0% 55.2%\\nDeepSeek-Coder-Base 1.3B 34.8% 31.1% 32.3% 24.2% 28.9% 36.7% 10.1% 28.6% 28.3% 46.2%\\nDeepSeek-Coder-Base 6.7B 49.4% 50.3% 43.0% 38.5% 49.7% 50.0% 28.5% 48.4% 44.7% 60.6%\\nDeepSeek-Coder-Base 33B 56.1% 58.4% 51.9% 44.1% 52.8% 51.3% 32.3% 55.3% 50.3% 66.0%\\nInstruction-Tuned Models\\nGPT-3.5-Turbo - 76.2% 63.4% 69.2% 60.9% 69.1% 70.8% 42.4% 67.1% 64.9% 70.8%\\nGPT-4 - 84.1% 76.4% 81.6% 77.2% 77.4% 79.1% 58.2% 78.0% 76.5% 80.0%\\nDeepSeek-Coder-Instruct 1.3B65.2% 45.3% 51.9% 45.3% 59.7% 55.1% 12.7% 52.2% 48.4% 49.4%\\nDeepSeek-Coder-Instruct 6.7B78.6% 63.4% 68.4% 68.9% 67.2% 72.8% 36.7% 72.7% 66.1% 65.4%\\nDeepSeek-Coder-Instruct 33B79.3% 68.9% 73.4% 72.7% 67.9% 74.1% 43.0% 73.9% 69.2% 70.0%\\nTable 3 |Performance of approaches on the Multilingual HumanEval and MBPP Benchmarks.\\nThe results are presented in Table 3. As we can see, DeepSeek-Coder-Base achieves state-\\nof-the-art performance with an average accuracy of 50.3% on HumanEval and 66.0% on MBPP .\\nIn comparison to the similarly sized open-source model CodeLlama-Base 34B, our model has\\ndemonstrated a notable improvement of 9% and 11% in accuracy, respectively. It’s worth\\nnoting that even our smaller model, DeepSeek-Coder-Base 6.7B, surpasses the performance\\nof CodeLlama-Base 34B. After instruction fine-tuning, our model surpasses the closed-source\\nGPT-3.5-Turbo model in HumanEval benchmark, significantly reducing the performance gap\\nbetween OpenAI GPT-4 and open-source models.\\nDS-1000 Benchmark HumanEval and MBPP have a significant drawback in that they rely\\nheavily on straightforward programming tasks that may not accurately represent the kind of\\ncode most programmers typically write. In contrast, the DS-1000 benchmark, as introduced in\\nthe work by Lai et al. (2023), offers a comprehensive collection of 1,000 practical and realistic data\\nscience workflows across seven different libraries. This benchmark evaluates code generation by\\nexecuting it against specific test cases. What sets DS-1000 apart is its categorization of problems\\nbased on the libraries involved, which encompass Matplotlib, NumPy, Pandas, SciPy, Scikit-\\n11'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 11}, page_content='Learn, PyTorch, and TensorFlow. The benchmark assesses the performance of base models in the\\ncode completion setting and we provide pass@1 results for each library, as well as overall score.\\nThe results of DS-1000 benchmark are shown in Table 4. As can be seen from the table, the\\nDeepSeek-Coder model achieves relatively high accuracy in all libraries, demonstrating that our\\nmodel is not only capable of generating good code but also of using libraries more accurately in\\nreal data science workflows.\\nModel Size Matplotlib Numpy Pandas Pytorch Scipy Scikit-Learn Tensorflow Avg\\nCodeGeeX2 6B 38.7% 26.8% 14.4% 11.8% 19.8% 27.0% 17.8% 22.9%\\nStarCoder-Base 16B 43.2% 29.1% 11.0% 20.6% 23.6% 32.2% 15.6% 24.6%\\nCodeLlama-Base 7B 41.9% 24.6% 14.8% 16.2% 18.9% 17.4% 17.8% 22.1%\\nCodeLlama-Base 13B 46.5% 28.6% 18.2% 19.1% 18.9% 27.8% 33.3% 26.8%\\nCodeLlama-Base 34B 50.3% 42.7% 23.0% 25.0% 28.3% 33.9% 40.0% 34.3%\\nDeepSeek-Coder-Base 1.3B 32.3% 21.4% 9.3% 8.8% 8.5% 16.5% 8.9% 16.2%\\nDeepSeek-Coder-Base 6.7B 48.4% 35.5% 20.6% 19.1% 22.6% 38.3% 24.4% 30.5%\\nDeepSeek-Coder-Base 33B 56.1% 49.6% 25.8% 36.8% 36.8% 40.0% 46.7% 40.2%\\nTable 4 |Performance of different approaches on the DS-1000-Tasks.\\nLeetCode Contest Benchmark To further validate the model’s capability in real-world pro-\\ngramming problems, we construct the LeetCode Contest benchmark 3. LeetCode 4 presents\\ncompetition-level problems, offering significant challenges that test the model’s problem under-\\nstanding and code generation skills. We collected the latest problems from LeetCode Contests to\\nprevent the appearance of both the problems or their solutions in our pre-training data. A total\\nof 180 problems were collected from July 2023 to January 2024. For each problem, we collected\\n100 test cases to ensure the test coverage. We use the template \"{problem_description}\\\\nPlease\\ncomplete the code below to solve the above problem:\\\\n```python\\\\n{code_template}\\\\n```\" to\\nbuild the instruction prompt.\\nThe evaluation results are shown in Table 5. In our evaluation, the DeepSeek-Coder models\\ndemonstrate remarkable performance over current open-source coding models. Specifically, the\\nDeepSeek-Coder-Instruct 6.7B and 33B achieve Pass@1 scores of 19.4% and 27.8% respectively\\nin this benchmark. This performance notably surpasses existing open-sourced models such as\\nCode-Llama-33B. The DeepSeek-Coder-Instruct 33B is the only open-sourced model that outper-\\nforms OpenAI’s GPT-3.5-Turbo in this task. However, there remains a substantial performance\\ngap when compared to the more advanced GPT-4-Turbo.\\nOur analysis indicates that the implementation of Chain-of-Thought (CoT) prompting no-\\ntably enhances the capabilities of DeepSeek-Coder-Instruct models. This improvement becomes\\nparticularly evident in the more challenging subsets of tasks. By adding the directive, \"You\\nneed first to write a step-by-step outline and then write the code.\" following the initial prompt,\\nwe have observed enhancements in performance. This observation leads us to believe that the\\nprocess of first crafting detailed code descriptions assists the model in more effectively under-\\nstanding and addressing the intricacies of logic and dependencies in coding tasks, particularly\\nthose of higher complexity. Therefore, we strongly recommend employing CoT prompting strate-\\ngies when utilizing DeepSeek-Coder-Instruct models for complex coding challenges. Such an\\napproach promotes a more methodical and logical framework for problem-solving, potentially\\nresulting in more precise and efficient outcomes in code generation tasks.\\n3We have published this benchmark in https://github.com/deepseek-ai/DeepSeek-Coder/tree/main\\n/Evaluation/LeetCode.\\n4https://leetcode.com/\\n12'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 12}, page_content='Model Size Easy (45) Medium (91) Hard (44) Overall(180)\\nWizardCoder-V1.0 15B 17.8% 1.1% 0.0% 5.0%\\nCodeLlama-Instruct 34B 24.4% 4.4% 4.5% 9.4%\\nPhind-CodeLlama-V2 34B 26.7% 8.8% 9.1% 13.3%\\nGPT-3.5-Turbo - 46.7% 15.4 % 15.9% 23.3%\\nGPT-3.5-Turbo + CoT - 42.2% 15.4% 20.5% 23.3%\\nGPT-4-Turbo - 73.3% 31.9% 25.0% 40.6%\\nGPT-4-Turbo + CoT - 71.1% 35.2% 25.0% 41.8%\\nDeepSeek-Coder-Instruct 1.3B 22.2% 1.1% 4.5% 7.2%\\nDeepSeek-Coder-Instruct + CoT 1.3B 22.2% 2.2% 2.3% 7.2%\\nDeepSeek-Coder-Instruct 6.7B 44.4% 12.1% 9.1% 19.4%\\nDeepSeek-Coder-Instruct + CoT 6.7B 44.4% 17.6% 4.5% 21.1%\\nDeepSeek-Coder-Instruct 33B 57.8% 22.0% 9.1% 27.8%\\nDeepSeek-Coder-Instruct + CoT 33B 53.3% 25.3% 11.4% 28.9%\\nTable 5 |Performance of different models on the LeetCode Contest Benchmark.\\nIt is important to acknowledge that despite our diligent efforts to gather the most recent\\ncode questions for model testing, the possibility of data contamination cannot be entirely ruled\\nout. We observed that the GPT-4-Turbo and DeepSeek-Coder models achieved higher scores\\nin the LeetCode Contest held in July and August. We encourage the research community to\\nconsider the potential issue of data contamination when evaluating models in future studies\\nusing our released LeetCode data.\\n4.2. Fill-in-the-Middle Code Completion\\nDeepSeek-Coder models are trained with a 0.5 FIM (Fill-In-the-Middle) rate during their pre-\\ntraining phase. This specialized training strategy empowers the model to proficiently generate\\ncode by filling in blanks based on the surrounding context, both prefix and suffix, of the given\\ncode snippet. This capability is particularly advantageous in the realm of code completion\\ntools. Several open-source models have emerged with similar capabilities. Notable among these\\nare SantaCoder (Allal et al., 2023), StarCoder (Li et al., 2023), and CodeLlama (Roziere et al.,\\n2023). These models have set a precedent in the field of code generation and completion. In\\nevaluating the performance DeepSeek-Coder models, we conducted a comparative analysis with\\nthe aforementioned models. The benchmark for this comparison was the Single-Line Infilling\\nbenchmarks, encompassing three different programming languages, as proposed by Allal et al.\\n(2023). This benchmark uses the line exact match accuracy as the evaluation metric.\\nModel Size python java javascript Mean\\nSantaCoder 1.1B 44.0% 62.0% 74.0% 69.0%\\nStarCoder 16B 62.0% 73.0% 74.0% 69.7%\\nCodeLlama-Base 7B 67.6% 74.3% 80.2% 69.7%\\nCodeLlama-Base 13B 68.3% 77.6% 80.7% 75.5%\\nDeepSeek-Coder-Base 1B 57.4% 82.2% 71.7% 70.4%\\nDeepSeek-Coder-Base 7B 66.6% 88.1% 79.7% 80.7%\\nDeepSeek-Coder-Base 33B 65.4% 86.6% 82.5% 81.2%\\nTable 6 |Performance of different approaches on the FIM-Tasks.\\nThe evaluation results are shown in Table 6. Despite being the smallest model with a capacity\\n13'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 13}, page_content='of 1.3 billion parameters, DeepSeek-Coder outperforms its larger counterparts, StarCoder and\\nCodeLlama, in these benchmarks. This superior performance can be attributed to the high\\nquality of the pre-trained data utilized by DeepSeek-Coder. Furthermore, a notable trend\\nobserved is the correlation between the size of the model and its performance. As the model\\nsize increases, there is a corresponding and responsible enhancement in performance. This\\ntrend underscores the importance of model capacity in achieving higher accuracy in code\\ncompletion tasks. Based on these findings, we recommend the deployment of the DeepSeek-\\nCoder-Base 6.7B model in code completion tools. This recommendation is grounded in the\\nmodel’s demonstrated balance between efficiency and accuracy. The DeepSeek-Coder-Base 6.7B\\nmodel, with its substantial parameter size, has proven to be highly effective in the context of\\ncode completion, making it an ideal choice for integrating advanced computational capabilities\\ninto coding environments.\\n4.3. Cross-File Code Completion\\nIn this section, we will evaluate the performance of existing open-source models in cross-file\\ncode completion tasks. Unlike code generation discussed in the previous section, cross-file code\\ncompletion requires the model to access and understand repositories that span multiple files\\nwith numerous cross-file dependencies. We use CrossCodeEval (Ding et al., 2023) to evaluate the\\ncapabilities of currently available open-source code models of 7B scale in cross-file completion\\ntasks. This dataset is constructed on a diverse set of real-world, open-sourced, permissively\\nlicensed repositories in four popular programming languages: Python, Java, TypeScript, and C#.\\nThe dataset is specifically designed to strictly require cross-file context for accurate completion.\\nNotably, this dataset was constructed from repositories created between March and June 2023,\\nwhile our pre-training data only includes code created before February 2023, which ensures that\\nthis dataset was not present in our pre-training data, thus avoiding data leakage.\\nModel Size Python Java TypeScript C#\\nEM ES EM ES EM ES EM ES\\nCodeGeex2 6B 8.11% 59.55% 7.34% 59.60% 6.14% 55.50% 1.70% 51.66%\\n+ Retrieval 10.73% 61.76% 10.10% 59.56% 7.72% 55.17% 4.64% 52.30%\\nStarCoder-Base 7B 6.68% 59.55% 8.65% 62.57% 5.01% 48.83% 4.75% 59.53%\\n+ Retrieval 13.06% 64.24% 15.61% 64.78% 7.54% 42.06% 14.20% 65.03%\\nCodeLlama-Base 7B 7.32% 59.66% 9.68% 62.64% 8.19% 58.50% 4.07% 59.19%\\n+ Retrieval 13.02% 64.30% 16.41% 64.64% 12.34% 60.64% 13.19% 63.04%\\nDeepSeek-Coder-Base 6.7B 9.53% 61.65% 10.80% 61.77% 9.59% 60.17% 5.26% 61.32%\\n+ Retrieval 16.14% 66.51% 17.72% 63.18% 14.03% 61.77% 16.23% 63.42%\\n+ Retrieval w/o Repo Pre-training 16.02% 66.65% 16.64% 61.88% 13.23% 60.92% 14.48% 62.38%\\nTable 7 |Performance of different models on cross-file code completion.\\nIn our evaluation of various models, we set the maximum sequence length to 2048 tokens,\\nthe maximum output length to 50 tokens, and a limit of 512 tokens for the cross-file context. For\\nthe cross-file context, we utilize the official BM25 search results provided by Ding et al. (2023).\\nEvaluation metrics include exact match and edit similarity. The results, presented in Table 7,\\ndemonstrate that DeepSeek-Coder consistently outperforms other models in cross-file comple-\\ntion tasks across multiple languages, showcasing its superior practical application capabilities.\\nWhen only utilizing file-level code corpus (w/o Repo Pre-training) to pre-train DeepSeek-Coder,\\nwe observe a decrease in performance in the Java, TypeScript, and C# languages, indicating the\\neffectiveness of the repository-level pre-training.\\n14'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 14}, page_content='4.4. Program-based Math Reasoning\\nProgram-based math reasoning involves evaluating a model’s ability to understand and solve\\nmathematical problems through programming. This type of reasoning is critical in fields\\nsuch as data analysis and scientific computing. To conduct this assessment, we utilize the\\nProgram-Aided Math Reasoning (PAL) method as outlined in Gao et al. (2023). This approach\\nis applied across seven distinct benchmarks, each offering unique challenges and contexts.\\nThese benchmarks includes GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), GSM-\\nHard (Gao et al., 2023), SVAMP (Patel et al., 2021), TabMWP (Lu et al., 2022), ASDiv (Miao\\net al., 2020) and MAWPS (Gou et al., 2023). In each of these benchmarks, the model is prompted\\nto alternately describe a solution step in natural language and then execute that step with\\ncode. As seen in Table 8, DeepSeek-Coder models achieve a remarkable performance across all\\nbenchmarks, especially the 33B variant, which demonstrates the potential of using such models\\nin applications that require complex mathematical computations and problem-solving abilities.\\nModel Size GSM8k MATH GSM-Hard SVAMP TabMWP ASDiv MAWPS Avg\\nMultilingual Base Models\\nCodeGeex-2 7B 22.2% 9.7% 23.6% 39.0% 44.6% 48.5% 66.0% 36.2%\\nStarCoder-Base 16B 23.4% 10.3% 23.0% 42.4% 45.0% 54.9% 81.1% 40.0%\\nCodeLlama-Base 7B 31.2% 12.1% 30.2% 54.2% 52.9% 59.6% 82.6% 46.1%\\nCodeLlama-Base 13B 43.1% 14.4% 40.2% 59.2% 60.3% 63.6% 85.3% 52.3%\\nCodeLlama-Base 34B 58.2% 21.2% 51.8% 70.3% 69.8% 70.7% 91.8% 62.0%\\nDeepSeek-Coder-Base 1.3B 14.6% 16.8% 14.5% 36.7% 30.0% 48.2% 62.3% 31.9%\\nDeepSeek-Coder-Base 6.7B 43.2% 19.2% 40.3% 58.4% 67.9% 67.2% 87.0% 54.7%\\nDeepSeek-Coder-Base 33B 60.7% 29.1% 54.1% 71.6% 75.3% 76.7% 93.3% 65.8%\\nTable 8 |Performance of different approaches on the program-aid math reasoning tasks.\\n5. Continue Pre-Training From General LLM\\nTo further enhance the natural language understanding and mathematical reasoning abilities\\nof the DeepSeek-Coder model, we perform additional pre-training from the general language\\nmodel DeepSeek-LLM-7B Base (DeepSeek-AI, 2024) on 2 trillion tokens, resulting in DeepSeek-\\nCoder-v1.5 7B. For this pre-training, we specifically use the data sources listed in Table 9. Unlike\\nDeepSeek-Coder, DeepSeek-Coder-v1.5 employs solely a next token prediction objective with a\\n4K context length during its pre-training phase.\\nData Source Percentage\\nSource Code 70%\\nMarkdown and StackExchange 10%\\nNatural language related to code 7%\\nNatural language related to math 7%\\nBilingual (Chinese-English) natural language 6%\\nTable 9 |Data sources for DeepSeek-Coder-v1.5 7B pre-training\\nWe conduct a comparison between DeepSeek-Coder-v1.5 7B and DeepSeek-Coder 6.7B, and\\nre-run all benchmarks using our evaluation pipeline to ensure a fair comparison. We evaluate\\nperformance across a wide range of tasks, which can be categorized as follows:\\n15'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 15}, page_content='• Programming: This category includes evaluations in a multilingual setting using the\\nHumanEval dataset by Chen et al. (2021), as well as evaluations in a Python setting using\\nthe MBPP dataset by Austin et al. (2021)\\n• Math Reasoning: We assess performance on math reasoning tasks using the GSM8K\\nbenchmark (Cobbe et al., 2021) and the MATH (Hendrycks et al., 2021) benchmark [4].\\nThese tasks involve solving math problems by generating programs.\\n• Natural LanguageOur evaluation in natural language tasks includes MMLU (Hendrycks\\net al., 2020), BBH (Suzgun et al., 2022), HellaSwag (Zellers et al., 2019), Winogrande (Sak-\\naguchi et al., 2021), and ARC-Challenge (Clark et al., 2018) benchmarks.\\nThe results for the Base and Instruct models are presented in Table 10. It is observed\\nthat the DeepSeek-Coder-Base-v1.5 model, despite a slight decrease in coding performance,\\nshows marked improvements across most tasks when compared to the DeepSeek-Coder-Base\\nmodel. In particular, in the Math Reasoning and Natural Language categories, DeepSeek-\\nCoder-Base-v1.5 significantly outperforms its predecessor across all benchmarks, which also\\ndemonstrates significant improvements in its mathematical reasoning and natural language\\nprocessing capabilities.\\nProgramming Math Reasoning Natural Language\\nModels Size HumanEval MBPP GSM8K MATH MMLU BBH HellaSwag WinoG ARC-C\\nDeepSeek-Coder-Base 6.7B 44.7% 60.6% 43.2% 19.2% 36.6% 44.3% 53.8% 57.1% 32.5%\\nDeepSeek-Coder-Base-v1.5 6.9B 43.2% 60.4% 62.4% 24.7% 49.1% 55.2% 69.9% 63.8% 47.2%\\nDeepSeek-Coder-Instruct 6.7B 66.1% 65.4% 62.8% 28.6% 37.2% 46.9% 55.0% 57.6% 37.4%\\nDeepSeek-Coder-Instruct-v1.5 6.9B 64.1% 64.6%72.6% 34.1% 49.5% 53.3% 72.2% 63.4% 48.1%\\nTable 10 |Comparative analysis of performance between DeepSeek-Coder-Base and\\nDeepSeek-Coder-Base-v1.5. Math tasks are solved through programming.\\n6. Conclusion\\nIn this technical report, we introduce a series of specialized Large Language Models (LLMs) for\\ncoding, named DeepSeek-Coder, available in three distinct scales: 1.3B, 6.7B, and 33B parameters.\\nThese models are uniquely trained on a meticulously curated project-level code corpus, utilizing\\na \"fill-in-the-blank\" pre-training objective to enhance code infilling capabilities. A significant\\nadvancement is the extension of the models’ context window to 16,384 tokens, thereby greatly\\nimproving their effectiveness in handling extensive code generation tasks. Our evaluations\\nreveal that the most advanced model in our series, DeepSeek-Coder-Base 33B surpasses existing\\nopen-source code models across a variety of standard tests. Impressively, the DeepSeek-Coder-\\nBase 6.7B model, despite its smaller scale, delivers performance on par with the 34B parameter\\nCodeLlama, a testament to the high quality of our pretraining corpus.\\nTo augment the zero-shot instruction capabilities of the DeepSeek-Coder-Base models, we\\nhave fine-tuned them with high-quality instructional data. This has led to the DeepSeek-Coder-\\nInstruct 33B model outperforming OpenAI’s GPT-3.5 Turbo in a range of coding-related tasks,\\nshowcasing its exceptional proficiency in code generation and understanding.\\nTo further improve the natural language understanding capabilities of the DeepSeek-Coder-\\nBase models, we have conducted additional pretraining based on the DeepSeek-LLM 7B check-\\npoint. This additional training involved processing a diverse dataset comprising 2 billion tokens,\\nincluding natural language, code, and mathematical data. The result is the creation of a new\\n16'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 16}, page_content='and improved code model, DeepSeek-Coder-v1.5. Our observations indicate that DeepSeek-\\nCoder-v1.5 not only maintains its predecessor’s high-level coding performance but also exhibits\\nenhanced natural language comprehension. This advancement underscores our belief that the\\nmost effective code-focused Large Language Models (LLMs) are those built upon robust general\\nLLMs. The reason is evident: to effectively interpret and execute coding tasks, these models\\nmust also possess a deep understanding of human instructions, which often come in various\\nforms of natural language. Looking ahead, our commitment is to develop and openly share\\neven more powerful code-focused LLMs based on larger-scale general LLMs.\\nAcknowledgements\\nWe would like to express our gratitude to Bo Liu, Chengqi Deng, Chong Ruan, Damai Dai,\\nJiashi Li, Kang Guan, Mingchuan Zhang, Panpan Huang, Shuiping Yu, Shirong Ma, Yaofeng\\nSun, Yishi Piao, Zhihong Shao, and Zhewen Hao for their invaluable discussions and assistance\\nduring training DeepSeek-Coder models.\\nReferences\\nL. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra,\\nA. Gu, M. Dey, et al. Santacoder: don’t reach for the stars! arXiv preprint arXiv:2301.03988,\\n2023.\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\\nQ. Le, and C. Sutton. Program synthesis with large language models, 2021.\\nM. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek, and M. Chen. Efficient\\ntraining of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.\\nF. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M.-H. Yee,\\nY. Zi, C. J. Anderson, M. Q. Feldman, et al. Multipl-e: a scalable and polyglot approach to\\nbenchmarking neural code generation. IEEE Transactions on Software Engineering, 2023.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374, 2021.\\nS. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via\\npositional interpolation. arXiv preprint arXiv:2306.15595, 2023.\\nP . Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think\\nyou have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint\\narXiv:1803.05457, 2018.\\nK. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168, 2021.\\nT. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\\nDeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv\\npreprint arXiv:2401.02954, 2024.\\n17'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 17}, page_content='Y. Ding, Z. Wang, W. U. Ahmad, H. Ding, M. Tan, N. Jain, M. K. Ramanathan, R. Nallapati,\\nP . Bhatia, D. Roth, et al. Crosscodeeval: A diverse and multilingual benchmark for cross-file\\ncode completion. In Thirty-seventh Conference on Neural Information Processing Systems\\nDatasets and Benchmarks Track, 2023.\\nZ. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model\\npretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 320–335,\\n2022.\\nD. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer,\\nand M. Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint\\narXiv:2204.05999, 2022.\\nL. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-\\naided language models. In International Conference on Machine Learning, pages 10764–\\n10799. PMLR, 2023.\\nG. Gemini Team. Gemini: A family of highly capable multimodal models, 2023. URL https:\\n//goo.gle/GeminiPaper.\\nZ. Gou, Z. Shao, Y. Gong, Y. Yang, M. Huang, N. Duan, W. Chen, et al. Tora: A tool-integrated\\nreasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measur-\\ning massive multitask language understanding. In International Conference on Learning\\nRepresentations, 2020.\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\\n2021.\\nHigh-Flyer. Hai-llm: An efficient and lightweight tool for training large models. 2023. URL\\nhttps://www.high-flyer.cn/en/blog/hai-llm.\\nkaiokendev. Things i’m learning while training superhot. https://kaiokendev.github.io\\n/til#extending-context-to-8k, 2023.\\nD. Kocetkov, R. Li, L. Jia, C. Mou, Y. Jernite, M. Mitchell, C. M. Ferrandis, S. Hughes, T. Wolf,\\nD. Bahdanau, et al. The stack: 3 tb of permissively licensed source code. Transactions on\\nMachine Learning Research, 2022.\\nV . A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro.\\nReducing activation recomputation in large transformer models. Proceedings of Machine\\nLearning and Systems, 5, 2023.\\nY. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih, D. Fried, S. Wang, and T. Yu.\\nDs-1000: A natural and reliable benchmark for data science code generation. In International\\nConference on Machine Learning, pages 18319–18345. PMLR, 2023.\\nK. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Deduplicat-\\ning training data makes language models better. In Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424–8445,\\n2022.\\n18'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 18}, page_content='R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim,\\net al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023.\\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019.\\nP . Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P . Clark, and A. Kalyan. Dynamic\\nprompt learning via policy gradient for semi-structured mathematical reasoning. In The\\nEleventh International Conference on Learning Representations, 2022.\\nS.-Y. Miao, C.-C. Liang, and K.-Y. Su. A diverse corpus for evaluating and developing english\\nmath word problem solvers. In Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics, pages 975–984, 2020.\\nD. Narayanan, A. Harlap, A. Phanishayee, V . Seshadri, N. R. Devanur, G. R. Ganger, P . B.\\nGibbons, and M. Zaharia. Pipedream: Generalized pipeline parallelism for dnn training.\\nIn Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1–15,\\n2019.\\nE. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen:\\nAn open large language model for code with multi-turn program synthesis. arXiv preprint\\narXiv:2203.13474, 2022.\\nE. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou. Codegen2: Lessons for training\\nllms on programming and natural languages, 2023.\\nOpenAI. Gpt-4 technical report, 2023.\\nA. Patel, S. Bhattamishra, and N. Goyal. Are nlp models really able to solve simple math\\nword problems? In Proceedings of the 2021 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human Language Technologies, pages 2080–\\n2094, 2021.\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P . J. Liu.\\nExploring the limits of transfer learning with a unified text-to-text transformer, 2023.\\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training tril-\\nlion parameter models. In SC20: International Conference for High Performance Computing,\\nNetworking, Storage and Analysis, pages 1–16. IEEE, 2020.\\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin,\\net al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd\\nschema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.\\nR. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword\\nunits. arXiv preprint arXiv:1508.07909, 2015.\\nJ. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with\\nrotary position embedding, 2023.\\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V . Le,\\nE. H. Chi, D. Zhou, , and J. Wei. Challenging big-bench tasks and whether chain-of-thought\\ncan solve them. arXiv preprint arXiv:2210.09261, 2022.\\n19'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 19}, page_content='R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P . Liang, and T. B. Hashimoto.\\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab\\n/stanford_alpaca, 2023.\\nH. Touvron, L. Martin, K. Stone, P . Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\\nP . Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.arXiv\\npreprint arXiv:2307.09288, 2023.\\nY. Wang, W. Wang, S. Joty, and S. C. Hoi. Codet5: Identifier-aware unified pre-trained encoder-\\ndecoder models for code understanding and generation. arXiv preprint arXiv:2109.00859,\\n2021.\\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really\\nfinish your sentence? In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, pages 4791–4800, 2019.\\nQ. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, L. Shen, Z. Wang, A. Wang, Y. Li, et al.\\nCodegeex: A pre-trained model for code generation with multilingual benchmarking on\\nhumaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, pages 5673–5684, 2023.\\n20'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 20}, page_content='A. Cases of Chatting with DeepSeek-Coder-Instruct\\nWe will present two cases of interactions with DeepSeek-Coder-Instruct, with one involving a\\nmulti-turn conversation about creating a database and performing data analysis, and the other\\ncentered around using a model to solve a sample problem from LeetCode.\\nIn the first scenario, depicted in Figure 5, we instruct the model to build a student database\\nusing Python and randomly insert 10 pieces of information. Subsequently, in the second round\\nof the conversation, we continue to ask the model by analyzing the age distribution of the\\nstudents. From Figure 5, it’s evident that the model can generate bug-free and comprehensive\\ncode, accompanied by explanatory details. In the second scenario, as illustrated in Figure 6,\\nwe further assess the model’s capabilities by testing it on an out-of-domain LeetCode contest\\nproblem. This particular problem was released in November 2023, after our data collection, and\\nthus, isn’t part of our model’s training data. The results show that our model excels at solving\\nproblems that extend beyond its training distribution.\\nQ1: Build a database in Python that includes \\nvarious pieces of information about students, \\ninsert ten random records and display.\\nSure, here is a simple example of how you can do this using \\nSQLite in Python.\\n```python\\nimport sqlite3\\nimport random\\n# Connect to SQLite database\\nconn = sqlite3.connect(\\'students.db\\')\\n# Create a cursor object\\ncur = conn.cursor()\\n…\\n# Create table\\ncur.execute(\\'\\'’\\nCREATE TABLE IF NOT EXISTS students (\\nid INTEGER PRIMARY KEY,\\nname TEXT,\\nage INTEGER,\\ngrade TEXT) \\'\\'\\')\\n…\\n# Display all records\\ncur.execute(\\'SELECT * FROM students\\')\\n…\\n```\\nThis script will create a SQLite database named \\'students.db\\' \\nand a table named \\'students\\' with four columns: \\'id\\', \\'name\\', \\'age\\', \\nand \\'grade\\'. It will then insert 10 random records into the table \\nand display all records. …\\nRun Code\\nQ2: create a graph depicting the age distribution.\\nTo create a graph depicting the age distribution of the students, you \\ncan use the matplotlib library in Python. Here\\'s how you can do it:\\n```python\\nimport sqlite3\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n# Connect to SQLite database\\nconn = sqlite3.connect(\\'students.db\\')\\n# Read the data from the database into a pandas DataFrame\\ndf = pd.read_sql_query(\"SELECT * from students\", conn)\\n# Create a histogram of the age distribution\\nplt.hist(df[\\'age\\'], bins=range(10, 30, 2), edgecolor=\\'black\\')\\nplt.title(\\'Age Distribution of Students\\')\\nplt.xlabel(\\'Age\\')\\nplt.ylabel(\\'Number of Students\\')\\nplt.show()\\n# Close the connection\\nconn.close()\\n```\\nThis script will connect to the SQLite database, read the data into a \\npandas DataFrame, then create a histogram of the age distribution. \\nThe histogram will show the number of students in each age range.\\nRun Code\\nFigure 5 |An example of building database and data analysis.\\n21'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 21}, page_content='Q: There are n teams numbered from 0 to n - 1 in a tournament; each team is \\nalso a node in a DAG.\\nYou are given the integer n and a 0-indexed 2D integer array edges of \\nlength m representing the DAG, where edges[i] = [ui, vi] indicates that there is a \\ndirected edge from team ui to team vi in the graph.\\nA directed edge from a to b in the graph means that team a is stronger than \\nteam b and team b is weaker than team a.\\nTeam a will be the champion of the tournament if there is no team b that \\nis stronger than team a.\\nReturn the team that will be the champion of the tournament if there is \\na unique champion, otherwise, return -1.\\nNotes\\n•A cycle is a series of nodes a1, a2, ..., an, an+1 such that node a1 is the same node \\nas node an+1, the nodes a1, a2, ..., an are distinct, and there is a directed edge from \\nthe node ai to node ai+1 for every i in the range [1, n].\\n•A DAG is a directed graph that does not have any cycle.\\n•Example 1:\\n…\\n•Example 2:\\n…\\nConstraints:\\n…\\nPlease follow the template below to solve the problem:\\n```python\\nclass Solution:\\ndef findChampion(self, n: int, edges: List[List[int]]) -> int: \\n```\\nTo solve this problem, we can use a topological sorting algorithm. The idea is to …\\n```python\\nfrom typing import List\\nclass Solution:\\ndef findChampion(self, n: int, edges: List[List[int]]) -> int:\\n# Create an adjacency list to represent the DAG\\nadj_list = {i: [] for i in range(n)}\\nfor u, v in edges:\\nadj_list[u].append(v)\\n# Initialize a list to keep track of the number of …\\nin_degree = [0] * n\\n# Calculate the in-degree for each team\\nfor u in adj_list:\\nfor v in adj_list[u]:\\nin_degree[v] += 1\\n# Initialize a list to keep track of the teams with no incoming edges\\nno_incoming_edges = [i for i in range(n) if in_degree[i] == 0]\\n# If there is more than one team with no incoming edges, there is no unique \\nchampion\\nif len(no_incoming_edges) != 1:\\nreturn -1\\n# Return the team with no incoming edges, which is the champion\\nreturn no_incoming_edges[0]\\n```\\nThis code first creates an adjacency list to represent the DAG. Then, …\\nSubmit Code\\nFigure 6 |An example of solving LeetCode Problem.\\nB. Benchmark curves during training of DeepSeek-Coder-Base\\nIn Figure 7, we present the benchmark curves illustrating the performance of DeepSeek-Coder-\\nBase models during their training phase. For validation, a carefully curated subset of the training\\ncorpus was employed, consisting of 8,000 code files. This subset was deliberately chosen to\\nensure a diverse and representative sample, critical for an accurate assessment of the models’\\ncapabilities. The performance metrics of these models are specifically detailed in the final two\\nsub-figures of Figure 7, offering a clear visual representation of their efficacy throughout the\\ntraining process.\\n22'), Document(metadata={'source': './data/DeepSeek-Coder When the Large Language Model Meets.pdf', 'page': 22}, page_content='0 250 500 750 1000 1250 1500 1750 2000\\nT okens (Billions)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5Value\\nHumanEval-Pass@1\\n0 250 500 750 1000 1250 1500 1750 2000\\nT okens (Billions)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6Value\\nHumanEval-cpp-Pass@1\\n0 250 500 750 1000 1250 1500 1750 2000\\nT okens (Billions)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6Value\\nHumanEval-java-Pass@1\\n0 250 500 750 1000 1250 1500 1750 2000\\nT okens (Billions)\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85Value\\nHumanEvalFIM-EM\\n0 250 500 750 1000 1250 1500 1750 2000\\nT okens (Billions)\\n0.0\\n0.2\\n0.4\\n0.6Value\\nMBPP-Pass@1\\n0 250 500 750 1000 1250 1500 1750 2000\\nT okens (Billions)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5Value\\nMeanHumanEval\\n0 250 500 750 1000 1250 1500 1750 2000\\nT okens (Billions)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4Value\\nValidation-Completion-EM\\n0 250 500 750 1000 1250 1500 1750 2000\\nT okens (Billions)\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5Value\\nValidation-CompletionFIM-EM\\nDeepSeek-Coder-Base-1.3B DeepSeek-Coder-Base-6.7B DeepSeek-Coder-Base-33B\\nFigure 7 |Benchmark curves during training of DeepSeek-Coder-Base.\\n23')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    DirectoryLoader,\n",
    ")\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"./data/DeepSeek-Coder When the Large Language Model Meets.pdf\")\n",
    "\n",
    "docs = pdf_loader.load()\n",
    "print(\"PDF Documents:\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0147cb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m我今天心情\u001b[39m\u001b[38;5;132;01m{mood}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create a chat model：初始化 DeepSeek\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[1;32m---> 11\u001b[0m     openai_api_key\u001b[38;5;241m=\u001b[39m\u001b[43mapi_key\u001b[49m,\n\u001b[0;32m     12\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.deepseek.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# DeepSeek官方API URL\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 也可以是 deepseek-coder\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Chain the prompt, model, and output parser\u001b[39;00m\n\u001b[0;32m     17\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m StrOutputParser()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'api_key' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"我今天心情{mood}\")\n",
    "\n",
    "# Create a chat model：初始化 DeepSeek\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=api_key,\n",
    "    base_url=\"https://api.deepseek.com\",  # DeepSeek官方API URL\n",
    "    model=\"deepseek-chat\",  # 也可以是 deepseek-coder\n",
    ")\n",
    "\n",
    "# Chain the prompt, model, and output parser\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\"mood\": \"真的非常糟\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
