å®šç¾©ï¼šLLM æ˜¯ä¸€ç¨®ä½¿ç”¨ç¥ç¶“ç¶²è·¯ï¼ˆç‰¹åˆ¥æ˜¯ Transformer æ¶æ§‹ï¼‰è¨“ç·´çš„å¤§å‹æ¨¡å‹ï¼Œé€éå¤§é‡èªè¨€è³‡æ–™å­¸ç¿’èªè¨€çµæ§‹èˆ‡èªæ„ï¼Œèƒ½å¤ è‡ªå‹•ç”¢ç”Ÿã€Œäººé¡èªè¨€é¢¨æ ¼çš„å…§å®¹ã€ã€‚

åŠŸèƒ½ç‰¹è‰²ï¼š
- é€éã€Œé æ¸¬ä¸‹ä¸€å€‹è©ã€å®Œæˆå¥å­ç”Ÿæˆ
- å¯ä»¥é€²è¡Œå•ç­”ã€ç¿»è­¯ã€æ‘˜è¦ã€å¯«ä½œç­‰ä»»å‹™
- ç†è§£è¼¸å…¥å…§å®¹æ˜¯é€éçµ±è¨ˆæ¨¡å¼è€Œéèªæ„æœ¬èº«

## 1. è¨“ç·´æµç¨‹ç°¡è¿°ï¼ˆLLM Training Pipelineï¼‰

|æ­¥é©Ÿ|èªªæ˜|
|---|---|
|Data Collection|æ”¶é›†é¾å¤§çš„æ–‡å­—èªæ–™ï¼ˆæ–‡ç« ã€ç¶²é ã€æ–°èã€æ›¸ç±ç­‰ï¼‰|
|Tokenization|å°‡æ–‡å­—åˆ†å‰²ç‚º model å¯ç†è§£çš„æœ€å°å–®ä½ï¼ˆtokensï¼‰|
|Training|ä½¿ç”¨ç¥ç¶“ç¶²è·¯å­¸ç¿’è¼¸å…¥-è¼¸å‡ºå°æ‡‰é—œä¿‚|
|Fine-tuning|æ ¹æ“šç‰¹å®šä»»å‹™é€²è¡Œå¾®èª¿ï¼ˆå¦‚é†«ç™‚ã€ç¨‹å¼ç¢¼ã€å®¢æœæ¨¡å‹ï¼‰|

## 2. é—œéµçµæ§‹ï¼šTransformer æ¶æ§‹
- **Encoder-Decoder æ¶æ§‹**ï¼šé›™å‘ç†è§£ + å–®å‘ç”Ÿæˆ
- **Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰**ï¼šæ¨¡å‹èƒ½èšç„¦åœ¨è¼¸å…¥ä¸­ã€Œé‡è¦ã€çš„å­—
- **Positional Encoding**ï¼šè£œå……åºåˆ—è³‡è¨Š
    
### 2.1. Encoder-Decoder

| æ¨¡çµ„     | åŠŸèƒ½                         | é¡æ¯”                         |
|----------|------------------------------|------------------------------|
| Encoder  | ç†è§£è¼¸å…¥å¥å­å…§å®¹               | ä¸­æ–‡è€å¸«é–±è®€ä¸€ç¯‡è‹±æ–‡æ–‡ç«          |
| Decoder  | æ ¹æ“šç†è§£ç”Ÿæˆæ–°å¥å­ï¼ˆè¼¸å‡ºï¼‰       | ä¸­æ–‡è€å¸«æŠŠé€™ç¯‡æ–‡ç« ç¿»è­¯æˆä¸­æ–‡       |

Encoder æŠŠè¼¸å…¥å¥å­ï¼ˆä¾‹å¦‚è‹±æ–‡ï¼‰è½‰æˆæ•¸å­¸å‘é‡ï¼ˆæ„æ€å‘é‡ï¼‰
Decoder å†æ ¹æ“šé€™å€‹å‘é‡ï¼Œä¸€å€‹å­—ä¸€å€‹å­—åœ°ç”¢ç”Ÿè¼¸å‡ºå¥å­ï¼ˆä¾‹å¦‚ä¸­æ–‡ï¼‰

### 2.2. **Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰**
åŠŸèƒ½ï¼š æ¨¡å‹åœ¨è™•ç†æ¯ä¸€å€‹è©æ™‚ï¼Œéƒ½æœƒæª¢æŸ¥é€™å€‹è©è·Ÿå¥ä¸­å…¶ä»–è©çš„é—œä¿‚ï¼Œæ±ºå®šã€Œè¦æ³¨æ„èª°ã€ã€‚
ã€Œé€™å¥è©±è£¡ï¼Œå“ªäº›å­—æœ€é‡è¦ï¼Ÿã€
é¡æ¯”ï¼š  åƒä½ çœ‹ä¸€å¥è©±æ™‚ï¼Œæœƒç‰¹åˆ¥æ³¨æ„é—œéµå­—ï¼Œä¾‹å¦‚åœ¨ã€Œæˆ‘æ˜¨å¤©åƒäº†æ‹‰éºµã€ä¸­ï¼Œ"åƒ" å’Œ "æ‹‰éºµ" æ˜¯æœ€é‡è¦çš„è³‡è¨Šã€‚
ä¾‹å¥ï¼šã€ŒThe cat sat on the matã€è™•ç† "cat" æ™‚ï¼Œæ¨¡å‹æœƒç™¼ç¾ "sat" æ¯” "the" æ›´æœ‰é—œä¿‚ â†’ çµ¦å®ƒæ›´é«˜æ³¨æ„åŠ›åˆ†æ•¸

### 2.3. âœ… 3. **Positional Encodingï¼ˆä½ç½®ç·¨ç¢¼ï¼‰**

å› ç‚º Transformer æ˜¯åŒæ™‚è™•ç†æ•´æ®µå¥å­ï¼ˆä¸åƒ RNN æ˜¯ä¸€å€‹å­—ä¸€å€‹å­—èµ°ï¼‰ï¼Œæ‰€ä»¥å®ƒä¸çŸ¥é“å“ªå€‹å­—æ˜¯ç¬¬å¹¾å€‹ã€‚
è§£æ³•ï¼šæ¯å€‹è©åŠ ä¸Šä¸€çµ„ã€Œä½ç½®å‘é‡ã€ä¾†æ¨™ç¤ºå®ƒåœ¨å¥å­è£¡çš„é †åºã€‚
ä¾‹å­ï¼šã€Œæˆ‘ å¾ˆ å–œæ­¡ ä½ ã€ vs ã€Œä½  å–œæ­¡ å¾ˆ æˆ‘ã€ => ç”¨ä½ç½®ç·¨ç¢¼ï¼Œæ¨¡å‹æ‰çŸ¥é“ã€Œèª°å…ˆèª°å¾Œã€å½±éŸ¿èªæ„
é¡æ¯”ï¼š  å°±åƒä¸€çµ„ç©æœ¨ä¸­æ¯å¡Šéƒ½æœ‰ç·¨è™Ÿï¼ˆé †åºï¼‰ï¼Œä½ æ‰çŸ¥é“æ€éº¼æ‹¼å°ã€‚
> Transformer æ²’æœ‰æ™‚é–“æ¦‚å¿µï¼Œé€™æ˜¯ã€Œè£œæ™‚é–“ã€çš„æ©Ÿåˆ¶



## 3. Tokenizer èˆ‡ Token çš„è§’è‰²
- LLM ä¸¦ä¸çœŸæ­£ã€Œç†è§£å¥å­ã€è€Œæ˜¯ã€Œç†è§£ token ä¹‹é–“çš„æ©Ÿç‡é—œä¿‚ã€
- **Tokenizer**ï¼šå°‡æ–‡å­—è½‰æ›ç‚ºæ¨¡å‹å¯è™•ç†çš„ tokenï¼ˆç‰‡æ®µï¼‰
	- ä¾‹å¦‚ï¼šã€ŒI eat apples.ã€ â†’ 4 tokens




## 4. Open Source vs Closed Source LLMs

|åˆ†é¡|å„ªé»|æŒ‘æˆ°|
|---|---|---|
|**é–‹æºæ¨¡å‹**|é€æ˜ã€å¯å®¢è£½åŒ–ã€ç¤¾ç¾¤æ”¯æ´|å»ºç½®é›£åº¦é«˜ã€éœ€ç¡¬é«”èˆ‡ç¶­è­·|
|**å°é–‰æ¨¡å‹**|ç©©å®šã€æ˜“ç”¨ã€åŠŸèƒ½å¼·å¤§|æˆæœ¬é«˜ã€éš±ç§é¢¨éšªã€ç„¡æ³•è‡ªè¨‚ã€ä¾›æ‡‰å•†ç¶å®š|


## 5. Context & Memory Management

Contextï¼ˆä¸Šä¸‹æ–‡ï¼‰ ï¼šæŒ‡ã€Œèˆ‡ç•¶å‰å°è©±ç›¸é—œçš„è³‡è¨Šèˆ‡ç‹€æ…‹ã€ï¼Œä¾‹å¦‚ä½ ä¸Šä¸€å¥å•äº†ä»€éº¼ã€ç³»çµ±æœ‰ä»€éº¼è¨­å®šç­‰ã€‚
Memory Managementï¼ˆè¨˜æ†¶ç®¡ç†ï¼‰ ï¼šæŒ‡ã€Œå„²å­˜ã€æå–ã€ç®¡ç†ä¸Šä¸‹æ–‡ã€çš„æ©Ÿåˆ¶ï¼Œè®“ AI èƒ½å¤ é€£è²«åœ°å°è©±ï¼Œä¸¦åœ¨å¤šè¼ªå°è©±ä¸­ç¶­æŒä¸€è‡´æ€§ã€‚

**ç‚ºä»€éº¼å¾ˆé‡è¦ï¼Ÿï¼ˆWhy It Mattersï¼‰**

-   Coherenceï¼ˆé€£è²«æ€§ï¼‰ï¼šä¿æŒå›æ‡‰ä¸€è‡´ã€ä¸è·³è„«ä¸»é¡Œ
-   Personalizationï¼ˆå€‹äººåŒ–ï¼‰ï¼šè¨˜å¾—ä½¿ç”¨è€…åå¥½èˆ‡æ­·å²äº’å‹•
-   Efficiencyï¼ˆæ•ˆç‡ï¼‰ï¼šé‡è¤‡ä½¿ç”¨å·²æœ‰çš„ä¸Šä¸‹æ–‡ï¼Œæ¸›å°‘é‡æ–°è™•ç†çš„è³‡æºæµªè²»

OpenAI API æ˜¯æ€éº¼è™•ç† Context çš„ï¼Ÿ

-   ä½¿ç”¨ `messages` é™£åˆ—å„²å­˜æ•´å€‹å°è©±æ­·å²
-   æ¯ç­†è¨Šæ¯æ ¼å¼ç‚ºï¼š

```python
messages = [
  {"role": "system", "content": "You are a helpful assistant."},
  {"role": "user", "content": "Hello!"},
  {"role": "assistant", "content": "Hi there! How can I help you?"}
]
```

### 5.1. Context Windowï¼ˆä¸Šä¸‹æ–‡è¦–çª—ï¼‰

ä¸€å€‹ **å¯ç”¨ Token çš„æœ€å¤§è¨˜æ†¶å®¹é‡**
ç•¶è¨Šæ¯å¤ªå¤šæ™‚ï¼Œæœƒï¼š

-   Truncateï¼ˆæˆªæ–·ï¼‰èˆŠè¨Šæ¯
-   æˆ– Summarizeï¼ˆæ‘˜è¦åŒ–ï¼‰éå»çš„å°è©±

```python
##### ä½¿ç”¨llamaï¼Œå¢åŠ ä¸Šä¸‹æ–‡è¨˜æ†¶ #####
import ollama

# å°è©±ä¸Šä¸‹æ–‡åˆå§‹åŒ–
messages = [
    {"role": "user", "content": "ä½ æœ‰éˆé­‚å—"},
]

# å„²å­˜ AI å›è¦†çš„å…§å®¹
assistant_response = ""

# åŸ·è¡Œ stream æ¨¡å¼
res = ollama.chat(
    model="llama3.2:latest",
    messages=messages,
    stream=True,
)

# ä¸€é‚Šå°å‡ºã€ä¸€é‚Šæ”¶é›† AI çš„å›ç­”
for chunk in res:
    content = chunk["message"]["content"]
    print(content, end="", flush=True)
    assistant_response += content

# æŠŠ AI çš„å›è¦†åŠ å…¥ messagesï¼Œç¶­æŒä¸Šä¸‹æ–‡è¨˜æ†¶
messages.append({"role": "assistant", "content": assistant_response})
```

## 6. LLM Logging

Logging æ˜¯åœ¨æ‡‰ç”¨ç¨‹å¼åŸ·è¡ŒæœŸé–“ï¼Œç´€éŒ„äº‹ä»¶ã€è¡Œç‚ºèˆ‡è³‡æ–™çš„éç¨‹ã€‚
ç”¨é€”ï¼š

1. **Debugging èˆ‡éŒ¯èª¤è¿½è¹¤**
    - LLM æ˜¯ä¸å¯é æ¸¬ã€è¤‡é›œç³»çµ±
    - Log å¯ä»¥å¹«åŠ©è¿½è¹¤ä½¿ç”¨è€…è¼¸å…¥ã€ç³»çµ±æµç¨‹èˆ‡éŒ¯èª¤è¨Šæ¯
2. **æ•ˆèƒ½ç›£æ§**ï¼šå›æ‡‰æ™‚é–“ã€token ä½¿ç”¨é‡ã€éŒ¯èª¤ç‡ï¼ˆerror rateï¼‰
3. **æ³•è¦éµå¾ªèˆ‡ç¨½æ ¸ï¼ˆCompliance & Auditingï¼‰**
4. **æå‡ä½¿ç”¨è€…é«”é©—ï¼ˆUXï¼‰**ï¼šè§€å¯Ÿå›æ‡‰å“è³ªã€åå¥½ã€äº’å‹•æ­·å²
5. **å®‰å…¨æ€§**ï¼šè¨˜éŒ„å¯ç”¨ä¾†æª¢æŸ¥æ˜¯å¦æœ‰æƒ¡æ„ä½¿ç”¨æˆ–æ¿«ç”¨æ¨¡å‹

### 6.1. Logging in LLM Applicationsï¼ˆæ¶æ§‹æµç¨‹ï¼‰

```
User Input
   â†“
LLM Application
   â”œâ”€â”€ Log User Input
   â”œâ”€â”€ Log Model Response
   â”œâ”€â”€ Log Errors & Exceptions
   â””â”€â”€ Log Performance Metrics
             â†“
          Log Storageï¼ˆå„²å­˜åœ¨æª”æ¡ˆã€è³‡æ–™åº«æˆ–é›²ç«¯ï¼‰
```

### 6.2. Logging Lifecycleï¼ˆæ—¥å¸¸é‹ä½œç”Ÿå‘½é€±æœŸï¼‰

```
User Interaction
   â†“
Log Eventsï¼ˆäº‹ä»¶è¨˜éŒ„ï¼‰
   â†“
Store Logsï¼ˆå„²å­˜ logï¼‰
   â†“
Analyze Logsï¼ˆåˆ†æè¨˜éŒ„ï¼‰
   â†“
Identify Issuesï¼ˆæ‰¾å‡ºå•é¡Œï¼‰
   â†“
Fix & Improveï¼ˆä¿®æ­£ä¸¦æ”¹é€²ï¼‰
   â†“
Better UXï¼ˆä½¿ç”¨è€…é«”é©—æå‡ï¼‰
```

### 6.3. Labï¼šèˆ‡ ollama äº¤äº’ï¼Œä¸¦ logging

```python
import logging
import json
from datetime import datetime
import uuid
import ollama

# ====== Step 1: å»ºç«‹ Logging è¨­å®š ======
def setup_logging():
    # å»ºç«‹ä¸€å€‹ logger ç‰©ä»¶ï¼Œåå­—å« "structured_logger"
    logger = logging.getLogger("structured_logger")
    logger.setLevel(logging.INFO)  # è¨˜éŒ„ç­‰ç´šè¨­ç‚º INFOï¼ˆæœƒè¨˜éŒ„ info ä»¥ä¸Šï¼‰

    # è‹¥ logger å·²ç¶“æœ‰ handlerï¼Œå°±æ¸…ç©ºï¼ˆé¿å…é‡è¤‡è¼¸å‡ºï¼‰
    if logger.hasHandlers():
        logger.handlers.clear()

    # âŠ æª”æ¡ˆè¼¸å‡ºï¼šlog ç´€éŒ„æœƒå¯«åˆ° chat_logs.json
    file_handler = logging.FileHandler("chat_logs.json")
    file_formatter = logging.Formatter('%(message)s')  # åªå¯«å‡ºç´” JSON å…§å®¹
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    # â‹ çµ‚ç«¯æ©Ÿè¼¸å‡ºï¼šlog ä¹Ÿæœƒå°åˆ°è¢å¹•ä¸Šï¼ˆæ™‚é–“ + è¨Šæ¯ï¼‰
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter('%(asctime)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    return logger  # å›å‚³é€™å€‹è¨­å®šå¥½çš„ logger


# ====== Step 2: ä¸»èŠå¤©æµç¨‹ ======
def main():
    logger = setup_logging()  # åˆå§‹åŒ– logger
    session_id = str(uuid.uuid4())  # æ¯æ¬¡å•Ÿå‹•çµ¦ä¸€å€‹å”¯ä¸€å°è©± session ç·¨è™Ÿ

    print("ğŸ’¬ ä½¿ç”¨ Ollama ä¸¦ç´€éŒ„å®Œæ•´å°è©± (è¼¸å…¥ 'exit' çµæŸ)")

    # åˆå§‹åŒ– messagesï¼Œä¸¦è¨­å®š AI åŠ©ç†çš„è§’è‰²
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½è¦ªåˆ‡çš„ AI åŠ©ç†ï¼Œæœƒç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"}
    ]

    while True:
        # ç­‰å¾…ä½¿ç”¨è€…è¼¸å…¥
        user_input = input("ä½ ï¼š").strip()

        # è‹¥è¼¸å…¥ exitï¼Œé›¢é–‹ç¨‹å¼
        if user_input.lower() == "exit":
            print("ğŸ‘‹ çµæŸç´€éŒ„ï¼Œå†è¦‹ï¼")
            break

        # ç©ºå­—ä¸²å°±è·³é
        if not user_input:
            continue

        # åŠ å…¥ user è¼¸å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­
        messages.append({"role": "user", "content": user_input})

        # è¨˜éŒ„è«‹æ±‚é–‹å§‹æ™‚é–“
        start = datetime.now()

        try:
            # å‘¼å« Ollamaï¼Œå‚³å…¥ç›®å‰æ‰€æœ‰ä¸Šä¸‹æ–‡ messages
            response = ollama.chat(
                model="llama3.2",
                messages=messages
            )
        except Exception as e:
            print("âš ï¸ Ollama å›æ‡‰å¤±æ•—ï¼š", str(e))
            continue  # è·³éé€™è¼ªè¼¸å…¥

        # è¨˜éŒ„è«‹æ±‚çµæŸæ™‚é–“
        end = datetime.now()

        # å–å¾— AI å›è¦†çš„æ–‡å­—
        ai_text = response["message"]["content"]
        print(f"AIï¼š{ai_text}\n")

        # æŠŠ AI çš„å›æ‡‰ä¹ŸåŠ å…¥ messagesï¼ˆç¶­æŒè¨˜æ†¶ï¼‰
        messages.append({"role": "assistant", "content": ai_text})

        # å»ºç«‹ä¸€ç­†å®Œæ•´ logï¼ˆä½¿ç”¨è€…è¼¸å…¥ + æ¨¡å‹å›è¦† + æ™‚é–“ + session IDï¼‰
        log_entry = {
            "timestamp": datetime.utcnow().isoformat() + "Z",  # UTC æ™‚é–“æ¨™æº–
            "session_id": session_id,                          # ç•¶å‰å°è©± ID
            "user_input": user_input,                          # ä½¿ç”¨è€…çš„å•é¡Œ
            "model_response": ai_text,                         # æ¨¡å‹çš„å›ç­”
            "response_time": round((end - start).total_seconds(), 2),  # å›è¦†ç§’æ•¸
            "tokens_used": None  # âš  Ollama æ²’æœ‰æä¾› tokensï¼Œæ‰€ä»¥é è¨­ç‚º None
        }

        # è¼¸å‡ºåˆ° log æª”æ¡ˆèˆ‡çµ‚ç«¯æ©Ÿ
        logger.info(json.dumps(log_entry, ensure_ascii=False))


# ====== ä¸»ç¨‹å¼é€²å…¥é» ======
if __name__ == "__main__":
    main()
```

## 7. Transformer Libraryï¼ˆå¦‚ HuggingFaceï¼‰

| å…ƒä»¶         | èªªæ˜                                   |
| ---------- | ------------------------------------ |
| Models     | é è¨“ç·´æ¨¡å‹ï¼ˆå¦‚ GPTã€BERTã€T5ï¼‰                 |
| Tokenizers | æ”¯æ´å¤šç¨®åˆ†è©å™¨ï¼ˆBPEã€WordPieceã€SentencePieceï¼‰ |
| Pipelines  | å¹«ä½ å¿«é€Ÿå¥—ç”¨æ¨¡å‹åˆ°å„ç¨®å¸¸è¦‹ä»»å‹™ï¼Œä¸ç”¨å¯«å¤ªå¤šç¨‹å¼              |

æ¨¡å‹é¸æ“‡çš„é—œéµæŒ‡æ¨™ï¼ˆDecision Factorsï¼‰
- **é ç®—é™åˆ¶ï¼ˆBudgetï¼‰** 
- **è³‡æ–™éš±ç§éœ€æ±‚ï¼ˆPrivacyï¼‰**
- **æŠ€è¡“è³‡æºï¼ˆEngineeringï¼‰**
- **æ•ˆèƒ½éœ€æ±‚ï¼ˆLatency/Throughputï¼‰**
- **æ˜¯å¦éœ€è¦è‡ªè¨‚æ¨¡å‹è¡Œç‚ºï¼ˆCustomizationï¼‰**

### 7.1. Labï¼šä½¿ç”¨hugging faceä¸Šçš„Transformer piplineé€²è¡Œæ–‡å­—ç”Ÿæˆ
è¦å…ˆåœ¨hugging faceä¸Šå»çš„access tokenï¼Œæœƒåœ¨huggingface-cli loginè¦è¼¸å…¥
ä½¿ç”¨æ¨¡å‹ï¼š[uer/gpt2-chinese-cluecorpussmall Â· Hugging Face](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall)
```python
# å®‰è£
!pip install transformers torch
!pip install huggingface_hub

# åœ¨çµ‚ç«¯æ©Ÿè¼¸å…¥
# huggingface-cli login 
```

```python
# ä½¿ç”¨æ¨¡å‹uer/gpt2-chinese-cluecorpussmall 

from transformers import pipeline

# ä½¿ç”¨é–‹æ”¾ä¸­æ–‡ GPT2 æ¨¡å‹ï¼ˆé©åˆä¸­æ–‡ç”Ÿæˆï¼‰
generator = pipeline("text-generation", model="uer/gpt2-chinese-cluecorpussmall", device=0)

prompt = "å¾å‰å¾å‰æœ‰ä¸€ä½å‹‡æ•¢çš„å¥³å­©ï¼Œå¥¹æ±ºå®šè¸ä¸Šæ—…ç¨‹å»å°‹æ‰¾"
output = generator(prompt, max_length=100, do_sample=True, temperature=0.8)

print(output[0]["generated_text"].replace(" ", "").strip())
```

## 8. ä½¿ç”¨ ChatGPT API é–‹ç™¼æ‡‰ç”¨


APIï¼ˆApplication Programming Interface ï¼‰ï¼š è®“ä½ çš„ç¨‹å¼å¯ä»¥**å‘¼å«å…¶ä»–äººçš„æ¨¡å‹/æœå‹™**ï¼Œä¸å¿…è‡ªå·±è¨“ç·´
èˆ‰ä¾‹ï¼š
- ä½ çš„ç¶²é ä½¿ç”¨è€…è¼¸å…¥å•é¡Œ
- å¾Œç«¯å°‡å•é¡Œç™¼é€è‡³ OpenAI API
- API å›å‚³å›ç­”ï¼Œå‰ç«¯é¡¯ç¤ºå‡ºä¾†
å¥½è™•
- å¿«é€Ÿé–‹ç™¼ï¼šä¸å¿…è¨“ç·´æ¨¡å‹ 
- å¼·å¤§å·¥å…·æ¥å…¥ï¼šåƒ GPT-4ã€LLaMA2ã€Claude ç­‰
- å®¹æ˜“æ“´å±•ï¼šå¤šä½¿ç”¨è€…ã€å¤šæœå‹™æ•´åˆ


| æ­¥é©Ÿ              | èªªæ˜                                       |
| --------------- | ---------------------------------------- |
| i. è¨­å®šç’°å¢ƒ         | å»ºç«‹ Python å°ˆæ¡ˆï¼Œå®‰è£ `openai` æˆ– `requests` å¥—ä»¶ |
| ii. API é‡‘é‘°ç®¡ç†    | ç”³è«‹ä¸¦ä¿ç®¡å¥½ API keyï¼ˆä¾‹å¦‚ ChatGPT çš„ OpenAI Keyï¼‰  |
| iii. ç¬¬ä¸€æ¬¡ API å‘¼å« | æ¸¬è©¦åŸºæœ¬çš„ `POST` è«‹æ±‚é€å…¥ prompt                 |
| iv. prompt æ¸¬è©¦   | è©¦ä¸åŒå•é¡Œã€èª¿æ•´å›æ‡‰æ ¼å¼                             |
| v. æˆæœ¬ç›£æ§         | æ§åˆ¶ token æ•¸é‡ã€ç®¡ç†å¸³å–®æˆæœ¬ï¼ˆå°¤å…¶æ˜¯ GPT-4ï¼‰            |



### 8.1. Labï¼šä½¿ç”¨ OpenAI API å‘¼å« ChatGPT æ¨¡å‹
è¦å…ˆç”³è«‹å¥½open apiï¼Œæ”¾åœ¨.envä¸­

```python
from openai import OpenAI 
import os

from dotenv import load_dotenv

load_dotenv()  # è®€å– .env ä¸­çš„è®Šæ•¸
client = OpenAI()


# å»ºç«‹èŠå¤©è«‹æ±‚
response = client.chat.completions.create(
    model="gpt-4o",  # ä½¿ç”¨çš„æ¨¡å‹ï¼ˆä½ ä¹Ÿå¯ä»¥æ”¹æˆ gpt-4ã€gpt-4oã€gpt-4o-miniã€gpt-3.5 ç­‰ï¼‰
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æ±æ–¹è©©äººã€‚"},  # æŒ‡å®šç³»çµ±è§’è‰²ç‚ºè©©äºº
        {
            "role": "user",
            "content": """è«‹å¯«ä¸€é¦–é—œæ–¼æœˆäº®çš„çŸ­è©©ã€‚
               ä½¿ç”¨ä¿³å¥ï¼ˆHaikuï¼‰çš„é¢¨æ ¼ä¾†å¯«ã€‚
               è«‹è¨˜å¾—åŠ ä¸Šè©©çš„æ¨™é¡Œã€‚""",  # ä½¿ç”¨è€…çš„è«‹æ±‚å…§å®¹
        },
    ],
)

print(response.choices[0].message.content)
```
![upgit_20250414_1744642588.png|658x424](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250414_1744642588.png)

## 9. Ollama 
**Ollama æ˜¯ä¸€å€‹å‘½ä»¤åˆ—å·¥å…·ï¼ˆCLIï¼‰**ï¼Œç”¨ä¾†åœ¨æœ¬åœ°ç«¯å®‰è£èˆ‡åŸ·è¡Œ LLM æ¨¡å‹ã€‚
ç›®æ¨™ï¼šç°¡åŒ–æœ¬åœ°éƒ¨ç½² LLM çš„æµç¨‹ã€‚
é¡ä¼¼æ–¼ Hugging Face + Docker çš„çµåˆé«”ï¼Œä½†æ›´ç°¡åŒ–ã€æ›´é‡å° LLM ä½¿ç”¨ã€‚


### 9.1. æ ¸å¿ƒç‰¹é»

- **Model Management**ï¼šé›†ä¸­ç®¡ç†å¤šå€‹æ¨¡å‹
- **Unified Interface**ï¼šçµ±ä¸€çš„ CLI æ“ä½œä»‹é¢
- **Extensibility**ï¼šå®¹æ˜“æ“´å……åŠŸèƒ½
- **Performance Optimization**ï¼šé‡å°ç¡¬é«”æ•ˆèƒ½æœ€ä½³åŒ–


### 9.2. Ollama çš„åº•å±¤æµç¨‹ç°¡ä»‹
ç›¸é—œï¼š[ã€Day 26ã€‘- Ollama: é©å‘½æ€§å·¥å…·è®“æœ¬åœ° AI é–‹ç™¼è§¸æ‰‹å¯åŠ - å¾å®‰è£åˆ°é€²éšæ‡‰ç”¨çš„å®Œæ•´æŒ‡å— - iT é‚¦å¹«å¿™::ä¸€èµ·å¹«å¿™è§£æ±ºé›£é¡Œï¼Œæ‹¯æ•‘ IT äººçš„ä¸€å¤©](https://ithelp.ithome.com.tw/articles/10348913)

1. ä½¿ç”¨è€…ä¸‹é” **query**ï¼ˆå•é¡Œï¼‰
2. æ–‡ä»¶è¢«åˆ†å‰²ç‚º **Chunks**
3. ä½¿ç”¨ **Embedding LLM** å°‡æ‰€æœ‰ chunk å‘é‡åŒ–
4. æ‰¾å‡ºæœ€ç›¸ä¼¼çš„ç‰‡æ®µï¼ˆTop-K retrievalï¼‰
5. å‚³å…¥ç”Ÿæˆå¼ LLMï¼Œå›å‚³ **å›æ‡‰ï¼ˆresponseï¼‰**

![upgit_20250414_1744646186.png|787x378](https://raw.githubusercontent.com/kcwc1029/obsidian-upgit-image/main/2025/04/upgit_20250414_1744646186.png)

### 9.3. è§£æ±ºä»€éº¼å•é¡Œï¼Ÿ

| å•é¡Œ      | Ollama è§£æ³•         |     |
| ------- | ----------------- | --- |
| âœ… éš±ç§å•é¡Œ  | æ¨¡å‹åœ¨æœ¬åœ°åŸ·è¡Œï¼Œä¸éœ€å°‡è³‡æ–™é€ä¸Šé›²ç«¯ |     |
| âœ… éƒ¨ç½²å›°é›£  | ä¸€è¡ŒæŒ‡ä»¤å°±èƒ½åŸ·è¡Œ LLM      |     |
| âœ… æˆæœ¬é«˜   | ç„¡é ˆè¨‚é–±é›²ç«¯ API        |     |
| âœ… å»¶é²é«˜   | æœ¬åœ°æ¨¡å‹ï¼Œå¹¾ä¹é›¶å»¶é²        |     |
| âœ… å®¢è£½åŒ–å›°é›£ | å¯è‡ªè¡Œé¸æ“‡æ¨¡å‹ä¸¦é€²è¡Œå¾®èª¿      |     |


### 9.4. åŸºæœ¬æŒ‡ä»¤
```
# æŸ¥çœ‹æ‰€æœ‰æŒ‡ä»¤èªªæ˜
ollama help 

# æŸ¥çœ‹æ‰€æœ‰å·²ä¸‹è¼‰æ¨¡å‹
ollama list

# å®‰è£æŒ‡ä»¤æ¨¡å‹
ollama run llama3:3b

# åˆªé™¤æŸå€‹æ¨¡å‹ï¼ˆé‡‹æ”¾ç©ºé–“ï¼‰
ollama remove llama3:3b

##### èŠå¤©æ¨¡å¼æŒ‡ä»¤ #####

# ä½¿ç”¨æŸå€‹ prompt å•Ÿå‹•ï¼ˆä¾‹å¦‚ç³»çµ±è§’è‰²ï¼‰
ollama run llama3 --system "ä½ æ˜¯ä¸€ä½ç†±æƒ…çš„æ•¸å­¸å®¶"

# æ¸…é™¤èŠå¤©ä¸Šä¸‹æ–‡ï¼ˆé‡æ–°é–‹å§‹ï¼‰
/clear

/help

# æ¨¡å‹è³‡è¨Š
/show info
```

```
>>> /show info
  Model
    architecture        llama # æ¨¡å‹æ¶æ§‹
    parameters          3.2B # åƒæ•¸é‡ï¼šä»£è¡¨æ¨¡å‹çš„å¤§å°èˆ‡å­¸ç¿’èƒ½åŠ›ã€‚
    context length      131072 # ä¸Šä¸‹æ–‡è™•ç†é•·åº¦
    embedding length    3072 #  è©åµŒå…¥ç¶­åº¦(æ•¸å€¼è¶Šå¤§ä»£è¡¨æ¨¡å‹èƒ½å¤ æ›´ç´°ç·»åœ°ç†è§£èªæ„)
    quantization        Q4_K_M #  é‡åŒ–æ–¹å¼
```

### 9.5. è‡ªå®šç¾©æ¨¡å‹è¨­å®šæª” Modelfile
Modelfile æ˜¯ä»€éº¼ï¼šæœ‰é»åƒ Ollama çš„ã€Œè¨­å®šæª”ã€ï¼Œå¯è®“ä½ åŸºæ–¼ä¸€å€‹ç¾æœ‰æ¨¡å‹ï¼ˆå¦‚ llama3ï¼‰å®šç¾©
- é è¨­è§’è‰²ã€èªæ°£
- æº«åº¦ï¼ˆå‰µé€ åŠ›ï¼‰
- åˆå§‹ prompt
- æŒ‡å®š tokenizer æˆ–å…¶ä»–è¨­å®š

STEP01ï¼šå»ºç«‹æª”æ¡ˆã€Modefileã€‘
```
# æŒ‡å®šä½ è¦åŸºæ–¼å“ªä¸€å€‹æ¨¡å‹ä¾†å»ºç«‹æ–°çš„æ¨¡å‹
FROM llama3.2:latest

##### åƒæ•¸(PARAMETER) #####
PARAMETER temperature 0.3 # å‰µæ„ç¨‹åº¦
PARAMETER top_p 0.9
# æœ€å¤§ç”Ÿæˆé•·åº¦
PARAMETER num_predict 1024
# é˜²æ­¢é‡è¤‡å‡ºç¾ç›¸åŒè©ã€‚å»ºè­°å€¼ï¼š1.1 ~ 1.3
PARAMETER repeat_penalty 1.2

##### SYSTEMï¼šè¨­å®š AI åŠ©ç†çš„è§’è‰²/èªæ°£ ##### 
SYSTEM """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è€å¸«ï¼Œæ“…é•·ç”¨æ·ºé¡¯ä¾‹å­è§£é‡‹è¤‡é›œæ¦‚å¿µã€‚
"""

##### LICENSE / MESSAGE / MODIFIERï¼ˆå¯é¸è£œå……ï¼‰##### 
# é¡¯ç¤ºåœ¨ ollama list ä¸­çš„è³‡è¨Šæç¤º
LICENSE MIT
MESSAGE "æœ¬æ¨¡å‹ç”¨é€”ç‚ºæ•™è‚²ç ”ç©¶ï¼Œè«‹å‹¿æ¿«ç”¨"
```

STEP02ï¼šä½¿ç”¨ã€Modefileã€‘å»ºç«‹æ¨¡å‹ï¼š
```
ollama create my-teacher -f Modelfile
```
STEP03ï¼šä½¿ç”¨æ¨¡å‹
```
ollama run my-teacher
```

### 9.6. è®“ Ollama è®Šæˆ REST APIï¼Œä¸¦ä½¿ç”¨pythoné€²è¡Œäº¤äº’
å•Ÿå‹• Ollama å¾Œï¼Œå®ƒæœƒé–‹ä¸€å€‹æœ¬åœ°ç«¯ API Serverï¼š
```
http://localhost:11434 => åªè¦å•Ÿå‹•äº† Ollamaï¼Œå°±å¯ä»¥ç”¨ POST æ–¹å¼å‘¼å«é€™å€‹ APIã€‚
# ç›¸é—œæ‡‰ç”¨ï¼Œä½¿ç”¨ Python å‘¼å« Ollama API
```

```python
# ä½¿ç”¨ Python å‘¼å« Ollama API
import requests
import json

url = "http://localhost:11434/api/generate"

payload = {
    "model": "llama3.2:latest",
    "prompt": "è«‹ç”¨ä¸€æ®µè©±ä»‹ç´¹ Arduino æ˜¯ä»€éº¼ã€‚",
    "stream": False
}

response = requests.post(url, json=payload, stream=True)
print(response.json()["response"])
```

#### 9.6.1. å¦‚æœä½ æƒ³å¾ã€Œåˆ¥çš„é›»è…¦ã€å­˜å– Ollamaï¼ˆè®Šæˆ Web æœå‹™ï¼‰ï¼Ÿ
- æ‰¾åˆ°ä½ è·‘ Ollama ä¸»æ©Ÿçš„ IPï¼Œä¾‹å¦‚ï¼š192.168.1.100
- é–‹æ”¾é˜²ç«ç‰†èˆ‡åŸ è™Ÿï¼ˆ11434ï¼‰
- å¾Œå…¶ä»–æ©Ÿå™¨å°±å¯ä»¥é€é`http://192.168.1.100:11434/api/generate`é€£ç·š

#### 9.6.2. âœ… Ollama æ‰€æœ‰å¸¸ç”¨ REST API Endpoint æ•´ç†

|Endpoint|æ–¹æ³•|ç”¨é€”èªªæ˜|
|---|---|---|
|`/api/generate`|POST|âœ…**ç”¢ç”Ÿæ–‡å­—å›æ‡‰**ï¼ˆé¡ä¼¼ ChatGPT å›æ‡‰ï¼‰|
|`/api/chat`|POST|âœ…**å…·å°è©±ä¸Šä¸‹æ–‡çš„èŠå¤©æ¨¡å¼**ï¼ˆå¤šè¼ªå°è©±ï¼‰|
|`/api/models`|GET|æŸ¥çœ‹å·²å®‰è£çš„æ¨¡å‹åˆ—è¡¨|
|`/api/pull`|POST|âœ… ä¸‹è¼‰ï¼ˆpullï¼‰æ¨¡å‹|
|`/api/create`|POST|å»ºç«‹è‡ªå®šæ¨¡å‹ï¼ˆé…åˆ `Modelfile` ä½¿ç”¨ï¼‰|
|`/api/delete`|DELETE|åˆªé™¤å·²å®‰è£çš„æ¨¡å‹|
|`/api/embeddings`|POST|ç”¢ç”Ÿæ–‡å­—çš„å‘é‡ï¼ˆembeddingï¼Œç”¨æ–¼æœå°‹ã€åˆ†é¡ç­‰ï¼‰|
|`/api/stop`|POST|æ‰‹å‹•åœæ­¢ç”Ÿæˆä¸­è«‹æ±‚ï¼ˆä¾‹å¦‚ä½¿ç”¨è€…é»å–æ¶ˆï¼‰|

### 9.7. æ­é…æ­é…ollamaä½¿ç”¨çš„åœ–å½¢ä»‹é¢APPï¼šmsty
[Msty - Using AI Models made Simple and Easy](https://msty.app/)

### 9.8. pythonäº¤äº’llamaåº«

```python
#### åŸºæœ¬èŠå¤© #####
import ollama

res = ollama.chat(
    model="llama3.2:latest",
    messages=[
        {"role": "user", "content": "ä½ æœ‰éˆé­‚å—"},
    ],
)

print(res["message"]["content"])
```

```python
#### Chat example streaming #####
import ollama

res = ollama.chat(
    model="llama3.2:latest",
    messages=[
        {"role": "user", "content": "ä½ æœ‰éˆé­‚å—"},
    ],
    stream=True,
)

for chunk in res:
    print(chunk["message"]["content"], end="", flush=True)
```

```python
#### Create a new model with modelfile #####
import ollama

modelfile = """
FROM llama3.2:latest
PARAMETER temperature 0.4
SYSTEM ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è€å¸«ï¼Œæ“…é•·ç”¨æ·ºé¡¯ä¾‹å­è§£é‡‹è¤‡é›œæ¦‚å¿µã€‚
"""

ollama.create(model="knowitall", modelfile=modelfile)

res = ollama.generate(model="knowitall", prompt="why is the ocean so salty?")
print(res["response"])

ollama.delete("knowitall") # delete model
```