{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5215093",
   "metadata": {},
   "source": [
    "## Readme\n",
    "é€™å€‹ Notebook æ˜¯ä¸€å€‹ç°¡æ˜“ç‰ˆçš„ RAGï¼ˆRetrieval-Augmented Generationï¼‰ç¤ºç¯„ç³»çµ±ã€‚\n",
    "\n",
    "ä¸»è¦åŠŸèƒ½åŒ…å«ï¼š\n",
    "-    å»ºç«‹å°å‹çŸ¥è­˜è³‡æ–™é›†ï¼ˆè‡ªå‹•ç”¢ç”Ÿå¤ªç©ºå°çŸ¥è­˜ä¸¦å„²å­˜æˆ CSVï¼‰\n",
    "-    å°‡çŸ¥è­˜è½‰æ›æˆå‘é‡ä¸¦å­˜å…¥ ChromaDB\n",
    "-    æ¥æ”¶ä½¿ç”¨è€…å•é¡Œï¼Œå¾è³‡æ–™åº«æœå°‹æœ€ç›¸é—œçš„çŸ¥è­˜\n",
    "-    æŠŠæ‰¾åˆ°çš„çŸ¥è­˜ï¼Œçµ„æˆæç¤ºèªï¼ˆPromptï¼‰ï¼Œäº¤çµ¦ LLM æ¨¡å‹å›ç­”\n",
    "-    æ”¯æ´åˆ‡æ›ä¸åŒçš„åµŒå…¥æ¨¡å‹ï¼ˆOpenAIã€Chromaã€Ollamaï¼‰èˆ‡ LLMï¼ˆGPT-4oã€Llama3.2ï¼‰\n",
    "\n",
    "\n",
    "ğŸ”¥ é€™ä»½æª”æ¡ˆèƒ½åšä»€éº¼\n",
    "- ç·´ç¿’å¦‚ä½•å°‡ã€Œè³‡æ–™ã€â”ã€Œå‘é‡åŒ–ã€â”ã€Œå»ºç´¢å¼•ã€â”ã€ŒæŸ¥è©¢ã€â”ã€Œç”Ÿæˆå›ç­”ã€çš„å®Œæ•´æµç¨‹\n",
    "- å­¸ç¿’æœ€åŸºæœ¬çš„ RAG ç³»çµ±éª¨æ¶\n",
    "- ç‚ºä»¥å¾Œæ¥æ›´å¤§å‹çš„çŸ¥è­˜æª¢ç´¢ç³»çµ±æ‰“åŸºç¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4a126",
   "metadata": {},
   "source": [
    "### è¼‰å…¥å¥—ä»¶ã€è®€å– API é‡‘é‘°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef60bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb1e43",
   "metadata": {},
   "source": [
    "### å®šç¾© EmbeddingModel é¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "442cf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€™å€‹é¡åˆ¥è² è²¬æ±ºå®šè¦ç”¨å“ªç¨®æ–¹æ³•æŠŠæ–‡å­—è½‰æˆå‘é‡ï¼Œä½ å¯ä»¥é¸æ“‡\n",
    "# openaiï¼ˆç”¨ OpenAI æ¨¡å‹è½‰å‘é‡ï¼‰\n",
    "# chromaï¼ˆç”¨ ChromaDB é è¨­å…§å»ºçš„è½‰å‘é‡ï¼‰\n",
    "# nomicï¼ˆç”¨ Ollama æœ¬åœ°æ¨¡å‹è½‰å‘é‡ï¼‰\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_type=\"openai\"):\n",
    "        self.model_type = model_type\n",
    "        if model_type == \"openai\":\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "            self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=api_key,\n",
    "                model_name=\"text-embedding-3-small\",\n",
    "            )\n",
    "        elif model_type == \"chroma\":\n",
    "            self.embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
    "        elif model_type == \"nomic\":\n",
    "            # using Ollama nomic-embed-text model\n",
    "            self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"ollama\",\n",
    "                api_base=\"http://localhost:11434/v1\",\n",
    "                model_name=\"nomic-embed-text\",\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026209eb",
   "metadata": {},
   "source": [
    "### å®šç¾© LLMModel é¡åˆ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bad725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€™å€‹é¡åˆ¥è² è²¬æ±ºå®šç”¨å“ªå€‹å¤§èªè¨€æ¨¡å‹ (LLM) ä¾†å›ç­”å•é¡Œï¼Œä½ å¯ä»¥é¸æ“‡ï¼š\n",
    "#     openaiï¼ˆç”¨ OpenAI GPTï¼‰\n",
    "#     ollamaï¼ˆç”¨æœ¬åœ° Llama æ¨¡å‹ï¼‰\n",
    "# åŒæ™‚ä¹Ÿæä¾›ä¸€å€‹ generate_completion() æ–¹æ³• â” æŠŠå•é¡Œä¸Ÿé€²å»ï¼Œå¾—åˆ°å›ç­”ã€‚\n",
    "\n",
    "class LLMModel:\n",
    "    def __init__(self, model_type=\"openai\"):\n",
    "        self.model_type = model_type\n",
    "        if model_type == \"openai\":\n",
    "            self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "            self.model_name = \"gpt-4o-mini\"\n",
    "        else:\n",
    "            self.client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "            self.model_name = \"llama3.2\"\n",
    "\n",
    "    def generate_completion(self, messages):\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                temperature=0.0,  # 0.0 is deterministic\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2622f6d",
   "metadata": {},
   "source": [
    "### é¸æ“‡æ¨¡å‹çš„ä»‹é¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63b2128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨ terminal ä¸Šè®“ä½ è‡ªå·±é¸è¦ç”¨å“ªç¨®æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š\n",
    "#     å“ªå€‹ LLMï¼ˆOpenAI é‚„æ˜¯ Ollamaï¼‰\n",
    "#     å“ªå€‹å‘é‡è½‰æ›å™¨ï¼ˆOpenAI / Chroma / Nomicï¼‰\n",
    "\n",
    "def select_models():\n",
    "    # Select LLM Model\n",
    "    print(\"\\nSelect LLM Model:\")\n",
    "    print(\"1. OpenAI GPT-4\")\n",
    "    print(\"2. Ollama Llama2\")\n",
    "    while True:\n",
    "        choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "        if choice in [\"1\", \"2\"]:\n",
    "            llm_type = \"openai\" if choice == \"1\" else \"ollama\"\n",
    "            break\n",
    "        print(\"Please enter either 1 or 2\")\n",
    "\n",
    "    # Select Embedding Model\n",
    "    print(\"\\nSelect Embedding Model:\")\n",
    "    print(\"1. OpenAI Embeddings\")\n",
    "    print(\"2. Chroma Default\")\n",
    "    print(\"3. Nomic Embed Text (Ollama)\")\n",
    "    while True:\n",
    "        choice = input(\"Enter choice (1, 2, or 3): \").strip()\n",
    "        if choice in [\"1\", \"2\", \"3\"]:\n",
    "            embedding_type = {\"1\": \"openai\", \"2\": \"chroma\", \"3\": \"nomic\"}[choice]\n",
    "            break\n",
    "        print(\"Please enter 1, 2, or 3\")\n",
    "\n",
    "    return llm_type, embedding_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98dcaf",
   "metadata": {},
   "source": [
    "### è‡ªå‹•ç”¢ç”Ÿä¸€å€‹å°å‹ CSV\n",
    "å…ˆå»ºç«‹ä¸€å€‹è³‡æ–™æ–‡ä»¶ï¼Œä¾†æ¨¡æ“¬è¦åšçš„è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "806d13f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSVæª”æ¡ˆ 'space_facts.csv' å·²æˆåŠŸå»ºç«‹ï¼\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def generate_csv():\n",
    "    facts = [\n",
    "        {\"id\": 1, \"fact\": \"ç¬¬ä¸€ä½ç’°ç¹åœ°çƒé£›è¡Œçš„äººé¡æ˜¯å°¤é‡ŒÂ·åŠ åŠ æ—ï¼ˆ1961å¹´ï¼‰ã€‚\"},\n",
    "        {\"id\": 2, \"fact\": \"é˜¿æ³¢ç¾…11è™Ÿä»»å‹™æ–¼1969å¹´é¦–æ¬¡å°‡äººé¡é€ä¸Šæœˆçƒã€‚\"},\n",
    "        {\"id\": 3, \"fact\": \"å“ˆä¼¯å¤ªç©ºæœ›é é¡æ–¼1990å¹´ç™¼å°„ï¼Œæä¾›äº†å®‡å®™çš„é©šäººå½±åƒã€‚\"},\n",
    "        {\"id\": 4, \"fact\": \"ç«æ˜Ÿæ˜¯å¤ªé™½ç³»ä¸­è¢«æ¢ç´¢æœ€å¤šçš„è¡Œæ˜Ÿï¼Œç¾åœ‹å¤ªç©ºç¸½ç½²å·²æ´¾å‡ºå¤šå°æ¢æ¸¬è»Šã€‚\"},\n",
    "        {\"id\": 5, \"fact\": \"åœ‹éš›å¤ªç©ºç«™ï¼ˆISSï¼‰è‡ª2000å¹´11æœˆä»¥ä¾†ä¸€ç›´æŒçºŒæœ‰äººå±…ä½ã€‚\"},\n",
    "        {\"id\": 6, \"fact\": \"æ—…è¡Œè€…1è™Ÿæ˜¯ç›®å‰é›¢åœ°çƒæœ€é çš„äººé€ ç‰©é«”ï¼Œæ–¼1977å¹´ç™¼å°„ã€‚\"},\n",
    "        {\"id\": 7, \"fact\": \"SpaceXç”±ä¼Šéš†Â·é¦¬æ–¯å…‹å‰µç«‹ï¼Œæ˜¯ç¬¬ä¸€å®¶å°‡äººé¡é€å…¥è»Œé“çš„ç§äººå…¬å¸ã€‚\"},\n",
    "        {\"id\": 8, \"fact\": \"è©¹å§†æ–¯Â·éŸ‹ä¼¯å¤ªç©ºæœ›é é¡æ–¼2021å¹´ç™¼å°„ï¼Œæ˜¯å“ˆä¼¯æœ›é é¡çš„å¾Œç¹¼è€…ã€‚\"},\n",
    "        {\"id\": 9, \"fact\": \"éŠ€æ²³ç³»åŒ…å«è¶…éä¸€åƒå„„é¡†æ†æ˜Ÿã€‚\"},\n",
    "        {\"id\": 10, \"fact\": \"é»‘æ´æ˜¯é‡åŠ›æ¥µå¼·ï¼Œé€£å…‰ä¹Ÿç„¡æ³•é€ƒè„«çš„æ™‚ç©ºå€åŸŸã€‚\"},\n",
    "    ]\n",
    "\n",
    "    with open(\"space_facts.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"id\", \"fact\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(facts)\n",
    "\n",
    "    print(\"CSVæª”æ¡ˆ 'space_facts.csv' å·²æˆåŠŸå»ºç«‹ï¼\")\n",
    "\n",
    "generate_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c39ec",
   "metadata": {},
   "source": [
    "### è¼‰å…¥ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b2b424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    df = pd.read_csv(\"./space_facts.csv\")\n",
    "    documents = df[\"fact\"].tolist()\n",
    "    # print(\"è¼‰å…¥æ–‡ä»¶\")\n",
    "    # for doc in documents:\n",
    "    #     print(f\"- {doc}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443772b",
   "metadata": {},
   "source": [
    "### æŠŠè³‡æ–™å¡é€² ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "156f0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chromadb(documents, embedding_model):\n",
    "    client = chromadb.Client()\n",
    "    try:\n",
    "        client.delete_collection(\"space_facts\")\n",
    "    except:\n",
    "        pass\n",
    "    collection = client.create_collection(\n",
    "        name=\"space_facts\", embedding_function=embedding_model.embedding_fn\n",
    "    )\n",
    "    collection.add(documents=documents, ids=[str(i) for i in range(len(documents))])\n",
    "    print(\"æ–‡ä»¶å·²æˆåŠŸåŠ å…¥ ChromaDB ï¼\\n\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aab9c",
   "metadata": {},
   "source": [
    "### æ ¹æ“šå•é¡Œï¼Œæ‰¾ç›¸é—œçš„å°æ®µè³‡æ–™\n",
    "ç•¶ä½ å•ä¸€å€‹å•é¡Œ\n",
    "\n",
    "å» ChromaDB è³‡æ–™åº«æ‰¾æœ€æ¥è¿‘å•é¡Œæ„æ€çš„å°çŸ¥è­˜\n",
    "\n",
    "å›å‚³æ‰¾å‡ºä¾†çš„è³‡æ–™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58f639aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_chunks(query, collection, top_k=2):\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "\n",
    "    print(\"\\nRelated chunks found:\")\n",
    "    for doc in results[\"documents\"][0]:\n",
    "        print(f\"- {doc}\")\n",
    "\n",
    "    return list(\n",
    "        zip(\n",
    "            results[\"documents\"][0],\n",
    "            (\n",
    "                results[\"metadatas\"][0]\n",
    "                if results[\"metadatas\"][0]\n",
    "                else [{}] * len(results[\"documents\"][0])\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91d586",
   "metadata": {},
   "source": [
    "### æ•´ç† Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a5de03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query, related_chunks):\n",
    "    context = \"\\n\".join([chunk[0] for chunk in related_chunks])\n",
    "    augmented_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    print(\"\\nAugmented prompt: â¤µï¸\")\n",
    "    print(augmented_prompt)\n",
    "\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a510c6",
   "metadata": {},
   "source": [
    "### æ•´å€‹ RAG æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cf37c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, collection, llm_model, top_k=2):\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "\n",
    "    related_chunks = find_related_chunks(query, collection, top_k)\n",
    "    augmented_prompt = augment_prompt(query, related_chunks)\n",
    "\n",
    "    response = llm_model.generate_completion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant who can answer questions about space but only answers questions that are directly related to the sources/documents given.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": augmented_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"\\nGenerated response:\")\n",
    "    print(response)\n",
    "\n",
    "    references = [chunk[0] for chunk in related_chunks]\n",
    "    return response, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede085c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the RAG pipeline demo...\n",
      "\n",
      "Select LLM Model:\n",
      "1. OpenAI GPT-4\n",
      "2. Ollama Llama2\n",
      "\n",
      "Select Embedding Model:\n",
      "1. OpenAI Embeddings\n",
      "2. Chroma Default\n",
      "3. Nomic Embed Text (Ollama)\n",
      "\n",
      "Using LLM: OLLAMA\n",
      "Using Embeddings: CHROMA\n",
      "æ–‡ä»¶å·²æˆåŠŸåŠ å…¥ ChromaDB ï¼\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processing query: What is the Hubble Space Telescope?\n",
      "\n",
      "Processing query: What is the Hubble Space Telescope?\n",
      "\n",
      "Related chunks found:\n",
      "- éŠ€æ²³ç³»åŒ…å«è¶…éä¸€åƒå„„é¡†æ†æ˜Ÿã€‚\n",
      "- ç«æ˜Ÿæ˜¯å¤ªé™½ç³»ä¸­è¢«æ¢ç´¢æœ€å¤šçš„è¡Œæ˜Ÿï¼Œç¾åœ‹å¤ªç©ºç¸½ç½²å·²æ´¾å‡ºå¤šå°æ¢æ¸¬è»Šã€‚\n",
      "\n",
      "Augmented prompt: â¤µï¸\n",
      "Context:\n",
      "éŠ€æ²³ç³»åŒ…å«è¶…éä¸€åƒå„„é¡†æ†æ˜Ÿã€‚\n",
      "ç«æ˜Ÿæ˜¯å¤ªé™½ç³»ä¸­è¢«æ¢ç´¢æœ€å¤šçš„è¡Œæ˜Ÿï¼Œç¾åœ‹å¤ªç©ºç¸½ç½²å·²æ´¾å‡ºå¤šå°æ¢æ¸¬è»Šã€‚\n",
      "\n",
      "Question: What is the Hubble Space Telescope?\n",
      "Answer:\n",
      "\n",
      "Generated response:\n",
      "Unfortunately, I don't have any information about the Hubble Space Telescope from the provided context. The text only mentions the number of stars in a galaxy and the exploration of Mars by NASA.\n",
      "\n",
      "However, if you'd like to provide more context or information, I can try to help you answer your question about the Hubble Space Telescope.\n",
      "\n",
      "Final Results:\n",
      "------------------------------\n",
      "Response: Unfortunately, I don't have any information about the Hubble Space Telescope from the provided context. The text only mentions the number of stars in a galaxy and the exploration of Mars by NASA.\n",
      "\n",
      "However, if you'd like to provide more context or information, I can try to help you answer your question about the Hubble Space Telescope.\n",
      "\n",
      "References used:\n",
      "- éŠ€æ²³ç³»åŒ…å«è¶…éä¸€åƒå„„é¡†æ†æ˜Ÿã€‚\n",
      "- ç«æ˜Ÿæ˜¯å¤ªé™½ç³»ä¸­è¢«æ¢ç´¢æœ€å¤šçš„è¡Œæ˜Ÿï¼Œç¾åœ‹å¤ªç©ºç¸½ç½²å·²æ´¾å‡ºå¤šå°æ¢æ¸¬è»Šã€‚\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Processing query: Tell me about Mars exploration.\n",
      "\n",
      "Processing query: Tell me about Mars exploration.\n",
      "\n",
      "Related chunks found:\n",
      "- æ—…è¡Œè€…1è™Ÿæ˜¯ç›®å‰é›¢åœ°çƒæœ€é çš„äººé€ ç‰©é«”ï¼Œæ–¼1977å¹´ç™¼å°„ã€‚\n",
      "- éŠ€æ²³ç³»åŒ…å«è¶…éä¸€åƒå„„é¡†æ†æ˜Ÿã€‚\n",
      "\n",
      "Augmented prompt: â¤µï¸\n",
      "Context:\n",
      "æ—…è¡Œè€…1è™Ÿæ˜¯ç›®å‰é›¢åœ°çƒæœ€é çš„äººé€ ç‰©é«”ï¼Œæ–¼1977å¹´ç™¼å°„ã€‚\n",
      "éŠ€æ²³ç³»åŒ…å«è¶…éä¸€åƒå„„é¡†æ†æ˜Ÿã€‚\n",
      "\n",
      "Question: Tell me about Mars exploration.\n",
      "Answer:\n",
      "\n",
      "Generated response:\n",
      "Unfortunately, I don't have any information on Mars exploration from the provided sources. The text only mentions the distance of the Voyager 1 spacecraft and the number of stars in the Milky Way galaxy.\n",
      "\n",
      "However, if you'd like to know more about Mars exploration, I can suggest some general information or try to find additional resources for you. Alternatively, if you have any specific questions related to the provided sources, I'll do my best to help!\n",
      "\n",
      "Final Results:\n",
      "------------------------------\n",
      "Response: Unfortunately, I don't have any information on Mars exploration from the provided sources. The text only mentions the distance of the Voyager 1 spacecraft and the number of stars in the Milky Way galaxy.\n",
      "\n",
      "However, if you'd like to know more about Mars exploration, I can suggest some general information or try to find additional resources for you. Alternatively, if you have any specific questions related to the provided sources, I'll do my best to help!\n",
      "\n",
      "References used:\n",
      "- æ—…è¡Œè€…1è™Ÿæ˜¯ç›®å‰é›¢åœ°çƒæœ€é çš„äººé€ ç‰©é«”ï¼Œæ–¼1977å¹´ç™¼å°„ã€‚\n",
      "- éŠ€æ²³ç³»åŒ…å«è¶…éä¸€åƒå„„é¡†æ†æ˜Ÿã€‚\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the RAG pipeline demo...\")\n",
    "\n",
    "# Select models\n",
    "llm_type, embedding_type = select_models()\n",
    "\n",
    "# Initialize models\n",
    "llm_model = LLMModel(llm_type)\n",
    "embedding_model = EmbeddingModel(embedding_type)\n",
    "\n",
    "print(f\"\\nUsing LLM: {llm_type.upper()}\")\n",
    "print(f\"Using Embeddings: {embedding_type.upper()}\")\n",
    "\n",
    "# Generate and load data\n",
    "# generate_csv()\n",
    "documents = load_csv()\n",
    "\n",
    "# Setup ChromaDB\n",
    "collection = setup_chromadb(documents, embedding_model)\n",
    "\n",
    "# Run queries\n",
    "queries = [\n",
    "    \"å“ˆå‹ƒå¤ªç©ºæœ›é é¡æ˜¯ä»€éº¼ï¼Ÿ\",\n",
    "    \"è·Ÿæˆ‘èªªèªªç«æ˜Ÿæ¢ç´¢\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Processing query: {query}\")\n",
    "    response, references = rag_pipeline(query, collection, llm_model)\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Response:\", response)\n",
    "    print(\"\\nReferences used:\")\n",
    "    for ref in references:\n",
    "        print(f\"- {ref}\")\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
