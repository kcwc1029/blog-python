{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5215093",
   "metadata": {},
   "source": [
    "## Readme\n",
    "這個 Notebook 是一個簡易版的 RAG（Retrieval-Augmented Generation）示範系統。\n",
    "\n",
    "主要功能包含：\n",
    "-    建立小型知識資料集（自動產生太空小知識並儲存成 CSV）\n",
    "-    將知識轉換成向量並存入 ChromaDB\n",
    "-    接收使用者問題，從資料庫搜尋最相關的知識\n",
    "-    把找到的知識，組成提示語（Prompt），交給 LLM 模型回答\n",
    "-    支援切換不同的嵌入模型（OpenAI、Chroma、Ollama）與 LLM（GPT-4o、Llama3.2）\n",
    "\n",
    "\n",
    "🔥 這份檔案能做什麼\n",
    "- 練習如何將「資料」➔「向量化」➔「建索引」➔「查詢」➔「生成回答」的完整流程\n",
    "- 學習最基本的 RAG 系統骨架\n",
    "- 為以後接更大型的知識檢索系統打基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4a126",
   "metadata": {},
   "source": [
    "### 載入套件、讀取 API 金鑰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef60bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb1e43",
   "metadata": {},
   "source": [
    "### 定義 EmbeddingModel 類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "442cf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這個類別負責決定要用哪種方法把文字轉成向量，你可以選擇\n",
    "# openai（用 OpenAI 模型轉向量）\n",
    "# chroma（用 ChromaDB 預設內建的轉向量）\n",
    "# nomic（用 Ollama 本地模型轉向量）\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_type=\"openai\"):\n",
    "        self.model_type = model_type\n",
    "        if model_type == \"openai\":\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "            self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=api_key,\n",
    "                model_name=\"text-embedding-3-small\",\n",
    "            )\n",
    "        elif model_type == \"chroma\":\n",
    "            self.embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
    "        elif model_type == \"nomic\":\n",
    "            # using Ollama nomic-embed-text model\n",
    "            self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"ollama\",\n",
    "                api_base=\"http://localhost:11434/v1\",\n",
    "                model_name=\"nomic-embed-text\",\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026209eb",
   "metadata": {},
   "source": [
    "### 定義 LLMModel 類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bad725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這個類別負責決定用哪個大語言模型 (LLM) 來回答問題，你可以選擇：\n",
    "#     openai（用 OpenAI GPT）\n",
    "#     ollama（用本地 Llama 模型）\n",
    "# 同時也提供一個 generate_completion() 方法 ➔ 把問題丟進去，得到回答。\n",
    "\n",
    "class LLMModel:\n",
    "    def __init__(self, model_type=\"openai\"):\n",
    "        self.model_type = model_type\n",
    "        if model_type == \"openai\":\n",
    "            self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "            self.model_name = \"gpt-4o-mini\"\n",
    "        else:\n",
    "            self.client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "            self.model_name = \"llama3.2\"\n",
    "\n",
    "    def generate_completion(self, messages):\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                temperature=0.0,  # 0.0 is deterministic\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2622f6d",
   "metadata": {},
   "source": [
    "### 選擇模型的介面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63b2128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 terminal 上讓你自己選要用哪種模型，包括：\n",
    "#     哪個 LLM（OpenAI 還是 Ollama）\n",
    "#     哪個向量轉換器（OpenAI / Chroma / Nomic）\n",
    "\n",
    "def select_models():\n",
    "    # Select LLM Model\n",
    "    print(\"\\nSelect LLM Model:\")\n",
    "    print(\"1. OpenAI GPT-4\")\n",
    "    print(\"2. Ollama Llama2\")\n",
    "    while True:\n",
    "        choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "        if choice in [\"1\", \"2\"]:\n",
    "            llm_type = \"openai\" if choice == \"1\" else \"ollama\"\n",
    "            break\n",
    "        print(\"Please enter either 1 or 2\")\n",
    "\n",
    "    # Select Embedding Model\n",
    "    print(\"\\nSelect Embedding Model:\")\n",
    "    print(\"1. OpenAI Embeddings\")\n",
    "    print(\"2. Chroma Default\")\n",
    "    print(\"3. Nomic Embed Text (Ollama)\")\n",
    "    while True:\n",
    "        choice = input(\"Enter choice (1, 2, or 3): \").strip()\n",
    "        if choice in [\"1\", \"2\", \"3\"]:\n",
    "            embedding_type = {\"1\": \"openai\", \"2\": \"chroma\", \"3\": \"nomic\"}[choice]\n",
    "            break\n",
    "        print(\"Please enter 1, 2, or 3\")\n",
    "\n",
    "    return llm_type, embedding_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98dcaf",
   "metadata": {},
   "source": [
    "### 自動產生一個小型 CSV\n",
    "先建立一個資料文件，來模擬要做的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "806d13f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV檔案 'space_facts.csv' 已成功建立！\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def generate_csv():\n",
    "    facts = [\n",
    "        {\"id\": 1, \"fact\": \"第一位環繞地球飛行的人類是尤里·加加林（1961年）。\"},\n",
    "        {\"id\": 2, \"fact\": \"阿波羅11號任務於1969年首次將人類送上月球。\"},\n",
    "        {\"id\": 3, \"fact\": \"哈伯太空望遠鏡於1990年發射，提供了宇宙的驚人影像。\"},\n",
    "        {\"id\": 4, \"fact\": \"火星是太陽系中被探索最多的行星，美國太空總署已派出多台探測車。\"},\n",
    "        {\"id\": 5, \"fact\": \"國際太空站（ISS）自2000年11月以來一直持續有人居住。\"},\n",
    "        {\"id\": 6, \"fact\": \"旅行者1號是目前離地球最遠的人造物體，於1977年發射。\"},\n",
    "        {\"id\": 7, \"fact\": \"SpaceX由伊隆·馬斯克創立，是第一家將人類送入軌道的私人公司。\"},\n",
    "        {\"id\": 8, \"fact\": \"詹姆斯·韋伯太空望遠鏡於2021年發射，是哈伯望遠鏡的後繼者。\"},\n",
    "        {\"id\": 9, \"fact\": \"銀河系包含超過一千億顆恆星。\"},\n",
    "        {\"id\": 10, \"fact\": \"黑洞是重力極強，連光也無法逃脫的時空區域。\"},\n",
    "    ]\n",
    "\n",
    "    with open(\"space_facts.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"id\", \"fact\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(facts)\n",
    "\n",
    "    print(\"CSV檔案 'space_facts.csv' 已成功建立！\")\n",
    "\n",
    "generate_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c39ec",
   "metadata": {},
   "source": [
    "### 載入 CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b2b424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    df = pd.read_csv(\"./space_facts.csv\")\n",
    "    documents = df[\"fact\"].tolist()\n",
    "    # print(\"載入文件\")\n",
    "    # for doc in documents:\n",
    "    #     print(f\"- {doc}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443772b",
   "metadata": {},
   "source": [
    "### 把資料塞進 ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "156f0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chromadb(documents, embedding_model):\n",
    "    client = chromadb.Client()\n",
    "    try:\n",
    "        client.delete_collection(\"space_facts\")\n",
    "    except:\n",
    "        pass\n",
    "    collection = client.create_collection(\n",
    "        name=\"space_facts\", embedding_function=embedding_model.embedding_fn\n",
    "    )\n",
    "    collection.add(documents=documents, ids=[str(i) for i in range(len(documents))])\n",
    "    print(\"文件已成功加入 ChromaDB ！\\n\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8aab9c",
   "metadata": {},
   "source": [
    "### 根據問題，找相關的小段資料\n",
    "當你問一個問題\n",
    "\n",
    "去 ChromaDB 資料庫找最接近問題意思的小知識\n",
    "\n",
    "回傳找出來的資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58f639aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_chunks(query, collection, top_k=2):\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "\n",
    "    print(\"\\nRelated chunks found:\")\n",
    "    for doc in results[\"documents\"][0]:\n",
    "        print(f\"- {doc}\")\n",
    "\n",
    "    return list(\n",
    "        zip(\n",
    "            results[\"documents\"][0],\n",
    "            (\n",
    "                results[\"metadatas\"][0]\n",
    "                if results[\"metadatas\"][0]\n",
    "                else [{}] * len(results[\"documents\"][0])\n",
    "            ),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91d586",
   "metadata": {},
   "source": [
    "### 整理 Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a5de03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query, related_chunks):\n",
    "    context = \"\\n\".join([chunk[0] for chunk in related_chunks])\n",
    "    augmented_prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    print(\"\\nAugmented prompt: ⤵️\")\n",
    "    print(augmented_prompt)\n",
    "\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a510c6",
   "metadata": {},
   "source": [
    "### 整個 RAG 流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cf37c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, collection, llm_model, top_k=2):\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "\n",
    "    related_chunks = find_related_chunks(query, collection, top_k)\n",
    "    augmented_prompt = augment_prompt(query, related_chunks)\n",
    "\n",
    "    response = llm_model.generate_completion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant who can answer questions about space but only answers questions that are directly related to the sources/documents given.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": augmented_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"\\nGenerated response:\")\n",
    "    print(response)\n",
    "\n",
    "    references = [chunk[0] for chunk in related_chunks]\n",
    "    return response, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede085c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the RAG pipeline demo...\n",
      "\n",
      "Select LLM Model:\n",
      "1. OpenAI GPT-4\n",
      "2. Ollama Llama2\n",
      "\n",
      "Select Embedding Model:\n",
      "1. OpenAI Embeddings\n",
      "2. Chroma Default\n",
      "3. Nomic Embed Text (Ollama)\n",
      "\n",
      "Using LLM: OLLAMA\n",
      "Using Embeddings: CHROMA\n",
      "文件已成功加入 ChromaDB ！\n",
      "\n",
      "\n",
      "==================================================\n",
      "Processing query: What is the Hubble Space Telescope?\n",
      "\n",
      "Processing query: What is the Hubble Space Telescope?\n",
      "\n",
      "Related chunks found:\n",
      "- 銀河系包含超過一千億顆恆星。\n",
      "- 火星是太陽系中被探索最多的行星，美國太空總署已派出多台探測車。\n",
      "\n",
      "Augmented prompt: ⤵️\n",
      "Context:\n",
      "銀河系包含超過一千億顆恆星。\n",
      "火星是太陽系中被探索最多的行星，美國太空總署已派出多台探測車。\n",
      "\n",
      "Question: What is the Hubble Space Telescope?\n",
      "Answer:\n",
      "\n",
      "Generated response:\n",
      "Unfortunately, I don't have any information about the Hubble Space Telescope from the provided context. The text only mentions the number of stars in a galaxy and the exploration of Mars by NASA.\n",
      "\n",
      "However, if you'd like to provide more context or information, I can try to help you answer your question about the Hubble Space Telescope.\n",
      "\n",
      "Final Results:\n",
      "------------------------------\n",
      "Response: Unfortunately, I don't have any information about the Hubble Space Telescope from the provided context. The text only mentions the number of stars in a galaxy and the exploration of Mars by NASA.\n",
      "\n",
      "However, if you'd like to provide more context or information, I can try to help you answer your question about the Hubble Space Telescope.\n",
      "\n",
      "References used:\n",
      "- 銀河系包含超過一千億顆恆星。\n",
      "- 火星是太陽系中被探索最多的行星，美國太空總署已派出多台探測車。\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Processing query: Tell me about Mars exploration.\n",
      "\n",
      "Processing query: Tell me about Mars exploration.\n",
      "\n",
      "Related chunks found:\n",
      "- 旅行者1號是目前離地球最遠的人造物體，於1977年發射。\n",
      "- 銀河系包含超過一千億顆恆星。\n",
      "\n",
      "Augmented prompt: ⤵️\n",
      "Context:\n",
      "旅行者1號是目前離地球最遠的人造物體，於1977年發射。\n",
      "銀河系包含超過一千億顆恆星。\n",
      "\n",
      "Question: Tell me about Mars exploration.\n",
      "Answer:\n",
      "\n",
      "Generated response:\n",
      "Unfortunately, I don't have any information on Mars exploration from the provided sources. The text only mentions the distance of the Voyager 1 spacecraft and the number of stars in the Milky Way galaxy.\n",
      "\n",
      "However, if you'd like to know more about Mars exploration, I can suggest some general information or try to find additional resources for you. Alternatively, if you have any specific questions related to the provided sources, I'll do my best to help!\n",
      "\n",
      "Final Results:\n",
      "------------------------------\n",
      "Response: Unfortunately, I don't have any information on Mars exploration from the provided sources. The text only mentions the distance of the Voyager 1 spacecraft and the number of stars in the Milky Way galaxy.\n",
      "\n",
      "However, if you'd like to know more about Mars exploration, I can suggest some general information or try to find additional resources for you. Alternatively, if you have any specific questions related to the provided sources, I'll do my best to help!\n",
      "\n",
      "References used:\n",
      "- 旅行者1號是目前離地球最遠的人造物體，於1977年發射。\n",
      "- 銀河系包含超過一千億顆恆星。\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the RAG pipeline demo...\")\n",
    "\n",
    "# Select models\n",
    "llm_type, embedding_type = select_models()\n",
    "\n",
    "# Initialize models\n",
    "llm_model = LLMModel(llm_type)\n",
    "embedding_model = EmbeddingModel(embedding_type)\n",
    "\n",
    "print(f\"\\nUsing LLM: {llm_type.upper()}\")\n",
    "print(f\"Using Embeddings: {embedding_type.upper()}\")\n",
    "\n",
    "# Generate and load data\n",
    "# generate_csv()\n",
    "documents = load_csv()\n",
    "\n",
    "# Setup ChromaDB\n",
    "collection = setup_chromadb(documents, embedding_model)\n",
    "\n",
    "# Run queries\n",
    "queries = [\n",
    "    \"哈勃太空望遠鏡是什麼？\",\n",
    "    \"跟我說說火星探索\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Processing query: {query}\")\n",
    "    response, references = rag_pipeline(query, collection, llm_model)\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Response:\", response)\n",
    "    print(\"\\nReferences used:\")\n",
    "    for ref in references:\n",
    "        print(f\"- {ref}\")\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
