LangChain æ˜¯ä¸€å€‹é–‹æºï¼ˆopen sourceï¼‰çš„æ¡†æ¶ï¼Œç”¨ä¾†å¿«é€Ÿå»ºæ§‹çµåˆ å¤šç¨® LLMs çš„æ‡‰ç”¨ç¨‹å¼ã€‚
é™¤äº†å…§å»º LLM å¤–ï¼Œä¹Ÿå¯ä»¥é€£çµå¤–éƒ¨è³‡æ–™ä¾†æºï¼ˆexternal sources of dataï¼‰ï¼


## 1. LLM Frameworksï¼ˆå¤§å‹èªè¨€æ¨¡å‹é–‹ç™¼æ¡†æ¶ï¼‰

ç‚ºä»€éº¼éœ€è¦ LLM æ¡†æ¶ï¼Ÿ
- ç°¡åŒ–æ•´åˆï¼šå°ˆæ³¨æ–¼é–‹ç™¼æ‡‰ç”¨ï¼Œè€Œä¸ç”¨è‡ªå·±ç®¡ç†åº•å±¤ç´°ç¯€
- æ¨¡çµ„åŒ–è¨­è¨ˆï¼šæ‹†åˆ†æˆå¯é‡ç”¨çš„å…ƒä»¶ï¼ˆæ˜“æ–¼æ“´å±•å’Œç¶­è­·ï¼‰
- æ”¯æ´ä¸Šä¸‹æ–‡ç®¡ç†ï¼šæ¨¡å‹éœ€è¦è¨˜æ†¶ä¸Šä¸‹æ–‡ï¼ˆæä¾›è¨˜æ†¶æ©Ÿåˆ¶ï¼‰
- æ–¹ä¾¿è³‡æ–™æ•´åˆï¼šæ¥ APIã€è³‡æ–™åº«å¾ˆç°¡å–®
- åŠ é€Ÿé–‹ç™¼æµç¨‹ï¼šé–‹ç™¼æ™ºèƒ½ä»£ç†ï¼ˆAgentsï¼‰ç­‰é«˜éšåŠŸèƒ½æ›´å¿«
    

å¸¸è¦‹ LLM Frameworksï¼š
- LangChainï¼ˆé€šç”¨å‹ï¼Œæ”¯æŒè¨˜æ†¶ã€ä»£ç†ã€æ¨¡çµ„åŒ–æµç¨‹ï¼‰
- LlamaIndexï¼ˆé‡è¦– RAG èˆ‡çµæ§‹åŒ–/éçµæ§‹åŒ–è³‡æ–™æª¢ç´¢ï¼‰
- Haystackï¼ˆé‡è¦–æœå°‹èˆ‡å•ç­”ç³»çµ±å»ºç½®ï¼‰
    

## 2. ğŸ”‘ LangChain æ ¸å¿ƒçµ„ä»¶ï¼ˆKey Componentsï¼‰

- Modelsï¼šåŒ…è£ä¸åŒæ¨¡å‹æ¥å£ï¼ˆGPT, Embedding model, Chat modelï¼‰
- Promptsï¼šçµ±ä¸€ç®¡ç†æç¤ºè©ã€æ”¯æ´ç¯„ä¾‹å­¸ç¿’ï¼ˆfew-shot learningï¼‰
- Chains
    - æ”¯æ´ä¸²é€£å¤šå€‹æ­¥é©Ÿï¼šå¦‚å…ˆå•è³‡æ–™åº«å†å• LLM        
    - æ”¯æ´è‡ªå®šç¾©å·¥ä½œæµç¨‹ï¼ˆCustom Chainsï¼‰
- Memory
    - çŸ­æœŸè¨˜æ†¶ï¼šåªè¨˜ä½å–®æ¬¡å°è©±
    - é•·æœŸè¨˜æ†¶ï¼šè·¨å¤šè¼ªå°è©±ä¿æŒä¸Šä¸‹æ–‡
- Toolsï¼šæä¾›å¤–éƒ¨å·¥å…·æ•´åˆï¼Œä¾‹å¦‚ API è³‡æ–™ä¾†æºã€è³‡æ–™åº«å­˜å–        
- Agentsï¼šè‡ªä¸»ç³»çµ±ï¼Œè®“æ¨¡å‹èƒ½æ ¹æ“šéœ€æ±‚é¸æ“‡è¡Œå‹•ï¼Œä¸åªå›è¦†å•é¡Œ





## 3. ğŸ“ˆ LangChain å¯¦éš›æ‡‰ç”¨å ´æ™¯

- æ–‡ä»¶å•ç­”ç³»çµ±ï¼ˆDocument QAï¼‰
- æ™ºèƒ½å°è©±æ©Ÿå™¨äºº
- ä¼æ¥­å…§éƒ¨çŸ¥è­˜åº«æª¢ç´¢
- è‡ªå‹•åŒ–è³‡æ–™è™•ç†ä»£ç†äºº
- è³‡æ–™è’é›†èˆ‡å¤–éƒ¨ API æ•´åˆ

### 3.1. Labï¼šä½¿ç”¨LangChainèˆ‡deepseeké€²è¡Œäº¤äº’åŸºæœ¬å•ç­”

```python
from dotenv import load_dotenv
load_dotenv()

from langchain.chat_models import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# DeepSeek API KEY
import os
api_key = os.getenv("DEEPSEEK_API_KEY")  # ä½ çš„ .env è¦å­˜é€™å€‹ API KEY

# åˆå§‹åŒ– DeepSeek
model = ChatOpenAI(
    openai_api_key=api_key,
    base_url="https://api.deepseek.com",  
    model="deepseek-chat",  
)

# å»ºç«‹è¨Šæ¯
messages = [
    SystemMessage(content="å°‡ä¸‹åˆ—çš„è‹±æ–‡ç¿»è­¯æˆä¸­æ–‡"),
    HumanMessage(content="hi!"),
]

# å‘¼å«
response = model.invoke(messages)
print(response.content)
```

### 3.2. Labï¼šä½¿ç”¨LangChainå»ºç«‹ **Prompt Template**ï¼ˆæœ‰è®Šæ•¸çš„æç¤ºè©ï¼‰
```python
from dotenv import load_dotenv
load_dotenv()

from langchain.chat_models import ChatOpenAI
# from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate

system_template = "å°‡ä¸‹åˆ—è‹±æ–‡ç¿»è­¯æˆ {language}"

prompt_template = ChatPromptTemplate.from_messages(
    [("system", system_template), ("user", "{text}")]
)

prompt = prompt_template.invoke({"language": "ä¸­æ–‡", "text": "hi"})
print(prompt)
response = model.invoke(prompt)
print(response.content)
```


## 4. Indexesã€Retrievers èˆ‡ Data Preparation

### 4.1. Data Preparationï¼ˆè³‡æ–™æº–å‚™ï¼‰

**ç›®æ¨™**ï¼š  æŠŠ**åŸå§‹è³‡æ–™**è½‰æˆå¯ä»¥çµ¦ AI æª¢ç´¢çš„çµæ§‹åŒ–è³‡æ–™ã€‚

**æµç¨‹**ï¼š
- **Raw Data**ï¼ˆåŸå§‹è³‡æ–™ï¼‰
- â¡ï¸ **Document Loader**ï¼ˆè®€å–æ–‡ä»¶ï¼‰
- â¡ï¸ **Text Splitters**ï¼ˆæ–‡å­—åˆ‡å‰²å™¨ï¼ŒæŠŠé•·æ–‡ä»¶åˆ‡å°æ®µï¼‰
- â¡ï¸ **Text Chunks**ï¼ˆä¸€å°æ®µä¸€å°æ®µçš„æ–‡æœ¬ï¼‰
- â¡ï¸ **Embeddings**ï¼ˆè½‰æˆå‘é‡ï¼‰
- â¡ï¸ å­˜å…¥ **Vector Store**ï¼ˆå‘é‡è³‡æ–™åº«ï¼‰
    

### 4.2. Indexesï¼ˆç´¢å¼•ï¼‰

**ç›®æ¨™**ï¼š  å¿«é€Ÿçµ„ç¹”å’Œæ•´ç†æ–‡ä»¶ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥è©¢ã€‚
æŠŠæ–‡ä»¶æ•´ç†æˆã€Œç´¢å¼•ã€ï¼Œå°±åƒåœ–æ›¸é¤¨å»ºç›®éŒ„ä¸€æ¨£ï¼Œæ–¹ä¾¿æ‰¾è³‡æ–™ã€‚
**å¸¸è¦‹ç´¢å¼•é¡å‹**ï¼š
- **Vector Store Index**  
    â†’ æŠŠæ–‡ä»¶è½‰æˆå‘é‡ï¼Œå¯ä»¥ç”¨ã€Œç›¸ä¼¼åº¦ã€ä¾†å¿«é€Ÿæ‰¾è³‡æ–™ã€‚  
    â†’ ç¯„ä¾‹ï¼šFAISSã€Pineconeã€Chromaã€‚
- **List Index**  â†’ æŠŠæ‰€æœ‰æ–‡ä»¶ç°¡å–®åœ°åˆ—æˆä¸€å€‹æ¸…å–®ã€‚
- **SQL Index**  â†’ ç”¨ SQL æ–¹å¼å„²å­˜çµæ§‹åŒ–è³‡æ–™ã€‚
- **Graph Index**  â†’ ç”¨ã€Œåœ–å½¢çµæ§‹ã€å„²å­˜è³‡æ–™ï¼Œæ¯”å¦‚çŸ¥è­˜åœ–è­œã€‚

### 4.3. Retrieversï¼ˆæª¢ç´¢å™¨ï¼‰
Retrievers å°±åƒåœ–æ›¸é¤¨æ«ƒæª¯å“¡ï¼Œå¹«ä½ æ ¹æ“šã€Œå•é¡Œã€å¿«é€Ÿæ‰¾åˆ°æœ€ç›¸é—œçš„è³‡æ–™ã€‚
**ç›®æ¨™**ï¼š  æ ¹æ“šç”¨æˆ¶è¼¸å…¥çš„å•é¡Œï¼Œæ‰¾åˆ°æœ€ç›¸é—œçš„æ–‡ä»¶æ®µè½ã€‚
**Retrievers çš„åŠŸèƒ½**ï¼š
- æ”¶åˆ° **Query**ï¼ˆå•é¡Œï¼‰
- â¡ï¸ é€²è¡Œæª¢ç´¢ï¼ˆæ ¹æ“šç´¢å¼•è³‡æ–™åº«ï¼‰
- â¡ï¸ æ‰¾åˆ° **Relevant Documents**ï¼ˆç›¸é—œæ–‡ä»¶ï¼‰

**Retrievers å¸¸è¦‹æ–¹å¼**ï¼š
- **Vector Search**ï¼ˆå‘é‡ç›¸ä¼¼åº¦æœå°‹ï¼‰
- **Keyword Search**ï¼ˆé—œéµå­—æœå°‹ï¼‰
- **Hybrid Search**ï¼ˆæ··åˆæœå°‹ï¼‰


### 4.4. Full Workflowï¼ˆå®Œæ•´æµç¨‹ï¼‰
1. åŸå§‹è³‡æ–™
2. â¡ï¸ Document Loaderï¼ˆè®€å…¥ï¼‰
3. â¡ï¸ Text Splittersï¼ˆåˆ‡å‰²ï¼‰
4. â¡ï¸ Text Chunksï¼ˆå°æ®µæ–‡å­—ï¼‰
5. â¡ï¸ Embeddingsï¼ˆå‘é‡è½‰æ›ï¼‰
6. â¡ï¸ Vector Storeï¼ˆå‘é‡è³‡æ–™åº«ï¼‰
7. â¡ï¸ Queryï¼ˆæå•ï¼‰
8. â¡ï¸ Retrieverï¼ˆæœå°‹ç›¸é—œæ®µè½ï¼‰
9. â¡ï¸ LLMï¼ˆå¤§èªè¨€æ¨¡å‹å›ç­”ï¼‰
10. â¡ï¸ Responseï¼ˆç”¢ç”Ÿå›è¦†ï¼‰
    

### 4.5. Labï¼šLangChain Text Splitter
æ€éº¼æŠŠä¸€ä»½å¤§æ–‡ä»¶ï¼Œåˆ‡æˆå¾ˆå¤šå°æ®µè½ (chunks)
æ¯æ®µéƒ½æ§åˆ¶åœ¨ä¸€å®šå¤§å°å…§ï¼Œæ–¹ä¾¿ä¹‹å¾Œçµ¦æ¨¡å‹ä½¿ç”¨ï¼
é€™æ˜¯åš RAG (Retrieval-Augmented Generation) çš„å‰ç½®æ­¥é©Ÿï¼
```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load your document 
text_loader = TextLoader("./dream.txt")  
documents = text_loader.load()  

# Create text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20,
    length_function=len,
)

# Split documents
splits = text_splitter.split_documents(documents)
for i, split in enumerate(splits):
    print(f"Split {i+1}:\n{split}\n")
```

```text
# è¼¸å‡ºæ•ˆæœ
Split 1:
page_content='And so even though we face the difficulties of today and tomorrow, I still have a dream. It is a' metadata={'source': './dream.txt'}

Split 2:
page_content='a dream. It is a dream deeply rooted in the American dream.' metadata={'source': './dream.txt'}

Split 3:
page_content='I have a dream that one day this nation will rise up and live out the true meaning of its creed:' metadata={'source': './dream.txt'}
...
```
### 4.6. Labï¼šLangChain Text Loaders
å®‰è£å¥—ä»¶
```
pip install faiss-cpu
```

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import re
import os


api_key = os.getenv("DEEPSEEK_API_KEY")  # ä½ çš„ .env è¦å­˜é€™å€‹ API KEY

# æ¸…ç†æ–‡å­—è³‡æ–™ï¼ˆData Cleaningï¼‰
def clean_text(text):
    text = re.sub(r"[^a-zA-Z\s]", "", text)  # åªç•™è‹±æ–‡å­—æ¯å’Œç©ºç™½
    text = re.sub(r"\s+", " ", text).strip()  # æŠŠå¤šé¤˜çš„ç©ºç™½åˆæˆä¸€å€‹ç©ºç™½
    text = text.lower()                      # å…¨éƒ¨è½‰å°å¯«
    return text

# è¼‰å…¥æ–‡å­—æª”æ¡ˆ
documents = TextLoader("./dream.txt").load()
cleaned_documents = [clean_text(doc.page_content) for doc in documents] # è®€å– dream.txt é€™å€‹æª”æ¡ˆå…§å®¹ã€‚

# åˆ‡å‰²æ–‡å­—æˆå°æ®µï¼ˆåˆ†æ®µï¼‰
# chunk_size=500ï¼šæ¯æ®µæœ€å¤š500å€‹å­—     
# chunk_overlap=100ï¼šå‰å¾Œæ®µè½é‡ç–Š100å€‹å­—ã€‚
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100) 
texts = text_splitter.split_documents(documents)
texts = [clean_text(text.page_content) for text in texts]

# æŠŠå°æ®µè½‰æˆå‘é‡ Embedding
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# å»ºç«‹æª¢ç´¢å™¨ Retriever
retriever = Chroma.from_texts(texts, embeddings).as_retriever()

# æŸ¥è©¢ retriever
query = "è«‹ä»¥ç²¾è¦é»æ¦‚è¿°æ¼”è¬›å…§å®¹"
docs = retriever.invoke(query) # æœ€æœ‰å¯èƒ½å›ç­”ä½ å•é¡Œçš„æ–‡ä»¶æ®µè½é›†åˆ
# print(docs)

# Chat with the model and our docs
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_template(
    "Please use the following docs {docs},and answer the following question {query}",
)


# åˆå§‹åŒ– DeepSeek
model = ChatOpenAI(
    openai_api_key=api_key,
    base_url="https://api.deepseek.com",  # DeepSeekå®˜æ–¹API URL
    model="deepseek-chat",  # ä¹Ÿå¯ä»¥æ˜¯ deepseek-coder
)
chain = prompt | model | StrOutputParser()

response = chain.invoke({"docs": docs, "query": query})
print(response)
```


### 4.7. Labï¼šLangChain PDFLoader
```python
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    CSVLoader,
    DirectoryLoader,
)

# è¼‰å…¥å–®å€‹PDF
pdf_loader = PyPDFLoader("./data/DeepSeek-Coder When the Large Language Model Meets.pdf")

docs = pdf_loader.load()
print("PDF Documents:", docs)
```


### 4.8. Labï¼šLangChain DirectoryLoader
è¦å…ˆåšä¸€å€‹è³‡æ–™å¤¾ã€dataã€‘ï¼Œç„¶å¾Œä»–æ˜¯å°‡ã€dataã€‘å…§æ‰€æœ‰æ–‡ä»¶è¼‰å…¥
```python
from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader, DirectoryLoader

dir_loader = DirectoryLoader("./data/", glob="**/*.txt", loader_cls=TextLoader)
dir_documents = dir_loader.load()

print("Directory Text Documents:", dir_documents)
```

### 4.9. Labï¼šchains
å¦‚ä½•æŠŠå¤šå€‹è™•ç†æ­¥é©Ÿã€Œä¸²æ¥ã€èµ·ä¾†ï¼Œåƒä¸€æ¢éˆä¸€æ¨£ï¼ŒæŠŠã€Œæç¤ºï¼ˆpromptï¼‰â†’æ¨¡å‹æ¨è«–â†’è¼¸å‡ºè§£æã€é€™äº›æ­¥é©Ÿæ•´åˆæˆä¸€å€‹æµç¨‹ã€‚

é€™é‚Šæ˜¯ä½¿ç”¨langchainé€£æ¥deepseekï¼Œæ‰€ä»¥è¦å…ˆå®‰è£å¥—ä»¶
```python
pip install langchain-deepseek
```

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_deepseek import ChatDeepSeek

from dotenv import load_dotenv
load_dotenv()
import os
api_key = os.getenv("DEEPSEEK_API_KEY")  # ä½ çš„ .env è¦å­˜é€™å€‹ API KEY


# Define a prompt template
prompt = ChatPromptTemplate.from_template("æˆ‘ä»Šå¤©å¿ƒæƒ…{mood}")

# Create a chat modelï¼šåˆå§‹åŒ– DeepSeek
model = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0.7,
    max_tokens=1024,
    timeout=30,
    max_retries=2,
)

# Chain the prompt, model, and output parser
chain = prompt | model | StrOutputParser()

# Run the chain
response = chain.invoke({"mood": "çœŸçš„éå¸¸ç³Ÿ"})
print(response)
```

### 4.10. Labï¼šLangChain æ¶æ§‹ä¸‹ï¼Œå…¸å‹çš„ RAGï¼ˆRetrieval-Augmented Generationï¼‰æµç¨‹
æµç¨‹
- è¦æº–å‚™ä¸€ä»½txtï¼Œä½œç‚ºæª¢ç´¢æ–‡ä»¶ï¼Œé€™é‚Šæ¸¬è©¦çš„æ–‡ä»¶æª”åç‚ºdream.txt
- è®€å– + æ¸…æ´— (clean_text)
- åˆ†æ®µ chunkï¼ˆæ¯æ®µ500å­—ï¼‰
- å‘é‡åŒ– embeddingï¼ˆtext-embedding-3-smallï¼‰
- å»ºç«‹å‘é‡è³‡æ–™åº« Chroma + æª¢ç´¢å™¨ retriever
- ä½¿ç”¨è€…æå• query
- retriever æ“·å–ç›¸é—œæ®µè½
- prompt å¡«å…¥ {docs} + {query}
- çµ„åˆæç¤º + å•Ÿå‹• DeepSeek æ¨¡å‹å›ç­”

```python
from langchain_community.document_loaders import TextLoader        # è®€å–æœ¬åœ°æ–‡å­—æª”
from langchain_community.vectorstores import Chroma                # ä½¿ç”¨ Chroma ä½œç‚ºå‘é‡è³‡æ–™åº«
from langchain_openai import OpenAIEmbeddings                     # ä½¿ç”¨ OpenAI çš„å‘é‡åµŒå…¥æ¨¡å‹
from langchain_deepseek import ChatDeepSeek                       # ä½¿ç”¨ DeepSeek èŠå¤©æ¨¡å‹
from langchain_text_splitters import RecursiveCharacterTextSplitter # å°‡é•·æ–‡æœ¬åˆ‡å‰²æˆå°æ®µè½
from langchain_core.output_parsers import StrOutputParser         # å°‡æ¨¡å‹å›æ‡‰è½‰æ›ç‚ºç´”å­—ä¸²
from langchain_core.prompts import ChatPromptTemplate             # å»ºç«‹èŠå¤©æç¤ºæ¨¡æ¿
from dotenv import load_dotenv                                     # è¼‰å…¥ .env è¨­å®šæª”ï¼ˆä¾‹å¦‚ API é‡‘é‘°ï¼‰
import os
import re                                                      


# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
api_key = os.getenv("DEEPSEEK_API_KEY")  # å¾ .env è®€å– DeepSeek API é‡‘é‘°


# å®šç¾©æ–‡å­—æ¸…ç†å‡½æ•¸
def clean_text(text):
    text = re.sub(r"[^a-zA-Z\s]", "", text)     # ç§»é™¤éè‹±æ–‡å­—æ¯èˆ‡ç©ºç™½
    text = re.sub(r"\s+", " ", text).strip()    # ç§»é™¤å¤šé¤˜ç©ºç™½
    return text.lower()                            # å…¨éƒ¨è½‰ç‚ºå°å¯«


# è¼‰å…¥ä¸¦è™•ç†æ–‡ä»¶
documents = TextLoader("./dream.txt").load()                      # è¼‰å…¥æ–‡å­—æ–‡ä»¶
documents = [clean_text(doc.page_content) for doc in documents]   # æ¸…æ´—æ¯æ®µå…§å®¹

# åˆ†å‰²æ–‡å­—ç‚ºå¤šæ®µ chunk
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = text_splitter.create_documents(documents)                # åˆ‡å‰²å¾Œè½‰æ›ç‚º Document æ ¼å¼
chunks = [clean_text(doc.page_content) for doc in chunks]         # å†æ¬¡æ¸…æ´—æ¯å€‹ chunk


# å‘é‡åŒ–æ–‡æœ¬ä¸¦å»ºç«‹æª¢ç´¢å™¨
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
retriever = Chroma.from_texts(
    texts=chunks,
    embedding=embeddings,
    collection_name="my_documents",
    # persist_directory="./chroma_db"  # å¦‚æœéœ€è¦æŒä¹…åŒ–å„²å­˜ï¼Œå¯å–æ¶ˆè¨»è§£
).as_retriever(search_kwargs={"k": 2}) # å–å›èˆ‡æŸ¥è©¢æœ€ç›¸è¿‘çš„å‰ 2 æ®µæ–‡å­—


# ä½¿ç”¨è€…å•é¡Œ + æª¢ç´¢ç›¸é—œå…§å®¹ 
query = "é€™ç¯‡æ–‡ç« çš„æ ¸å¿ƒæ€æƒ³æ˜¯?"
docs = retriever.invoke(query)   # æ ¹æ“š query å¾ Chroma æª¢ç´¢ç›¸é—œå…§å®¹

# æ¨¡å‹å›æ‡‰
prompt = ChatPromptTemplate.from_template(
    "è«‹æ ¹æ“šæ–‡ä»¶ {docs}, ä¸¦ä½¿ç”¨ä¸­æ–‡å›ç­”ä»¥ä¸‹å•é¡Œ {query}"
)

model = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0.7,
    max_tokens=1024,
    timeout=30,
    max_retries=2,
)

chain = prompt | model | StrOutputParser()     # çµ„åˆ promptã€æ¨¡å‹ã€è§£æå™¨ç‚ºä¸€æ¢éˆ
response = chain.invoke({"docs": docs, "query": query})     # åŸ·è¡Œéˆä¸¦å–å¾—æ¨¡å‹å›æ‡‰


print(f"Model Response:::\n{response}")
```